[{"content":"2025 徒步记录及摄影总结 为了更好的显示效果，请您点击图片以便查看高清原图\n鸡公岭（01-05） 深圳湾公园（01-12） 1 月 12 日天气晴，空气透亮，适合去码头整点薯条。\n钓鱼翁（01-19） 相关资料：钓鱼翁\n沙澳逐蛇过深涌（02-24） 上窑走蛇万宜西（03-02） 东澳道中却相迎 (04-13) 蒙古国的沙尘暴终于吹到了香港，罕见 AQI 破 200，灰色是东澳古道的主色调。\n踏屻逐云双峰改（04-20） 南丫行游沧海笑（04-27） 大埔滘林径混穿（05-18） 香港海洋公园（08-27） 更多有关海洋公园的照片请访问我的 Unsplash 主页 查看。\n紫罗兰孖岗山（08-30） 老虎头郊游径（10-11） 浪茄湾破边洲（10-26） 由于周末人流量较大，因此先行前往浪茄湾玩水。后抵达破边洲时天气转阴，因此此次未拍摄破边洲照片。\n日落东山芒风起（11-02） 昂平黄牛天窗石（11-16） 附上夏季的 对比图。\n印度尼西亚伊真火山（12-31） 详见 印度尼西亚游记。\n结语 在 2024 年的基础上，本年度也算是走遍了香港的知名路线，其中不乏故地重游。年终临近时也因工作事物太多鸽了多次，但好在没有错过最美的风景。如果用一句话来结束 2025 年，那一定会是：\n只有已经在路上时，你才能遇到同行的人。\n","date":"2025-12-31T23:59:59+08:00","permalink":"https://jinggqu.github.io/posts/hiking-summary-2025/","title":"2025 徒步记录及摄影总结"},{"content":"使用 RustDesk 实现内网穿透 RustDesk 是一款开源远程桌面软件，支持 TCP 隧道（tunneling）功能，这可以用于转发端口，从而允许从其他设备（Windows 或 macOS）通过 SSH 访问 Ubuntu 服务器。即使服务器无 GUI，RustDesk 也可以在 headless（无头）模式下运行作为服务端。TCP 隧道本质上是客户端发起的端口转发：客户端连接到服务器的 RustDesk 后，设置本地端口转发到服务器的 SSH 端口（默认 22），然后从客户端本地 SSH 到该端口即可访问服务器。\n此设置假设 Ubuntu 服务器的 SSH 服务已启用（sudo apt install openssh-server \u0026amp;\u0026amp; sudo systemctl start ssh）。 默认情况下，RustDesk 使用公共中继服务器。如果需要自托管中继服务器（例如防火墙限制），可以额外安装 RustDesk Server，但这里聚焦于基本设置。 TCP 隧道需要在客户端的 RustDesk GUI 中配置（Windows/macOS 有 GUI），服务器侧只需启用并运行 RustDesk 服务。 安全提示：使用强密码，并考虑自托管以避免公共中继。 在 Ubuntu 服务器上安装和配置 RustDesk（CLI 操作） 更新系统包：\n1 sudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y 下载最新 RustDesk deb 包（检查 RustDesk GitHub Releases 获取最新版本，例如 1.4.2）：\n1 wget https://github.com/rustdesk/rustdesk/releases/download/1.4.2/rustdesk-1.4.2-x86_64.deb 安装 deb 包：\n1 sudo dpkg -i rustdesk-*.deb 修复任何依赖问题：\n1 sudo apt-get install -f 安装 headless 支持所需包（虚拟显示驱动，可选）：\n1 sudo apt install ubuntu-desktop xserver-xorg-video-dummy lightdm -y 启用 headless 模式：\n1 sudo rustdesk --option allow-linux-headless Y 作为系统服务启动 RustDesk（自动后台运行）：\n1 2 sudo systemctl start rustdesk sudo systemctl enable rustdesk # 开机自启 获取 RustDesk ID（用于客户端连接）：\n1 sudo rustdesk --get-id （输出类似：123456789）\n设置永久密码（替换 \u0026lt;your_password\u0026gt; 为强密码）：\n1 sudo rustdesk --password \u0026lt;your_password\u0026gt; 启用 TCP 隧道（默认已启用，但可手动确认/设置）：\n编辑配置文件（通常在 /root/.config/rustdesk/RustDesk.toml）： 1 sudo vim /root/.config/rustdesk/RustDesk.toml 添加或修改： 1 enable-tunnel = \u0026#34;Y\u0026#34; 保存后，重启服务： 1 sudo systemctl restart rustdesk 在客户端设备上配置 RustDesk 和 TCP 隧道 各平台都可从 RustDesk GitHub Releases 下载并安装 RustDesk 客户端。安装完成后打开 RustDesk 客户端，输入服务器的 ID 和密码进行连接。连接成功后，在 RustDesk 界面中设置 TCP 隧道。\n设置端口转发 点击已添加设备菜单中的 TCP Tunneling 按钮\n添加新隧道：\n本地端口：任意可用端口，例如 50002（本地监听端口）。 远程主机：localhost（或服务器内网 IP，如果不同）。 远程端口：22（SSH 端口）。 上述端口转发设置将实时生效，在使用过程中请勿关闭端口转发窗口。\n测试隧道 在客户端命令行（Windows: PowerShell 或 cmd；macOS: Terminal）运行：\n1 ssh \u0026lt;ubuntu_username\u0026gt;@localhost -p 50002 输入 Ubuntu 服务器的 SSH 密码，即可连接。这会将客户端的本地 50002 端口流量转发到服务器的 22 端口，实现 SSH 访问。\n或者可以通过 Visual Studio Code / Cursor 等支持 SSH 访问的软件进行上述测试，详细的 SSH 配置如下：\n1 2 3 4 Host my-linux-rustdesk HostName localhost User root Port 50002 其他使用场景 除了上述的 SSH 连接之外，还可以通过将 Windows 设备的 Remote Desktop Protocol 端口（RDP 端口默认为 3389）映射到本地，例如在上述 Port Forwarding 窗口中配置本地 40002 端口转发到远端 3389 端口。然后使用 Windows APP 新建 RDP 远程桌面连接，Windows APP 中的设置如下图，设置远端 Windows 用户名和密码后，即可通过 RustDesk 端口转发的形式完成远程桌面访问。\n但话说回来，RustDesk 本身就支持远程桌面访问，上述需求也许算是一个伪需求吧～\n为 RustDesk 设置代理 在某些情况下穿透不太稳定，因此可以设置代理使远程连接更加丝滑。如果本机已经安装代理软件且默认开放局域网访问（如 Clash 系列软件，默认开启本机 7890 端口接收其他软件或设备的代理请求），因此可以将 RustDesk 的流量转发给 Clash（127.0.0.1:7890）使其托管，设置方法如下：\n常见问题排查 连接失败：检查防火墙（sudo ufw allow 21114:21119/tcp \u0026amp;\u0026amp; sudo ufw allow 21116/udp），或使用自托管服务器（参考 https://rustdesk.com/docs/en/self-host/）。 无输出：CLI 命令需 sudo 执行。 密码加密：如果配置文件中密码加密，可用 --get-temp-password 获取临时密码。 多设备：每个客户端独立连接并设置隧道，支持同时多个。 ","date":"2025-08-18T12:00:00+08:00","permalink":"https://jinggqu.github.io/posts/rustdesk/","title":"使用 RustDesk 实现内网穿透"},{"content":"东京游记 前言 换现金 根据游玩时长，每人每天准备 JPY 2000 几乎就可以满足日常消费需求。同时，大部分店铺均支持信用卡付款，便利店甚至支持微信与支付宝支付，因此不建议换取太多日元现金，以免收获一大堆需要时又找不到的硬币（笑）。\n填写入境卡 无论持有单次签证或多次签证，均需提前填写入境卡。在航班上空乘会发放纸质版的入境卡，或者可访问以下网址在线填写入境卡，方便保存和使用（入境审查、海关申报及免税购买所需）。点击 Visit Japan Web 在线填写。\nSuica 及东京三日通 在机场即可购买 Suica （绿色版）及 Welcome Suica （红色旅客版，免押金）。同时还可以购买东京单日、双日及三日通（Tokyo Subway Ticket），可适用于东京地铁和都营地铁，票价分别为 JPY 800、JPY 1200 及 JPY 1500。相比起步价为 JPY 178 的东京地铁，使用三日通乘坐大约 8 次即可赚回本金，推荐购买。可查看 Tokyo Subway Ticket 官方介绍 了解更多信息。\n东京地铁的换乘系统较为复杂，建议严格按照 Google Maps 的指引进行换乘。Google Maps 会显示出最优的换乘路线和时间，避免走冤枉路。东京地铁线路图可以参见官方 运行线路图。\n可使用三日通乘车的路线仅包含上述运行图中的单字母路线（线路标识通常为空心圆形或圆角矩形加字母，如：A 浅草线、G 银座线、M 丸之内线等），而不包含 JR 线路（通常为双字母线路，如 JK 京滨东北线、JY 山手线）及私营线路（线路标识通常为圆形，如：R 临海线）。\n由于东京地铁皆由字母+站点数字编号标识，因此在辨识方向时，仅需查看站点数字编号的变化趋势即可。例如：从东京站前往新宿站，仅需乘坐 M 丸之内线从第 17 站到第 8 站。地铁站内辨别列车方向时，亦可通过站点数字编号来判断列车行驶方向。详见下图站点名左侧蓝色方框部分。\n若使用三日通卡乘坐了地铁 + JR 线路或私营线路，需在出站时补交差价。游客需要前往人工服务台请求精算服务，工作人员会根据乘车记录计算出差价并收取现金，同时提供精算券，使用精算券即可出闸。若使用 Suica 乘车，在出站时闸机会自动计算地铁 + JR 线路的总费用，一次性扣除。\nJR 线路不同类型列车及相关票面信息可见于 JR 东日本官网。\n电话卡 中国大陆及港澳地区的游客，可直接购买中国移动香港提供的 CMLink 电话卡套餐，有多种套餐可供选择，同时还提供 eSIM 及旧卡套餐续订服务，非常便捷。详情可访问 CMLink 套餐介绍页面 了解更多信息。\n语音翻译 建议安装由日本 NICT 推出的 VoiceTra 沃译通 语音翻译软件，支持多种语言互译。该软件可在 Google Play 和 App Store 中下载。\n住宿 此次住宿选择了位于银座附近的酒店，靠近日比谷线筑地站（H-11）与有乐町线新富町站（Y-20）。每人每晚约合 HKD 540，性价比极高。详见 東京銀座首都酒店茜館。\n第一天：富士山 东京站 - 山中湖 - 河口湖 - 大石公园 - 浅间公园 - 日川时计店 - 东京站\n由于天气原因，富士山仍犹抱琵琶半遮面，但在天气不断变化之间仍然可以看到它的轮廓。请不要相信富士山的能见度预报，下图为满级能见度（10 分）的富士山实拍，且仅有5分钟可见。\n第二天：雨中赏樱 上野公园 - 东京大学 - 千鸟渊\n第三天：传统与现代 新宿御苑 - 涩谷 - 新宿 - 歌舞伎町\n第四天：镰仓与东京塔 藤泽站 - 江之岛 - 镰仓高校前站 - 七里滨 - 长谷寺 - 镰仓站\n从酒店出发乘坐横滨方向的 JR JT 线，约 1 小时到达藤泽站。在藤泽站即可购买江之电一日车票，票价 JPY 600，这是一种可以在江之电全区间、一天之内不限次数，可以自由上下车的车票。江之电沿线站点图及相关介绍见 江之电官网.\n第五天：目黑川 结束语 东京之旅短暂且匆忙，也没有一次晴天。虽不见富士山真容，却收获海量樱花，也算不愧此行。不知下次再与东京相逢，是否还会是这个季节？\n清明节连绵的阴雨天气让我想起了新海诚的《言叶之庭》，此处引用一句其中台词作为游记结束语：\n隱約雷鳴，陰霾天空，但盼風雨來，能留你在此。\n隱約雷鳴，陰霾天空，即使天無雨，我亦留此地。\n","date":"2025-04-03T23:59:59+08:00","permalink":"https://jinggqu.github.io/posts/trip-to-tokyo/","title":"东京游记"},{"content":"2024 徒步记录及摄影总结 为了更好的显示效果，请您点击图片以便查看高清原图\n太平山（01-20） 相关资料：發掘太平山頂趣味玩法\n龙脊-鹤咀（06-02） 相关资料：香港島龍脊：坐擁醉人景致的標誌性遠足徑\n昂平高原（06-10） 相关资料：昂平高原\n太平山卢吉道（08-24） 相关资料：港島徑第一段山頂盧吉道無敵海景\n长洲（09-07） 相关资料：長洲：懷舊魅力和時尚景點交融，為小島悠閒氛圍注入新活力\n城门水塘（09-29） 相关资料：城門水塘半日遊行山路線，穿梭優美白千層樹林\n澳门（10-05） 鹤咀（10-11） 毕架山-狮子山（10-20） 东龙洲（10-27） 大东山（11-02） 青山-下白泥（11-10） 最无心拍照的、难度最大的一次行山。\n相关资料：【冷門行山好去處】上青山行大峽谷 直落下白泥睇日落\n深圳仙湖植物园（11-22） 最轻松的一次徒步，甚至都算不上行山。\n相关资料：深圳仙湖植物园\n合欢山（12-02） 照片资料及景区介绍请参考之前的台湾游记合欢山部分。\n龙脊-大浪湾（12-15） 行程较简单，全程阴天，相机难得休息一次。\n桥咀洲（12-22） 相关资料：橋咀洲（半日遊）\n破边洲-麦理浩径二段（12-23） 4 小时 30 分奔袭 26 公里，可惜天公不作美，全程阴天。\n塔门岛（12-24） 结语 2024 年去过很多地方，认识了很多新朋友，也有很多新的体验。希望 2025 年也能继续保持这种状态，探索更多未知的风景。\n","date":"2024-12-31T23:59:59+08:00","permalink":"https://jinggqu.github.io/posts/hiking-summary-2024/","title":"2024 徒步记录及摄影总结"},{"content":"使用 VSCode 编写 LaTeX 项目的一些技巧 表格单元格多行内容 在 LaTeX 中，表格中的单元格内容默认是单行显示的，如果内容过长，会导致表格宽度过宽。此问题有两种解决方案：\n使用 makecell 宏包中的 \\makecell 命令来实现多行内容。 使用 p{width} 来指定单元格宽度，其中 width 为单元格宽度，如 p{3cm}。 使用 m{width} 来指定单元格垂直居中，其中 width 为单元格宽度，如 m{3cm}。 结合 m{width} 的自定义命令 使用 makecell makecell 宏包提供了 \\makecell 命令，可以在表格中的单元格中插入多行内容。使用方法如下：\n1 2 3 4 5 \\usepackage{makecell} \\begin{tabular}{|c|} \\makecell{This is a long line \\\\ with a line break} \\\\ \\end{tabular} 但 makecell 宏包的缺点是，如果表格中有多个单元格需要多行内容，需要在每个单元格中都使用 \\makecell 命令，这样会使得代码变得冗长。同时，\\makecell 命令需要手动在需要换行的地方插入 \\\\，这样会使得表格内容发生变化时难以维护。\n使用 p{width} p{width} 是 LaTeX 中的一个列格式，可以指定单元格的宽度。使用方法如下：\n1 2 3 \\begin{tabular}{|p{3cm}|} This is a long line with a line break \\\\ \\end{tabular} 使用 p{width} 后，单元格中的内容默认顶部对齐且两端对齐，如果需要垂直居中，需要手动添加 \\centering 命令。\n使用 m{width} m{width} 是 LaTeX 中的一个列格式，可以指定单元格的宽度并且垂直居中。使用方法如下：\n1 2 3 \\begin{tabular}{|m{3cm}|} This is a long line with a line break \\\\ \\end{tabular} 使用 m{width} 后，单元格中的内容默认垂直居中且两端对齐，如果需要左对齐，需要手动添加 \\raggedright 命令。\n结合 m{width} 的自定义命令 为了方便使用 m{width} 列格式，同时实现更好的代码复用，可以定义以下命令用于在表格中插入多行内容。使用方法如下：\n1 2 3 \\newcolumntype{L}[1]{\u0026gt;{\\raggedright\\let\\newline\\\\\\arraybackslash\\hspace{0pt}}m{#1}} \\newcolumntype{C}[1]{\u0026gt;{\\centering\\let\\newline\\\\\\arraybackslash\\hspace{0pt}}m{#1}} \\newcolumntype{R}[1]{\u0026gt;{\\raggedleft\\let\\newline\\\\\\arraybackslash\\hspace{0pt}}m{#1}} 上述命令中，L 里通过 \\raggedright 指令让文字左对齐且右边不做对齐（即“ragged right”），而 p{width} 默认会进行左右对齐（两端对齐）或者受表格全局设置影响。C 里通过 \\centering 指令让文字居中对齐，R 里通过 \\raggedleft 指令让文字右对齐且左边不做对齐（即“ragged left”）。\n使用方法如下：\n1 2 3 \\begin{tabular}{|L{3cm}|} This is a long line with a line break \\\\ \\end{tabular} 通过定义 L{width}、C{width}、R{width} 三个命令，可以快速实现在表格中插入多行内容，并且与默认的 l、c、r 类似，可以显式指定内容的对齐方式。\n正文及表格自动格式化 在 VSCode 编写 LaTeX 项目中的表格时，最常用到的功能即为自动格式化。LaTeX 文件自动格式化依赖 VSCode 中的 LaTeX Workshop 插件，在使用格式化前请先安装该插件。\n在 Windows 平台中格式化的快捷键为 Alt + Shift + F。格式化时，VSCode 会根据 \u0026amp; 与 \\\\ 的位置自动调整表格的对齐方式。通常情况下，表格中的每一行内容末尾都会有 \\\\ 符号用于标志换行，而每一列内容之间都会有 \u0026amp; 符号用于标志列的分隔。\n但在某些情况下，我们可能使用 \\makecell 命令手动插入 \\\\ 符号用于单元格内容手动换行。而在格式化时，VSCode 会根据 \u0026amp; 与 \\\\ 的位置自动调整表格的对齐方式，在使用 \\makecell 的情况下，自动格式化功能可能会依据第一个 \\\\ 作为表格行末，这时候我们可以通过改变 LaTeX Workshop 设置来调整格式化的方式。\n通过在 LaTeX Workshop 设置中指定按照每行最后一个 \\\\ 作为右边界格式化表格，即可解决上述问题。打开 VSCode 设置，搜索 latex-workshop.formatting.latexindent.args，点击 Add Item 选项，添加以下内容：\n1 -y=lookForAlignDelims:tabular:alignFinalDoubleBackSlash:1 MacOS 无法格式化 若格式化时 VSCode - Output - LaTeX Workshop 中出现以下错误内容，则说明缺少 File::HomeDir perl 模块。\nstderr: Can\u0026rsquo;t locate File/HomeDir.pm in @INC (you may need to install the File::HomeDir module) (@INC contains:\u0026hellip;\n可通过以下命令安装，安装完成后重启 VSCode 即可。\n1 cpan -i File::HomeDir 在安装上述 File::HomeDir 时，可能会因为未安装 xcode-command-line-tools 而报错，此时需要先安装 xcode-command-line-tools。报错信息如下：\nxcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun\n使用以下命令安装 xcode-command-line-tools，执行命令后按照弹窗指示安装，然后重复上述安装步骤即可。\n1 xcode-select --install 表格列分隔宽度 在 LaTeX 中，表格中的列默认是没有分隔线的，如果需要添加分隔线，可以使用 \\hline 和 \\cline{x-y} 命令添加水平分隔线，或者使用 | 符号添加垂直分隔线。但在某些情况下，我们可能需要调整列分隔线的宽度，这时我们可以手动指定列分隔线的宽度。\n在 LaTeX 中，可以通过 \\tabcolsep 命令来指定分隔线的宽度。使用方法如下：\n1 2 3 4 5 6 7 8 9 10 \\begin{table} \\setlength{\\tabcolsep}{10pt} \\begin{tabular}{|c|c|} \\hline 1 \u0026amp; 2 \\\\ \\hline 3 \u0026amp; 4 \\\\ \\hline \\end{tabular} \\end{table} 在上述代码中，\\setlength{\\tabcolsep}{10pt} 命令用于设置分隔线的宽度为 10pt。在表格中，每个单元格的左右两侧都会有 10pt 的空白，即分隔线的宽度为 10pt。\n自带类型引用 \\cref 在 LaTeX 中，引用文献、公式、图表等内容时，通常使用 \\ref 命令。但 \\ref 命令只能引用编号，无法引用内容的类型（如“figure”、“section”、“table”等）。为了引用内容的类型，可以使用 cleveref 宏包中的 \\cref 命令。使用方法如下：\n1 2 3 4 5 6 7 8 \\usepackage{cleveref} \\begin{figure} \\caption{This is a figure} \\label{fig:example} \\end{figure} \\cref{fig:example} 在上述代码中，\\cref{fig:example} 命令会输出 figure 1，即引用了图表类型。但在某些情况下，我们可能需要引用内容的类型的缩写（如“fig”、“sec”、“tab”等），这时我们可以通过改变 \\cref 命令的设置来实现。\n通过在 LaTeX 文件中添加以下代码，即可实现引用内容类型的缩写：\n1 2 \\crefname{figure}{fig.}{figs.} \\Crefname{figure}{Fig.}{Figs.} 在上述代码中，\\crefname{figure}{fig.}{figs.} 命令用于设置引用图表类型的缩写为“fig.”、“figs.”，使用方式仍为 \\cref{fig:example}。\\Crefname{figure}{Fig.}{Figs.} 命令用于设置引用图表类型的缩写为“Fig.”、“Figs.”，使用方式为 \\Cref{fig:example}。\n若只需要大写的类型缩写，则可直接将 \\crefname 中的 fig. 改为 Fig.，figs. 改为 Figs.。如下：\n1 2 3 \\crefname{figure}{Fig.}{Figs.} \\crefname{table}{Table}{Tables} \\crefname{equation}{Eq.}{Eqs.} 常用缩写定义 在 LaTeX 中，有一些常用的缩写，如“e.g.”、“i.e.”、“etc.”等，这些缩写在文档中经常使用，为了方便使用，可以在 LaTeX 文件中定义这些缩写。使用方法如下：\n1 2 3 4 \\newcommand{\\eg}{\\textit{e.g.} } \\newcommand{\\ie}{\\textit{i.e.} } \\newcommand{\\etc}{\\textit{etc.} } \\newcommand{\\etal}{\\textit{et~al.}} 在上述代码中，分别定义了 \\eg、\\ie、\\etc、\\etal 四个命令，用于输出“e.g.”、“i.e.”、“etc.”、“et al.”。在文档中使用时，只需要调用相应的命令即可。\n空格的使用 在 LaTeX 中，空格的使用是非常重要的，不同的空格符号会产生不同的效果。在 LaTeX 中，空格符号有以下几种：\n空格符号： （空格键） 硬空格符号：~（波浪线键） 空格命令：\\quad 其中，空格符号是最常用的空格符号，用于在单词之间添加可断行空格。硬空格符号用于在单词之间添加不可断行的空格，即在硬空格符号处断行时，硬空格符号不会断开。空格命令 \\ 用于在单词之间添加空格，效果与空格符号相同。空格命令 \\quad 用于在单词之间添加一个较大的空格。若需要添加更大的空格，可以使用 \\qquad、\\hspace{1cm} 等命令。\n在引用文献、公式、图表、使用缩写等情况下，通常在连接处添加硬空格符号，以避免断行时内容分离。示例如下：\n1 Zhang~\\etal~\\cite{zhang2024} proposed a new method. 参考资料 How to create fixed width table columns with text raggedright/centered/raggedleft? tabular aligning on first double backslash instead of last How do I change column or row separation in LaTeX tables? cleveref – Intelligent cross-referencing ","date":"2024-12-25T12:00:00+08:00","permalink":"https://jinggqu.github.io/posts/latex-tricks/","title":"使用 VSCode 编写 LaTeX 项目的一些技巧"},{"content":"台湾游记 出发前准备 换现金 建议每人准备 TWD 2000-5000，方便日常消费。入境后，可以在任意 711 便利店或 ATM 机使用中国大陆银联卡取现，实时汇率，附加少量手续费。\n金福气 出发前至少一天登记金福气，入境后可以参加抽奖活动。登记网址：金福气官网。\n入境注意事项 填写入境卡 持有多次签证入境时需要填写入境卡，单次签证无需填写。机场有免费 WiFi 提供，可点击 入境卡填写 在线填写。\n悠游卡 入境后按照指示牌直接前往捷运站购买悠游卡。建议在捷运闸机处的自助机购买，不要去其他便利店，以免被加价。悠游卡是台湾最常用的交通卡，类似八达通卡，亦可在便利店消费。\n电话卡 可在中华电信柜台办理五日不限流量卡，费用 TWD 300 每张，支持信用卡或 Google Pay 等电子支付方式，方便随时上网。\n高铁和台铁购票 台湾的高铁站一般在市区外围，台铁站大多在市中心。高铁速度较快，但票价较贵；台铁速度慢，但适合城市间旅行。可以通过「高铁订票通」和「台铁订票通」APP 查看列车时刻表。\n在 711 便利店的高铁自助机可使用护照购票，但仅支持现金支付。也可以到高铁站或台铁站购票，支持信用卡或 Google Pay。\n网约车 台湾支持 Uber 等网约车服务，便捷的交通选择。\n旅行路线 香港 -\u0026gt; 台北 -\u0026gt; 台中 -\u0026gt; 台南 -\u0026gt; 高雄 -\u0026gt; 香港\n台北 从机场入境后，可以直接搭乘台北捷运前往市区，建议选择快线（紫色线），快速便捷，尽管票价比普通线路（蓝线，站站乐）稍贵。\n住宿 台北三德大饭店，双床房每人每晚约 HK$400（包含早餐），距离民权西路捷运站很近，交通非常便利。在线预订（无利益相关，下同）：台北三德大饭店。 景点 台北 101：可以乘高速电梯到达顶层（30 秒上升 80+ 层），欣赏台北 360 度的美丽夜景，门票 TWD 540（学生有优惠，记得携带学生证），支持 Google Pay。台北 101 营业到 21:00。 国立台湾大学：需从台北 101 附近乘公交车前往。公交车招手即停，上车下车均需刷卡，下车要提前按铃。 台北故宫博物院：参观完整且快速地逛完需要大半天的时间，门票 TWD 350。这里有丰富的文物，建议安排充足的时间。台北故宫位置偏远，但有餐厅且风景优美，如在故宫用餐建议坐在窗边位置，可以适用信用卡或 Google Pay 等电子支付方式。故宫内设有纪念品商店，包含经典画作复刻版、经典文物冰箱贴等，可电子支付。 艋舺（měngxiá）龙山寺和艋舺公园：较多人在寺庙内参拜，建议提前安排好时间。 淡水河日落：日落前，从艋舺龙山寺至少提前 30 分钟出发，走到淡水河边欣赏壮丽的日落，景色非常迷人。 西门町：沿着淡水河可一路步行至西门町吃晚饭逛夜市。 台中 由台北市乘坐高铁即可前往台中，票价 TWD 700 每人，行程共约一个小时。台中高铁站在台中市区西南角，前往酒店时可搭乘捷运（运营时间为06:00-24:00）或在高铁站负一层乘坐出租车。\n住宿 台中东旅，双床房每人每晚约 HK$220（包含早餐），距离台铁台中站很近，方便出行。在线预订：台中东旅。 浮云客栈，双床房每人每晚约 HK$240（无早餐），靠近逢甲大学，南侧逢甲夜市有大量食肆。在线预订：浮云客栈。 合欢山景区内亦提供住宿服务，详情参见 园区线上订房系统。 景点 合欢山：合欢山国家森林游乐区 东峰海拔约为 3421 米。乘车抵达合欢山游客服务中心，可经松雪楼由步道前往山顶，总长约 1000 米，爬升约 300 米。山顶气温较低（约 10 摄氏度），风大且紫外线强，建议备防风衣物与防晒霜。山顶有机会看到「金翼白眉」鸟类。全程包车费用 TWD 5000（往返市区与合欢山景区），单程耗时约 2 小时 30 分钟。可能会有轻微的高原反应。 清境农场：在这里可以与小羊亲密接触，购买羊饲料和小羊互动，黄昏时分非常美丽。 台南 从台中乘坐台铁前往台南，车程约 1 小时 30 分钟，台铁台南站距下述下榻酒店仅 10 分钟步行路程。\n住宿 台南三道门建筑文创旅店，双床房每人每晚约 HK$260（不含早餐）。在线预订：台南三道门建筑文创旅店。 景点 台湾文学馆：市区散步时，可朝「汤德章纪念公园」方向前进，此处为台南市的交通动脉，七条交通要道汇聚于此。台湾文学馆亦在此地。这里是台南市的重要文化地标，可以了解台湾的文学历史。 四草绿色隧道：需要乘车前往，游览总时间约 30 分钟，票价 TWD 200。绿意盎然的环境非常适合放松。 安平港渔港北堤灯塔：绿色隧道游览结束后，需提早在日落前抵达安平港渔港北堤灯塔（请注意当天日落时间）。北堤附近有大片沙滩，日落方向即为大陆福建省广东省。 食肆 安平豆花（满分推荐）。 所长茶叶蛋。 高雄 从酒店出发步行到台铁台南站，继续乘坐台铁前往高雄，车程约 40 分钟。在高雄站内有行李寄存柜，详情可在小红书搜索查看。\n景点 驳二艺术特区：驳二艺术特区将码头仓库群改造艺术园区，设有商铺、食肆、书店（诚品书店，值得一去）等，亦可观赏到多个艺术装置、运行中的有轨电车以及废旧机车等。详情请参考 驳二艺术特区官方网站。 旗津岛：从高雄新滨码头出发，搭乘轮渡前往旗津岛。由于旗津岛属狭长布局，因此建议上岸后在旗津码头租借四轮车，方便后续游览。旗津岛高雄灯塔在旗津码头西侧，且需要步行上山（约 15 分钟）。灯塔处可俯瞰旗津岛老街的彩色房子。骑四轮车沿着旗津三路即可到达彩虹教堂，教堂外设有两个艺术装置，可拍照打卡。由于旗津岛属狭长布局，可多留一些时间慢慢驾车游览。 结束语 虽山川异域，却风月同天。\n","date":"2024-12-04T23:59:59+08:00","permalink":"https://jinggqu.github.io/posts/trip-to-taiwan/","title":"台湾游记"},{"content":"A Few Thoughts Proof You Can Do Hard Things And if you’re not someone who knows they can do hard things, find a way to prove it to yourself. Build a habit, learn a skill, create something, whatever it is that turns your default stance on challenges from “that seems hard” to “I can figure it out.”\nsource: https://blog.nateliason.com/p/proof-you-can-do-hard-things\nStop Obsessing Over Tools I often see people in a constant search for the best note taking app. Or the best Linux distro and desktop setup. Or the best AI tool to enhance productivity. Or the best game engine. And so on, you guys get the point.\nThis search takes you nowhere. Doing this doesn’t make you productive. You will never find the best or the most perfect setup. Settling for good enough is most often the best thing you do. Otherwise you’ll find yourself productive in the search of being productive.\nsource: https://plug-world.com/posts/stop-obsessing-over-tools/\n重来：更为简单有效的商业思维 From chapter 音乐就在你的指尖流淌\n吉他大师说：“音乐就在你的指尖流淌。”就算你买了和艾迪·范·海伦（Eddie Van Halen）一样的吉他、效果踏板、扩音器，但是当你用这套装备来演奏时，弹出来的依然是你自己的风格。\n同理，给艾迪配一套从当铺倒腾出来的劣质装备，人家一出手，你还是能听出是艾迪·范·海伦的水平。好的装备的确能带来一些帮助，但事实是，你的演奏水平是由你自己的手指决定的。\n人们总忍不住要执着于工具，而不关注要用这些工具去做的事情。你见过这类人：能玩转一大堆震撼的艺术字体和漂亮的 Photoshop 滤镜效果的设计师，却不知道该表达什么。业余摄影爱好者总为使用胶片相机还是数码相机而争论不休，却没人关注拍出绝妙照片的决定因素是什么。\n很多业余高尔夫球手执着于加入昂贵的俱乐部，但是真正重要的是如何挥杆，而不是加入哪个俱乐部。就算让老虎伍兹加入廉价的俱乐部，他也照样能摆平你。\n人们把装备当作取胜的法宝，却不愿意花时间去练习，于是一直泡在专业器材店里。他们想要寻找捷径，然而，最好的工具不是用在普通领域的。而且你在起步阶段肯定用不上它。\n在商业领域，太多人纠结于工具的好坏、软件技巧、规模大小、舒适的办公环境、豪华的家具以及其他浮华的东西，而不去关注真正的要点。真正的要点就是怎样赢得客户、如何赢利。\n我们还可以看到一些人想要通过博客、播客或拍摄纪录片来宣传他们的业务，却受困于不知选择什么工具。真正要紧的是宣传的内容。你可以花大手笔购买超级棒的设备，但是如果没有什么内容可表达……那么，你还真没什么可说的。\n就用你现在手头有的或者能负担得起的，然后开始做吧。工具不重要，就用现有的工具也可以做得一样棒，音乐就在你的指尖流淌。\nsource: https://book.douban.com/subject/30184215/\n","date":"2023-11-13T21:00:00+08:00","permalink":"https://jinggqu.github.io/posts/a-few-thoughts/","title":"A Few Thoughts"},{"content":"使用 C#、WPF 与 ONNX 模型实现手写数字识别 前言 项目完整代码见 GitHub 仓库： jinggqu/MLNetDemo。\n本文将介绍如何使用 C#、WPF 与 ONNX 模型实现一个简单的手写数字识别项目。整个流程跑通后，即可应用更加复杂的深度学习模型。\nWPF 界面开发 项目的用户界面相对简单，主要包含一个 InkCanvas（Name 设置为 inkCanvas） 用于用户绘制数字，一个识别按钮与一个清除 Canvas 内容的按钮。软件用户界面如下图所示。\n项目最终效果图如下图所示。\nWPF 界面采用 xaml 文件进行定义，项目界面源代码详见 GitHub 仓库。\nONNX ONNX 简介 ONNX 是一种用于表示机器学习模型的开放格式。ONNX 定义了一组通用运算符（机器学习和深度学习模型的构建基块）和通用文件格式，使 AI 开发人员能够使用具有各种框架、工具、运行时和编译器的模型。详见ONNX 官方网站。\nONNX 模型库及项目模型选用 ONNX Model Zoo 收录了大量的预训练模型，包括计算机视觉领域常用的目标检测、图像分类及自然语言处理领域的 GPT 模型。处于演示需要，本项目选择 ONNX Model Zoo 中的手写数字识别模型作为实验模型。\n查看模型结构 下载 ONNX 模型后，我们还需要找到模型中的输入变量名与输出变量名。因此使用 Netron 来检视模型。Netron 打开 ONNX 模型后的效果如下图所示。\n从图中右侧可以看到，模型输入变量名为 Input3，数据类型为 float 数组，尺寸为 1×1×28×28。输出变量名为 Plus214_Output_0，数据类型为 float 数组，尺寸为 1×10，即为十分类中的每个类别概率值。\n整合模型 Visual Studio 安装深度学习插件 针对不同的平台，可以在 ONNX Runtime 官网选择对应的插件或依赖，详见ONNX Runtime。\n本项目采用 Visual Studio 2022 开发，按照官方说明需要安装 Microsoft.ML.OnnxRuntime，读者可以在 Visual Studio 中解决方案资源管理器中右键单击解决方案名称，选择管理 NuGet 程序包选项，搜索安装 Microsoft.ML.OnnxRuntime。\n除了安装上述插件外，还需要安装微软开发的应用于.Net 平台的机器学习开发包，同时本项目还涉及到图像处理，因此也需要安装图像处理相关的包。故需要安装的所有依赖包如下：\nMicrosoft.ML Microsoft.ML.OnnxRuntime Microsoft.ML.OnnxTransformer System.Drawing.Common 输入输出数据定义 对于输入数据，通过 Netron 得到输入变量名和数据类型后，即可得到如下的输入数据定义。\n1 2 3 4 5 6 public class InputData { [VectorType(1 * 1 * 28 * 28)] [ColumnName(\u0026#34;Input3\u0026#34;)] public float[] Image { get; set; } } 其中 VectorType 用于表征数据尺寸，由于本项目不涉及输入批次，仅为单张图片输入模型，因此第一维的 Batch Size 设置为 1，第二维的通道数也设置为 1。因此省略前两个维度，写成 [VectorType(28 * 28)] 也是可以的。ColumnName 需要与上述 Netron 中展示的输入变量名严格对应。\n对于输出数据，同上可得到如下的数据定义，不再赘述。\n1 2 3 4 5 public class OutputData { [ColumnName(\u0026#34;Plus214_Output_0\u0026#34;)] public float[] Result { get; set; } } C## 图像处理 本项目涉及的图像处理总体流程：\n获取 InkCanvas 的内容并转为位图 Bitmap 将 Bitmap 从原始尺寸变换到到模型输入规定的尺寸 将变换尺寸后的 Bitmap 转为单通道 8 位灰度图 将 Bitmap 对象转为 float 一维数组 获取 InkCanvas 的内容并转为位图 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 private float[] ConvertInkCanvasToFloatArray() { // 获取InkCanvas的大小 int width = (int)inkCanvas.ActualWidth; int height = (int)inkCanvas.ActualHeight; // 创建RenderTargetBitmap RenderTargetBitmap rtb = new RenderTargetBitmap(width, height, 96, 96, PixelFormats.Default); rtb.Render(inkCanvas); // 转换为8位灰度图 FormatConvertedBitmap grayscaleBitmap = new FormatConvertedBitmap(); grayscaleBitmap.BeginInit(); grayscaleBitmap.Source = rtb; grayscaleBitmap.DestinationFormat = PixelFormats.Gray8; // 8-bit grayscale grayscaleBitmap.EndInit(); // 将WPF的BitmapSource转换为System.Drawing.Bitmap Bitmap bitmap; using (MemoryStream outStream = new MemoryStream()) { BitmapEncoder enc = new BmpBitmapEncoder(); enc.Frames.Add(BitmapFrame.Create(grayscaleBitmap)); enc.Save(outStream); bitmap = new Bitmap(outStream); } // 变换 Bitmap 尺寸 Bitmap bmp = ReSizeImage(bitmap, inputWidth, inputHeight); // 通道变换，获取单通道灰度图 Bitmap graybmp = GetGaryImage(bmp); // 将图像转为一维数组并返回 return ConvertBitmapToFloatArray(graybmp); } 变换 Bitmap 尺寸 本项目中画布的尺寸为 336×336，但手写数组识别模型使用的训练数据集为 MNIST，从模型输入数据中可以看到其图像尺寸为 28×28，因此需要变换位图的尺寸。\n1 2 3 4 5 6 7 8 9 private static Bitmap ReSizeImage(Image img, int width, int height) { Bitmap bitmap = new Bitmap(width, height); Graphics g = Graphics.FromImage(bitmap); g.InterpolationMode = InterpolationMode.HighQualityBicubic; g.DrawImage(img, 0, 0, bitmap.Width, bitmap.Height); g.Dispose(); return bitmap; } 通道变换 由于上述过程中生成的图像为 RGB 图像，但本例仅需要单通道灰度图，即与 MNIST 数据集保持一致，因此需要对其进行通道变换。此处选用 GDI+ 的 ColorMatrix 特性实现通道变换，代码参考自ML.NET (。・∀・)ノ 来用 C## 跑机器学习吧!。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 private static Bitmap GetGaryImage(Bitmap src) { float[][] colorMatrix = { new float[] {0.299f, 0.299f, 0.299f, 0, 0}, new float[] {0.587f, 0.587f, 0.587f, 0, 0}, new float[] {0.114f, 0.114f, 0.114f, 0, 0}, new float[] { 0, 0, 0, 1, 0}, new float[] { 0, 0, 0, 0, 1} }; ImageAttributes ia = new ImageAttributes(); ColorMatrix cm = new ColorMatrix(colorMatrix); ia.SetColorMatrix(cm, ColorMatrixFlag.Default, ColorAdjustType.Bitmap); Graphics g = Graphics.FromImage(src); g.DrawImage( src, new Rectangle(0, 0, src.Width, src.Height), 0, 0, src.Width, src.Height, GraphicsUnit.Pixel, ia ); g.Dispose(); return src; } 变换后得到的灰度图如下图所示（28×28，图片显示效果较小）。\n将图像转为一维数组 根据上述分析，输入数据为一维 float 数组，因此还需要将灰度图转换为一维数组。\n1 2 3 4 5 6 7 8 9 10 11 12 13 private float[] ConvertBitmapToFloatArray(Bitmap graybmp) { float[] graydata = new float[inputWidth * inputHeight]; for (int i = 0; i \u0026lt; inputWidth; i += 1) { for (int j = 0; j \u0026lt; inputHeight; j += 1) { System.Drawing.Color rescolor = graybmp.GetPixel(j, i); graydata[(i * inputWidth) + j] = rescolor.R / 255.0f; } } return graydata; } 初始化模型 首先定义两个全局变量 _modelPath 和 _predictionEngine，分别代表 ONNX 的模型存放地址与 TTransformer 模型推理变量。在初始化模型时，加载模型后给模型传入一组空输入参数以创建推理变量。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 private readonly string _modelPath = \u0026#34;../../../assets/mnist.onnx\u0026#34;; private PredictionEngine\u0026lt;InputData, OutputData\u0026gt; _predictionEngine; private void InitializeModel() { MLContext context = new MLContext(); var pipeline = context.Transforms.ApplyOnnxModel(_modelPath); var emptyData = new List\u0026lt;InputData\u0026gt;(); var data = context.Data.LoadFromEnumerable(emptyData); var model = pipeline.Fit(data); _predictionEngine = context.Model.CreatePredictionEngine\u0026lt;InputData, OutputData\u0026gt;(model); } 清除画布 为了清除画布，需要为 WPF 布局中的 Clear Digit 按钮绑定名为 ClearButtonClick 的事件，事件实现如下。其中，numberLabel为显示识别结果的标签，清除画布时需要同时将其置空。\n1 2 3 4 5 private void ClearButtonClick(object sender, RoutedEventArgs e) { inkCanvas.Strokes.Clear(); numberLabel.Text = \u0026#34;\u0026#34;; } 预测结果 在上述图像处理的基础上，调用模型预测结果之前还需要将图像处理结果传给模型，同时为 recognize 按钮绑定 RecognizeDigit。其中，numberLabel为显示识别结果的标签，因此需要将模型预测结果（1×10 个概率值）中最大的概率值所代表的数字，赋值给 numberLabel。\n1 2 3 4 5 private void RecognizeDigit(object sender, RoutedEventArgs e) { var result = _predictionEngine.Predict(new InputData() { Image = ConvertInkCanvasToFloatArray() }); numberLabel.Text = result.Result.ToList().IndexOf((float)result.Result.Max()).ToString(); } 参考文献 ML.NET (。・∀・)ノ 来用 C## 跑机器学习吧! 使用 ML.Net 轻松接入 AI 模型！ Chat GPT 4 Tutorial: Create a Windows Machine Learning UWP application (C#) ","date":"2023-08-18T15:00:00+08:00","permalink":"https://jinggqu.github.io/posts/handwritten-digits-recognition-using-csharp-wpf-onnx/","title":"使用C#、WPF与ONNX模型实现手写数字识别"},{"content":"一个月突破雅思 7.0 笔者使用六月一整个月时间备考，平均每天 4 小时，备考后半程几乎每天都会严格按照雅思考试的时间限制做一套雅思真题。7 月 10 号参加了雅思考试，最终取得如下成绩。\n听力 阅读 写作 口语 总成绩 7.5 7.5 6.0 6.5 7.0 本文将详细介绍个人的备考雅思经历，以及如何在短时间内达到目标分数。\n背景 笔者在 2018 年 6 月通过了大学英语六级考试，获得 524 分（听力 198 分，阅读 202 分，写作和翻译 124 分。写作部分可以说非常拉跨了……），此后再未参加过专业英语考试。笔者平时有观看收听英语视频、影视剧及音乐的习惯，因此单词量基本维持在合理水平，自测单词量保持在 6000~7000 左右。读者可通过以下网站测试单词量，获取自己的词汇大致水平，以便对后续的复习计划做出相应调整。\n扇贝词汇量测试 Preply - Test your English vocabulary 重要前提 雅思，全称 International English Language Testing System，简写为 IELTS，是评估非英语母语人士的英语语言能力的一套国际标准化测试系统。既然它是一套标准化测试系统，那么读者就应当弄清楚系统的运作方式，方能知己知彼百战不殆。\n评分标准 雅思详细的评分标准可见其官方网站：雅思评分标准介绍。\n雅思题型可见官网介绍雅思题型介绍。\n听力与阅读部分的计分规则可见 新东方雅思考试评分标准对照表。\n值得注意的是，通过四部分分数计算雅思总成绩时，遵循四舍五入到最近 0.5 分的原则，例如：\n序号 听力 阅读 写作 口语 总分 平均分 总成绩 1 7 6.5 6 5.5 25 6.25 6.5 2 7 7 6 6.5 26.5 6.625 6.5 3 7.5 7.5 6 6.5 27.5 6.875 7.0 由于这种规则的特殊性，我们可以适当“投机取巧”达到目标分数，如例 1 中的均分仅为 6.25，最终总成绩为 6.5（其实这也是笔者原定的各科目标及总分）。即如果读者需要总成绩 6.5 分，那么实际上达到平均 6.25 即可；类似的，如果需要总成绩 7.0，那么平均分达到 6.75 即可。读者可以通过以下网站计算分数，以便于合理规划各考试部分的目标分数。\nOverall Band Score Calculator IELTS Band Score Calculators 单词量 又到了老生常谈的背单词环节。作为一门语言的基石，单词的重要性不言而喻。关于如何背单词，笔者已有的经验主要包括以下两点：\n采取“大水漫灌”策略。笔者认为，每天仅仅背 50 个甚至 20 个等小数量的单词意义不大，不仅不能有效复习前一天的单词，也不能学习到足量的新词，实属“食之无味，弃之可惜”。笔者在备考期间，每天花费大约 40~60 分钟记单词，每日总单词量约 240 个，其中包含 180 个之前学过的旧词，以及 60 个新词。 配合例句食用。如果仅仅记忆一个孤零零的单词，不仅记忆难度大，也容易遗忘。试想一下，我们在学习汉学到新的字或词后，第一反应都是使用这些词造句，以加深记忆。同理，学习英语单词时，我们也应该记忆配合该单词出现的例句。切勿使用不附带例句的背单词软件或应用。 相关资料与应用 笔者在复习期间使用的相关应用如下：\n背单词：扇贝单词网页版。扇贝单词网页版可使用键盘快捷操作，同时在选择不认识当前单词后，首先跳出来的并不是单词的汉语意思，而是单词例句，这是笔者认为这款产品最优秀的亮点。 真题模拟网页端：新东方雅思机考。没什么好说的，直接吹爆。软件界面和考试真机界面保持一致，且免费使用，模考后不仅自动打分，还可看到答案与解析，方便听力阅读和作文重听与改错。 逐个击破 雅思考试的四个部分包括听说读写，我们可以将其归类为：输入（听力和阅读）与输出（口语和写作）。一般情况下，输出能力建立在充足的输入信息的基础上，因此如何输入以及大量输入至关重要。\n听力 笔者身边很多朋友反馈说听力很是头疼，听不懂别人在说什么，也不知道如何提高。实际上，唯一的办法就是多听。根据笔者自己的一些经验，想要提升听力主要可以分为以下两个方面。\n日常会话听力 日常会话听力是指随机性较强、没有经过特殊编排、且不需要每个字每个词都完全听得清楚的听力内容，例如英语播客、自媒体视频以及流行音乐等。面对这些听力材料，即使我们并不能听懂所有的内容，但仍然可以根据上下文内容进行合理猜测，以便理解大意。同时，这些材料我们也不需要反反复复精听。在日常生活中，笔者基本上每天都会接触一些英语内容，包括自媒体视频、影视剧等内容，主要以兴趣为导向。为抛砖引玉，笔者在此处推荐一些个人常看常听的英语内容：\n哔哩哔哩 - 汤圆学英语 哔哩哔哩 - 瑞秋英语 Rachel 哔哩哔哩 - polyglot_maniac 哔哩哔哩 - 王有菜_ 哔哩哔哩 - LinusTechTips 哔哩哔哩 - Chubbyemu YouTube - BBC Learning English YouTube - Kurzgesagt YouTube - Beau Miles YouTube - Cynthia Zhou 应试听力 相较于日常会话听力的轻松随意，应试听力就显得有些“刻意而为之”。因此，针对应试听力材料，我们应该采用三步走策略，具体步骤如下：\n第一次听：提前快速浏览阅读听力题目与选项，标注题目或选项中的关键词，全神贯注聆听录音并做出选择。做出选择后，除非时间特别富裕或者特别确定选错了，否则不要回头更改答案； 第二次听：重新播放听力材料，重新仔细阅读错题题目和选项，重新播放对应部分听力材料再次作答； 第三次听：在上述两个步骤之后，如果还有错题，那么可以查看答案、听力原文或者题目解析，再次播放对应部分听力材料，在搞懂正确选项的同时总结错听、漏听的原因。 上述三步走策略是笔者从高中考试就一直应用的一种听力备考方法，虽然没能取得特别高的听力分数，但是至少可以确保听力不会成为短板项目，因此建议读者参考并按照自己的实际情况做出调整。\n阅读 与听力类似，阅读也可以采用类似的三步走策略。但是在雅思考试中，总共有三篇文章、40 个题目需要作答，而阅读部分的总时间仅为 60 分钟，这就非常考验我们的阅读能力以及快速寻找信息的能力。\n因此，在有限时间内突破雅思阅读通常依赖关键词法与平行阅读法。通常情况下，不同题型之间可能并不是按照文章段落先后顺序排布，但是每一种题型内部（如 T/F/NG 判断题、关键词填空题、人名材料匹配题等）通常是有序的。因此，我们可以并行多种题型，同时在文中找出与题干相关联的关键词，以达到最大效率。\n平行阅读法及相关资料可见：\n新东方在线雅思阅读技巧：平行阅读法 Simon 雅思阅读课程（请读者务必看一遍，哪怕 2 倍速也要看一遍） 写作 写作是笔者花费时间最多、考得最烂的部分，但还是有一些经验可以分享。写作分为小作文和大作文，小作文是看图表做事实写作，无需发表主观意见，描述客观事实即可；大作文是针对题目所给的话题做议论写作，通常是一些社会话题，例如对大学入学前 GAP 一年的看法、对政府大力投资航空航天的看法等，需要旗帜鲜明地写出观点，并举例以支撑提出的观点。\n时间分配方面，小作文通常安排 20 分钟，大作文 40 分钟。内容方面，小作文最少需要 150 词，大作文最少需要 250 词。根据 Simon 雅思作文思想，笔者的大小作文均采用四段式书写结构，简而言之可以归纳为如下：\n小作文：题干转述（12 句）、总体段（12 句）、主要对比特征一（34 句）、主要对比特征二（34 句）\n大作文：题干转述 + 自己观点（23 句）、正面观点并举例（5 句）、反面观点并举例（5 句）、结论（23 句）\n参考资料方面，笔者主要使用了以下参考资料：\nSimon 雅思作文课程。Simon 的课程详细讲解了作文的主题结构规划以及行文逻辑，一定要多看几遍； 慎小嶷 - 十天突破雅思写作。十天突破雅思写作这本书，给出了大量观点语料，同时还提供了适当的观点例子。书中包含大量的真题作文高分范文，并做了详细的点评分析，建议读者通读一遍，牢记语料库中的观点语料，避免考试时大脑宕机一片空白。 由于笔者作文分不高，因此在做题技巧方面没有太多可分享的内容，但请读者一定要多加练习，并严格遵循考试时间限制。\n口语 口语是最出乎笔者意料的一部分，在备考时笔者曾购买过两次雅思口语模考一对一课程，两次课程的老师都给了笔者口语 5.5，但最终在考试中居然拿到了 6.5，真是不可思议。口语主要分为以下三部分：\n在 Topic 1 中，雅思口语考官会提出多个较为基础的问题，针对每个问题考生都应该简要回答，通常是 2~3 句话。通常围绕你所居住的城市、家乡、学习或工作等常见话题；\n在 Topic 2 中，雅思口语考官会给出一个话题，考生要围绕话题引出故事、回答主要问题、事后感受等内容，回答时间不超过 2 分钟，超时会被考官打断；\n在这一部分，模考老师给了一个回答模板，读者可以参考。\n例如题目：\nDescribe a place away from your home and you want to visit in the future\nYou should say:\nWhere you would like to go\nWhen you would like to go\nWho do you want to go with\nAnd explain why you want to visit the place\n回答模板主要包括以下部分\nIntroduction: I\u0026rsquo;d like to talk about the city away from home that I want to visit in the future \u0026hellip;\nMain points: 回答上述题目中提出的问题\nFuture / Feelings: 描述对未来的规划或事后的感受\n在 Topic 3 中，雅思口语考官会针对 Topic 2 中的话题延申提问，同时还有可能提一些更加宏观的问题，例如儿童应该从传统中学到什么、邻居是否应该互相帮助、人们是否应该多买本国生产的商品等等。\n雅思口语题库通常会在 1、5、9 月更换，在备考口语时应该以当季题库作为基准，熟练掌握必备题，扩展掌握当季新题。在备考时，一定不要背诵答案，要按照话题自己给自己提问，然后按照自己的思路组织语言回答。\n读者可以自行搜索“雅思口语题库”拿到当季题库。例如，2023 年 5~8 月的题库可见新东方在线 - 2023 年雅思口语题库 5 月-8 月完整版（含答案）汇总。\n值得注意的是，不论是机考（视频通话形式）还是真人考官面对面口语考试，可以想象成和一位讲英语的朋友面对面聊聊天，这样就可以快速放松下来。在整个过程中，考生一定要充满自信，同时要和考官保持适当的眼神接触。\n结语 雅思考试分为纸笔考试和上机考试两种模式，纸笔考试出分慢（2~3 周），机考出分快（3 个自然日）。时至今日，除非读者不能熟练操作鼠标和键盘，笔者都建议参加机考。抛开出分的快慢不谈，就只针对机考写作部分可以非常方便地修改调整，并实时显示词数这一优点而言，笔者认为选择机考可以在很大程度上提高雅思作答效率。\n最后，下面这句话送给阅读本文的读者，预祝雅思考试顺利。\n纸上得来终觉浅，绝知此事要躬行\n","date":"2023-07-12T19:00:00+08:00","permalink":"https://jinggqu.github.io/posts/ielts-7.0/","title":"一个月突破雅思 7.0"},{"content":"美团 Java 岗算法笔试记录（2022/08/27） 第一题 题目描述 小美在摆弄她的字符串。最近小团送了小美一个特殊字符 \u0026lsquo;*\u0026rsquo;，这个字符可以和其他所有字符匹配，除了这个字符外，其他字符只能自己和自己匹配。小美先前有一个字符串 S，长度为 n，现在她又新组合了一个可有特殊字符 \u0026lsquo;*\u0026rsquo; 的字符串 s，长度为 m。小美想知道有多少个位置 i，使得 S[i+j] 与 s[j] 对于 1≤j≤m 均能匹配上？其中 X[y] 代表字符串 X 中第 y 位的字符。\n测试样例 第一行两个空格隔开的正整数 n 和 m，分别表示字符串 S 和字符串 s 的长度；\n接下来一行长度为 n 的仅包含小写英文字母的字符串 S；\n接下来一行长度为 m 的包含小写英文字母以及特殊字符 \u0026lsquo;*\u0026rsquo; 的字符串 s；\n对于所有数据，1≤m≤n≤2000，输出一行一个整数，表示满足要求的位置数量\n测试样例 1\n输入\n7 3\nabcaacc\na*c\n输出\n3\n样例 1 解释\n可以对 i=0,3,4 这三个位置的子串 abc、aac、acc 匹配上 a*c，即\nabcaacc\nabcaacc\nabcaacc\n思路与代码 模拟即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 public class Main { public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(), m = in.nextInt(); char[] S = in.next().trim().toCharArray(); char[] s = in.next().trim().toCharArray(); if (n == m) { System.out.println(Arrays.equals(S, s) ? 1 : 0); return; } int res = 0; for (int i = 0; i \u0026lt;= n - m; i++) { if (S[i] == s[0] || s[0] == \u0026#39;*\u0026#39;) { int j = 0; for (; j \u0026lt; m; j++) { if (s[j] == \u0026#39;*\u0026#39;) continue; if (S[i + j] != s[j]) break; } if (j == m) res++; } } System.out.println(res); } } 第二题 题目描述 小美有一个精致的珠宝链子。初始这个链子上有 n 个宝石，从左到右分别编号为 1~n（宝石上的编号不会因为交换位置而改变编号）。接着，小美为了美观对这个项链进行微调，有 m 次操作，每次选择一个编号 x ,将编号 x 的宝石放到最左边（不改变其他宝石的相对位置）。小美想知道，所有操作完成后最终链子从左到右宝石编号是多少。\n测试样例 第一行两个正整数 n 和 m，分别表示链子上的宝石数和操作次数。\n接下来一行 m 个数 x1,x2,\u0026hellip;,xm，依次表示每次操作选择的编号 x 值。\n数字间两两有空格隔开\n对于所有数据，1≤m,n≤50000, 1≤xi≤n，输出一行 n 个整数，表示答案。\n测试样例 1\n输入\n5 3\n2 3 4\n输出\n4 3 2 1 5\n样例 1 解释\n第一次微调完，链子为 2 1 3 4 5\n第二次微调完，链子为 3 2 1 4 5\n第三次微调完，链子为 4 3 2 1 5\n思路与代码 语法题，使用双端队列模拟即可。将指定的宝石从链子中移除，并将其添加到链子头部。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import java.util.ArrayDeque; import java.util.Scanner; public class Main { public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(), m = in.nextInt(); int[] ops = new int[m]; for (int i = 0; i \u0026lt; m; i++) { ops[i] = in.nextInt(); } ArrayDeque\u0026lt;Integer\u0026gt; deque = new ArrayDeque\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; n; i++) { deque.offerLast(i + 1); } for (int op : ops) { deque.remove(op); deque.offerFirst(op); } for (int i = 0; i \u0026lt; n; i++) { System.out.printf(\u0026#34;%d \u0026#34;, deque.removeFirst()); } } } 第三题 题目描述 小团最近获得了美美团团国的裁缝资格证，成为了一个裁缝！现在小团有一个长度为 n 的大布料 S（在这个国家布料其实是一个仅包含小写英文字母的字符串），小团可以将布料在任意位置剪切，例如布料 abcd 可以被裁剪为 a 与 bcd 或 ab 与 cd 或 abc 与 d，不过，裁剪完之后是不能拼接起来的，因为小团还不是很擅长拼接布料。现在小团想知道能不能有一种裁剪方式能让他把布料恰好裁剪成客人要求的小布料。形式化地，有一个串 S，问是否能将其划分成 m 个不相交的连续子串，使得这些连续子串可以与要求的连续子串一一对应。两个串相对应是指这两个串完全相等。例如\u0026quot;aab\u0026quot;=\u0026ldquo;aab\u0026rdquo; 但 \u0026ldquo;aab\u0026rdquo;≠\u0026ldquo;baa\u0026rdquo;。\n测试样例 第一行两个空格隔开的正整数 n 和 m，分别表示大布料 S 长度和客人要求的小布料数量。\n接下来一行一个长度为 n 的仅包含小写英文字母的串 S，表示大布料的组成。\n接下来一行 m 个空格隔开的数 x1,x2, \u0026hellip;,xm，依次表示所要求的小布料长度。\n接下来开始 m 行，每行一个长度为 xi 的仅包含小写英文字母的串 si，表示第 i 个小布料的组成。\n如果存在这样的方案，输出方案总数。如果不存在任何方案，输出 0。\n两个方案 A、B 不相同当且仅当方案 A 中存在一个相对于原始长布料的裁剪位置 i，而方案 B 中并未在该位置 i 裁剪。\n例如 aaaaaa 裁剪方案 aaa|aaa 与方案 aaa|aaa 是相同的方案。而方案 aa|aaaa 与方案 aaaa|aa 是不同的方案，\n虽然划分出的结果都是 aa 与 aaaa，但前者在第二个 a 处进行了裁剪，后者并没有在这里进行裁剪，所以视为不同的裁剪方案。\n测试样例 1\n输入\n6 2\naaaaaa\n4 2\naaaa\naa 输出\n2\n样例 1 解释\n有两种方案，第一种是 aaaa|aa，第二种是 aa|aaaa，代表一次裁剪。\n思路与代码 根据子集/排列思想，本题元素可重但不可复选。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 public class Main { static int res = 0; public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(), m = in.nextInt(); in.nextLine(); String S = in.nextLine().trim(); in.nextLine(); String[] fragments = new String[m]; for (int i = 0; i \u0026lt; m; i++) { fragments[i] = in.next().trim(); } // 如果长度相同，则按照字典序排序，否则按照长度排序 Arrays.sort(fragments, ((o1, o2) -\u0026gt; { if (o1.length() == o2.length()) return o1.compareTo(o2); else return o1.length() - o2.length(); })); backtrack(S, new StringBuilder(), fragments, new boolean[m]); System.out.println(res); } private static void backtrack(String S, StringBuilder sb, String[] fragments, boolean[] used) { if (S.equals(sb.toString())) { res++; return; } for (int i = 0; i \u0026lt; fragments.length; i++) { // 元素可重，但不可复选 if (i \u0026gt; 0 \u0026amp;\u0026amp; fragments[i].equals(fragments[i - 1]) \u0026amp;\u0026amp; used[i - 1]) continue; if (used[i]) continue; sb.append(fragments[i]); used[i] = true; backtrack(S, sb, fragments, used); sb.replace(sb.length() - fragments[i].length(), sb.length(), \u0026#34;\u0026#34;); used[i] = false; } } } 第四题 题目描述 小团正忙着用机器人收衣服！因为快要下雨了，小团找来了不少机器人帮忙收衣服。他有 n 件衣服从左到右成一行排列，所在位置分别为 1~n，在每个位置上已经有一个就绪的机器人可以帮忙收衣服，但第 i 个位置上的机器人需要 pi 的电量来启动，然后这个机器人会用 ti 的时间收衣服，当它收完当前衣服后，会尝试去收紧邻的右边的一件衣服 (如果存在的话)，即 i+1 处的衣服，如果 i+1 处的衣服已经被其他机器人收了或者其他机器人正在收，这个机器人就会进入休眠状态，不再收衣服。不过如果机器人没有休眠，它会同样以 ti 时间来收这件 i+1 处的衣服（注意，不是 ti+1 的时间，收衣服的时间为每个机器人固有属性），然后它会做同样的检测来看能否继续收 i+2 处的衣服，一直直到它进入休眠状态或者右边没有衣服可以收了。形象地来说，机器人会一直尝试往右边收衣服， 收 k 件的话就耗费 k*ti 的时间，但是当它遇见其他机器人工作的痕迹，就会认为后面的事情它不用管了，开始摸鱼，进入休眠状态。小团手里总共有电量 b，他准备在 0 时刻的时候将所有他想启动的机器人全部一起启动，过后不再启动新的机器人， 并且启动的机器人的电量之和不大于 b。他想知道在最佳选择的情况下，最快多久能收完衣服。若无论如何怎样都收不完衣服，输出 -1。\n测试样例 第一行两个正整数 n 和 b，分别表示衣服数量和小团持有电量。\n接下来一行 n 个数 p1,p2, \u0026hellip;,pn，含义如题所述，为机器人唤醒需求电量。\n接下来一行 n 个数 t1,t2, \u0026hellip;,tn，含义如题所述，为机器人收衣服所需时间。\n数字间两两有空格隔开。\n输出最短所需时间。\n测试样例 1\n输入\n3 5\n1 2 3\n7 5 3\n输出\n10\n样例 1 解释\n可以同时启动第一个机器人和第二个机器人，耗电量为 1+2=3，这样花费时间为 max(7, 52)=10\n也可以同时启动第一个机器人和第三个机器人，耗电量为 1+3=4，这样花费时间为 max(72, 3)=14\n所以答案为 10\n测试样例 2\n输入\n3 5\n6 2 3\n7 5 3\n输出\n-1\n样例 2 解释 因为必须要启动第一个机器人，耗电量至少为 6，但是持有电量只有 5，因此无法收完所有衣服，输出 -1\n思路与代码 // TODO\n1 // TODO 第五题 题目描述 小美在回家路上碰见很多缠人的可爱猫猫！因为猫猫太可爱了以及小美十分有爱心，每遇到一只猫猫，小美忍不住停下来花费 T 的时间抚摸猫猫让猫猫不再缠着小美。而一路上小美能捡到很多亮闪闪的小玩具，这里我们给每个小玩具的种类都编了号，从 1~k，一共 k 种小玩具，对于每个所属种类 i 的小玩具，小美可以选择将它送给遇到的一只猫猫玩，这样的话可以只花费 ti 的时间就可以让这只猫猫心满意足的离开。小美想知道，在她以最佳的对小玩具的用法下，她最少耗费多少时间在打发猫猫（即只考虑摸猫时间以及用小玩具打发猫的时间）。注意，每个捡到的小玩具只能用一次。\n测试样例 第一行三个正整数 n、k 和 T，分别表示小美回家遇见的事件数、小玩具种类总数以及摸猫时间！\n接下来一行 k 个数 t1,t2, \u0026hellip;,tk, 含义如题所述，为每种小玩具打发猫猫所用时间。\n接下来一行 n 个数 e1,e2, \u0026hellip;,en，表示 n 次事件，对第 i 次事件，如果 ei=0，则表示遇到了一只猫猫，小美可以选择花费 T 的时间去抚摸，或者用一个小玩具送给猫猫来打发它 (如果小美有的话)。 如果 ei\u0026gt;0，则表示小美在这里捡到了一个小玩具，种类为 ei。初始时候小美身上没有任何小玩具，她可以携带任意多个小玩具。\n输出最少耗费多少时间在打发猫猫（即只考虑摸猫时间以及用小玩具打发猫的时间）。\n测试样例 1\n输入\n6 2 100\n1 50\n0 1 2 0 1 0\n输出\n102\n样例 1 解释\n一开始没有小玩具，遇到一只猫猫只能抚摸，花费了 100 的时间。\n接下来获得了小玩具 1 和 2，然后遇到一只猫猫，用了小玩具 1，只花费了 1 的时间。\n接下来又获得一个小玩具 1 之后又遇见一只猫猫，因为又有小玩具 1 了，所以还是只用花费 1 的时间。\n总共用时 102\n思路与代码 贪心法。使用 PriorityQueue 将所有的玩具打发猫猫的时间存储起来，如果玩具打发猫猫的时间大于抚摸猫猫的时间，则该玩具不要也罢。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 public class Main { public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(), k = in.nextInt(), T = in.nextInt(); int[] t = new int[k], e = new int[n]; for (int i = 0; i \u0026lt; k; i++) { t[i] = in.nextInt(); } for (int i = 0; i \u0026lt; n; i++) { e[i] = in.nextInt(); } int res = 0; PriorityQueue\u0026lt;Integer\u0026gt; timeOfToys = new PriorityQueue\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; n; i++) { if (e[i] == 0) { if (timeOfToys.isEmpty()) res += T; else res += timeOfToys.poll(); } else { if (t[e[i] - 1] \u0026gt; T) continue; timeOfToys.add(t[e[i] - 1]); } } System.out.println(res); } } ","date":"2022-08-29T10:00:00+08:00","permalink":"https://jinggqu.github.io/posts/meituan-java-note-3/","title":"美团 Java 岗算法笔试记录（2022/08/27）"},{"content":"美团 Java 岗算法笔试记录（2022/08/20） 第一题 题目描述 小团想要自己来烤串！不过在烤串之前，需要串好烤串。小团有 n 个荤菜和 n 个素菜，他想按顺序分别一个荤菜一个素菜串起来，想请你帮他串好！给出两个长度分别为 n 的仅包含小写英文字母的串 A 和 B，分别代表荤菜和素菜的种类（用字母来表示菜的种类）。请你以从左到右的顺序依次串好他们！例如对于荤菜串 A1A2\u0026hellip;An 和素菜串 B1B2\u0026hellip;Bn，串好应该是 A1B1A2B2\u0026hellip;AnBn。\n测试样例 第一行一个正整数 n，表示烤串长度； 第二行为一个长度为 n 的字符串 A，表示荤菜按次序都是哪些菜； 第三行为一个长度为 n 的字符串 B，表示素菜按次序都是哪些菜； 对于 80% 的数据，n≤1000，对于 20% 的数据，n≤50000。于所有数据，A 和 B 为仅包含小写英文字母的字符串； 输出一行，包含 2n 个字符串表示串好的烤串。\n测试样例 1\n输入\n4\nabcd\nefgh\n输出\naebfcgdh\n思路与代码 语法题，直接交替插入两个字符串的字符即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public class Main { public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(); in.nextLine(); String A = in.nextLine(); String B = in.nextLine(); StringBuilder builder = new StringBuilder(); for (int i = 0; i \u0026lt; n; i++) { builder.append(A.charAt(i)); builder.append(B.charAt(i)); } System.out.println(builder); } } 第二题 题目描述 小团在地图上放了三个定位装置，想依赖他们来进行定位！小团的地图是一个 n×n 的一个棋盘，他在 (x1,y1),(x2,y2),(x3,y3) xi,yi ∈ Z ∩ [1,n] 这三个位置分别放置了一个定位装置（两两不重叠）。然后小团在一个特定的位置 (a,b)a,b ∈ Z ∩[1,n] 放置了一个信标。每个信标会告诉小团它自身到那个信标的曼哈顿距离，即对 i=1,2,3 小团知道 (|xi-a|+|yi-b|)，现在小团想让你帮他找出信标的位置！注意，题目保证最少有一个正确的信标位置。因为小团不能定位装置确定出来的信标位置是否唯一，如果有多个，输出字典序最小的那个。(a,b) 的字典序比 (c,d) 小，当且仅当 a\u0026lt;c 或者 a==c∧b\u0026lt;d。\n测试样例 第一行一个正整数 n，表示棋盘大小；\n第二行两个整数，分别表示 x1 与 y1，即第一个定位器的位置；\n第三行两个整数，分别表示 x2 与 y2，即第二个定位器的位置；\n第四行两个整数，分别表示 x3 与 y3，即第三个定位器的位置；\n第五行三个整数，分别表示第一、二、三个定位器到信标的曼哈顿距离。第 i 个定位器到信标的曼哈顿距离即 (|xi-a|+|yi-b|)；\n数字间两两有空格隔开，对于所有数据，n≤50000, 1≤xi,yi≤n，输出一行两个整数，表示字典序最小的可能的信标位置。\n测试样例 1\n输入\n3 2 1 2 2 2 3 2 1 2\n输出\n1 2\n样例 1 解释\n与 (2, 1) 的哈曼顿距离为 2 的位置有三个，分别是 (1, 2), (2, 3), (3, 2)\n与 (2, 2) 的哈曼顿距离为 1 的位置有四个，分别是 (1, 2), (2, 1), (2, 3), (3, 2)\n与 (2, 3) 的哈曼顿距离为 2 的位置有三个，分别是 (1, 2), (2, 1), (3, 2)\n所以只有 (1, 2), (3, 2) 这两个位置有可能是信标，而 (1, 2) 的字典序最小，所以输出 (1, 2)\n思路与代码 曼哈顿距离即水平和竖直方向上的距离之和。使用 directions 表示方向，对距离 $d$ 进行遍历，若水平方向取距离 $d_x$，则竖直方向距离 $d_y = d - d_x$，枚举出所有点后判断点是否在边界内，若在则有效。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 public class Main { static int[][] directions = {{1, -1}, {1, 1}, {-1, 1}, {-1, -1}}; public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(); int x1 = in.nextInt(), y1 = in.nextInt(); int x2 = in.nextInt(), y2 = in.nextInt(); int x3 = in.nextInt(), y3 = in.nextInt(); int d1 = in.nextInt(), d2 = in.nextInt(), d3 = in.nextInt(); Set\u0026lt;String\u0026gt; set1 = getAdjoin(n, x1, y1, d1); Set\u0026lt;String\u0026gt; set2 = getAdjoin(n, x2, y2, d2); Set\u0026lt;String\u0026gt; set3 = getAdjoin(n, x3, y3, d3); PriorityQueue\u0026lt;int[]\u0026gt; queue = new PriorityQueue\u0026lt;\u0026gt;((a, b) -\u0026gt; { if (a[0] == b[0]) return a[1] - b[1]; else return a[0] - b[0]; }); for (String s : set1) { if (set2.contains(s) \u0026amp;\u0026amp; set3.contains(s)) { String[] pair = s.split(\u0026#34;-\u0026#34;); queue.offer(new int[]{Integer.parseInt(pair[0]), Integer.parseInt(pair[1])}); } } int[] pair = queue.poll(); System.out.println(pair[0] + \u0026#34; \u0026#34; + pair[1]); } static Set\u0026lt;String\u0026gt; getAdjoin(int n, int x, int y, int d) { Set\u0026lt;String\u0026gt; set = new HashSet\u0026lt;\u0026gt;(); for (int i = d; i \u0026gt;= 0; i--) { for (int[] dir : directions) { int newX = x + i * dir[0], newY = y + (d - i) * dir[1]; if (newX \u0026gt;= 1 \u0026amp;\u0026amp; newX \u0026lt;= n \u0026amp;\u0026amp; newY \u0026gt;= 1 \u0026amp;\u0026amp; newY \u0026lt;= n) set.add(newX + \u0026#34;-\u0026#34; + newY); } } return set; } } 第三题 题目描述 小美即将进行期末考试！小美现在盘算了一下，一共有 n 道试题，对于第 i 道试题，小美有着 pi 的概率做对，获得 ai 的分值，另外 (1-pi) 的概率做错，获得 0 分。小美的总分即是每道题获得的分数之和。小美不甘于此！她决定突击复习，因为时间有限，她最多复习 m 道试题，使得复习后的试题正确率提升到 100%。小美想知道，如果她以最佳方式进行复习，能获得的期望总分最大是多少。\n测试样例 第一行两个正整数 n 和 m，表示总试题数和最多复习试题数。\n接下来一行 n 个整数，分别为 p1 p2\u0026hellip;pn，表示小美有 pi%的概率，即 pi=pi/100 的概率做对第 i 个题。（注意，这里为了简单起见，将概率 pi 扩张 100 倍成为整数 pi 方便输入）\n接下来一行 n 个整数，分别表示 a1 a2\u0026hellip;an，分别表示第 i 个题做对的分值。\n数字间两两有空格隔开，对于所有数据，1≤m≤n≤50000,0≤pi≤100,1≤ai≤1000\n输出一行一个恰好两位的小数，表示能获得的最大期望总分。（如果答案为 10 应输出 10.00，2.5 应输出 2.50）\n测试样例 1\n输入\n2 1\n89 38\n445 754 输出\n1150.05\n样例 1 解释\n如果都不复习，小美总分的期望为 89% * 445 + 38% * 754 = 682.57\n如果复习第一道题，小美总分的期望为 100% * 445 + 38% * 754 = 731.52\n如果复习第二道题，小美总分的期望为 89% * 445 + 100% * 754 = 1150.05\n所以选择复习第二道题，这样能获得最大期望总分 1150.05\n根据每题复习后的收益进行排序即可\n思路与代码 自定义数据结构 Pair，使用 PriorityQueue 对 Pair 按照收益倒序排列，即收益最大的课排在前。再通过贪心法，选取前 $m$ 门收益最大的课进行复习，剩余 $n-m$ 门课则不复习。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class Main { static class Pair { int a; // 满分 double diff; // 满分与不复习得分的差值（收益） public Pair(int a, double diff) { this.a = a; this.diff = diff; } } public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(), m = in.nextInt(); int[] p = new int[n], a = new int[n]; PriorityQueue\u0026lt;Pair\u0026gt; queue = new PriorityQueue\u0026lt;\u0026gt;((o1, o2) -\u0026gt; { return o2.diff - o1.diff; }); for (int i = 0; i \u0026lt; n; i++) { p[i] = in.nextInt(); } for (int i = 0; i \u0026lt; n; i++) { a[i] = in.nextInt(); } for (int i = 0; i \u0026lt; n; i++) { queue.offer(new Pair(a[i], a[i] * (1 - p[i] / 100.0))); } double res = 0; for (int i = 0; i \u0026lt; n; i++) { Pair pair = queue.poll(); if (i \u0026lt; m) { res += pair.a; } else { res += pair.a - pair.diff; } } System.out.println(res); } } 第四题 题目描述 小团生日收到妈妈送的两个一模一样的数列作为礼物！他很开心的把玩，不过不小心没拿稳将数列摔坏了！现在他手上的两个数列分别为 A 和 B，长度分别为 n 和 m。小团很想再次让这两个数列变得一样。他现在能做两种操作，操作一是将一个选定数列中的某一个数 a 改成数 b，这会花费|b-a|的时间，操作二是选择一个数列中某个数 a，将它从数列中丢掉，花费|a|的时间。小团想知道，他最少能以多少时间将这两个数列变得再次相同！\n测试样例 第一行两个空格隔开的正整数 n 和 m，分别表示数列 A 和 B 的长度。\n接下来一行 n 个整数，分别为 A1 A2\u0026hellip;An\n接下来一行 m 个整数，分别为 B1 B2\u0026hellip;Bm\n对于所有数据，1≤n,m≤2000， |Ai|,|Bi|≤10。输出一行一个整数，表示最少花费时间，来使得两个数列相同。\n测试样例 1\n输入\n1 1\n-9821\n7742\n输出\n17563\n样例 1 解释\n可以选择两次第二种操作，消除数列 A 的第一个数和数列 B 的第一个数，需要花费 9821+7742=17563 的时间\n也可以选择一次第一种操作，将数列 A 的第一个数改成数列 B 的第一个数，也是需要花费 9821+7742=17563 的时间\n所以答案为 17563\n思路与代码 动态规划，与 leetcode 72.编辑距离 类似，但要注意设置本题的边界条件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class Main { public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(), m = in.nextInt(); int[] A = new int[n]; int[] B = new int[m]; for (int i = 0; i \u0026lt; n; i++) { A[i] = in.nextInt(); } for (int i = 0; i \u0026lt; m; i++) { B[i] = in.nextInt(); } int[][] dp = new int[n + 1][m + 1]; // 丢掉 A[i] for (int i = 1; i \u0026lt;= n; i++) { dp[i][0] = dp[i - 1][0] + Math.abs(A[i - 1]); } // 丢掉 B[i] for (int i = 1; i \u0026lt;= m; i++) { dp[0][i] = dp[0][i - 1] + Math.abs(B[i - 1]); } for (int i = 1; i \u0026lt;= n; i++) { int a = A[i - 1]; for (int j = 1; j \u0026lt;= m; j++) { int b = B[j - 1]; if (a == b) { dp[i][j] = dp[i - 1][j - 1]; } else { dp[i][j] = Math.min(dp[i - 1][j - 1] + Math.abs(b - a), Math.min(dp[i - 1][j] + Math.abs(a), dp[i][j - 1] + Math.abs(b))); } } } System.out.println(dp[n][m]); } } 第五题 题目描述 小团的玩具火箭有点磨损了，上面有很多地方翘起来了，小团想要用强力胶进行修补，但在强力胶凝结之前，需要找点东西压住。幸好小团有很多这样的东西。小团有 m 种配重材料，第 i 种材料重 ai 单位重量（因为小团有太多了，可以认为每种都有任意多个）。火箭上有 n 个地方翘起来了，需要至少 bi 单位重量的东西来压住，而且只能用一个配重材料来压，(多了的话不好压，多个配重材料容易散开，所以小团不想用多个来折腾)。小团想一次就把所有翘起来的地方全都修补好，请问他需要使用的配重材料重量之和最少是多少？\n测试样例 第一行两个正整数 n 和 m，分别代表需要修补的地方个数以及材料种类数。\n接下来一行 n 个数 b1,b2,\u0026hellip;,bn，含义如题。\n接下来一行 m 个数 a1,a2,\u0026hellip;,am，含义如题。\n对于 40% 的数据，n,m≤100，对于另外 30% 的数据，n,m≤2000，对于所有数据，1≤n,m≤50000，1≤ai,bi≤104，输出小团最少需要使用的配重材料重量之和。如果没有任何办法满足，输出-1。\n测试样例 1\n输入\n1 1\n5\n4\n输出\n-1\n样例 1 解释\n需要 5 单位重量，只有 4 单位重量的材料，压不住，输出-1。\n测试样例 2\n输入\n3 3\n4 1 3\n4 2 1\n输出\n9\n样例 2 解释\n第一个地方需要重量为 4 的，第二个地方可以用重量为 1 的，第三个地方只能选择重量为 4 的才能压住。所以总重量需求为 9。可以证明没有更优方案。\n思路与代码 利用 TreeSet 的特性，遍历每个需要的重量 $b_i$，通过 ceiling(bi) 方法得到大于等于 $b_i$ 的最小值，若不存在这样的值则说明压不住，返回-1。或者对材料重量与所需重量排序后，基于上述思路二重遍历即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class Main { public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(), m = in.nextInt(); int[] b = new int[n]; TreeSet\u0026lt;Integer\u0026gt; ts = new TreeSet\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; n; i++) { b[i] = in.nextInt(); } for (int i = 0; i \u0026lt; m; i++) { ts.add(in.nextInt()); } long res = 0; for (int bi : b) { Integer ceiling = ts.ceiling(bi); if (ceiling == null) { System.out.println(-1); return; } else { res += ceiling; } } System.out.println(res); } } 参考文献 1.08/20 美团后端笔试\n2.20220820 美团笔试题解\n","date":"2022-08-24T10:00:00+08:00","permalink":"https://jinggqu.github.io/posts/meituan-java-note-2/","title":"美团 Java 岗算法笔试记录（2022/08/20）"},{"content":"联想面试记录 投递岗位 Java 开发工程师-China Geo\n一面（2022/08/16） 面试官自我介绍及团队业务介绍； 自我介绍； 项目相关问题； 逻辑题，在直角坐标系第一象限给定两个矩形，如何求出其相交面积？直播讲思路，写伪码； 线程的生命周期； 线程怎么跑起来的，为什么一个线程可以执行？ 线程都有哪些状态？什么时候 wait()，什么时候 sleep()？在写代码的时候会主动调用 wait() 吗？ Java 中常用的集合都有哪些？ HashMap 是什么结构？ HashMap 怎么实现的键值对？key 和 value 是什么结构？ HashMap 底层是怎么实现的； HashMap 为什么要使用红黑树； 什么是 O(1)、O(n)； 手写快排； 做项目的流程，在做项目的过程中有没有遇到问题； 是否可以接受工作中编码只占一小部分的情况； 评价与结语。 二面（2022/08/23） 自我介绍； 介绍项目，对负责的部分做详细的介绍； 从 Java 层面，如何做跨语言的程序调用； 假设有两台 Java 服务，如何互相做代码调用； 项目中常用的开发工具以及开发包都有什么？（Python 和 Java） 好的应用设计应该遵循什么标准； 常用的设计模式； 上述设计模式在哪些场景下使用； 代理模式在 Spring 里是怎样工作的； 常规 MVC 模式与 Spring MVC 模式有什么异同？ 项目中遇到技术难题，通过什么方式解决； 如何快速学习一门新的技术；（基于实践学习，自顶而下） 上述学习过程中，如何掌握新技术的基础知识； 讲讲 Java 内存回收；（对象生命周期分类，判断对象死亡方法，垃圾回收方法，垃圾回收器） 在实际项目中，应该在什么时候去设置 Java 堆栈大小？ 线下及线上服务的调试方法都有哪些，以及相应的工具； 未来的职业规划； 有没有什么想问的？（ChinaGeo 与 SSG 的区别，剩下几次面试，本次面试表现） ","date":"2022-08-16T19:00:00+08:00","permalink":"https://jinggqu.github.io/posts/lenovo-java-interview/","title":"联想面试记录"},{"content":"美团 Java 岗算法笔试记录（2022/08/13） 第一题 题目描述 炸鸡店拥有一名会传送魔法的外卖派送员。该外卖派送员派送单子时，可以消耗时间 t 来正常派送单子（一次只能派送一个单子，不能多个同时派送），也可以使用魔法不耗费时间地隔空瞬间投送。现在炸鸡店在时刻 0 接收到了若干炸鸡订单，每个单子都有它的截止送达时间。外卖派送员需要保证送达时间小于等于这个截止时间。现在询问外卖员最少要使用几次魔法来保证没有外卖超时。\n测试样例 测试样例 1\n输入\n6 5\n5 6 7 8 9 10\n输出\n3\n测试样例 2\n输入\n6 5\n100 101 102 103 104 105\n输出\n0\n思路与代码 贪心算法。先排序，然后看看每一次送达时间是否能在截止时间前。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public static void main(String[] args) throws IOException { Scanner in = new Scanner(System.in); int n = in.nextInt(), t = in.nextInt(), time = 0; int[] deadline = new int[n]; for (int i = 0; i \u0026lt; n; i++) deadline[i] = in.nextInt(); Arrays.sort(deadline); int res = n; for (int i = 0; i \u0026lt; n; i++) { if (deadline[i] \u0026gt;= time) { res--; time += t; } } System.out.println(res); } 第二题 题目描述 你买了一个扫地机器人，你想要知道这个扫地机器人是否能够将房间打扫干净。为了简化问题，我们不妨假设房间被划分为 n*m 的方格。定义打扫干净为这 n*m 的方格全部被打扫过至少一次。你为扫地机器人下达了若干指令。每个指令为上下左右移动中的一种。机器人会将经过的路径上的方格打扫干净。初始时假设机器人处于第一行第一列的方格中。这个方格初始时会被机器人直接打扫干净。现在询问你机器人能否将房间打扫干净，能则输出 Yes，不能则输出 No。对于 Yes 的情况下，还要求你继续输出到哪个指令结束后，房间就打扫干净了。对于 No 的情况下，还要求你输出还有多少个地块没有打扫干净。保证机器人在打扫的过程中不会越过房间边界。换句话说机器人始终保持在 n*m 的方格图中。\n测试样例 测试样例 1\n输入\n2 2 5\n\u0026ldquo;SDWAS\u0026rdquo;\n输出\n\u0026ldquo;Yes\u0026rdquo;\n3\n测试样例 2\n输入\n2 2 5\n\u0026ldquo;SWSWS\u0026rdquo;\n输出\n\u0026ldquo;No\u0026rdquo;\n2\n思路与代码 暴力解法即可。无须考虑边界问题，因为题目已经保证机器人不会越界。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(), m = in.nextInt(), total = in.nextInt(); char[] orders = in.next().toCharArray(); boolean[][] cleaned = new boolean[n][m]; cleaned[0][0] = true; int x = 0, y = 0, remain = n * m - 1; for (int i = 0; i \u0026lt; total; i++) { char order = orders[i]; if (order == \u0026#39;W\u0026#39;) { x--; } else if (order == \u0026#39;A\u0026#39;) { y--; } else if (order == \u0026#39;S\u0026#39;) { x++; } else if (order == \u0026#39;D\u0026#39;) { y++; } if (!cleaned[x][y]) { remain--; cleaned[x][y] = true; } if (remain == 0) { System.out.println(\u0026#34;Yes\u0026#34;); System.out.println(i + 1); return; } } System.out.println(\u0026#34;No\u0026#34;); System.out.println(remain); } 第三题 题目描述 Alice 和 Bob 在玩一个游戏。有 n 张卡牌，点数分别为 1 到 n。进行洗牌后，n 张牌从上到下叠放形成一个牌堆。每次 Alice 先将当前牌堆顶的一张牌放到牌堆底，然后 Bob 再将当前牌堆顶的一张牌放到牌堆底。（特别地，当牌堆中只有一张牌时，相当于不进行任何操作）接着，他们会翻开当前牌堆顶的牌，并记下它的点数。当所有牌都被翻开后，他们也记下了 n 个点数。现在他们想根据记下的这个序列来还原一开始的牌（从牌堆顶到牌堆底每一张牌的点数）。\n测试样例 测试样例 1\n输入\n4\n1 2 3 4\n输出\n4 2 1 3\n样例 1 解释\n初始牌堆为：4 2 1 3\nAlice 和 Bob 分别操作后牌堆为：1 3 4 2，此时 1 被翻开，牌堆变为 3 4 2 Alice 和 Bob 分别操作后牌堆为：2 3 4，此时 2 被翻开，牌堆变为 3 4 Alice 和 Bob 分别操作后牌堆为：3 4，此时 3 被翻开，牌堆变为 4 Alice 和 Bob 分别操作后牌堆依旧为 4，此时 4 被翻开。 思路与代码 模拟倒推即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(); int[] poker = new int[n]; for (int i = 0; i \u0026lt; n; i++) { poker[i] = in.nextInt(); } LinkedList\u0026lt;Integer\u0026gt; list = new LinkedList\u0026lt;\u0026gt;(); // 题干操作顺序 // for (int i = 0; i \u0026lt; n; ++i) { // list.addLast(list.removeFirst()); // list.addLast(list.removeFirst()); // poker[i] = list.removeFirst(); // System.out.print(poker[i] + \u0026#34; \u0026#34;); // } for (int i = n - 1; i \u0026gt;= 0; i--) { list.addFirst(poker[i]); list.addFirst(list.removeLast()); list.addFirst(list.removeLast()); } for (int i = 0; i \u0026lt; n; i++) { System.out.printf(\u0026#34;%d \u0026#34;, list.get(i)); } } 第四题 题目描述 给一个长度为 n 的序列 a[1], a[2], …, a[n]，请问有多少个三元组 (i,j,k) 满足 i\u0026lt;j\u0026lt;k 且 a[i]-a[j]=2a[j]-a[k]？输出符合要求的三元组的数量。\n测试样例 测试样例 1\n输入\n4\n4 2 2 2\n输出\n3\n思路与代码 直接暴力解即可，注意数据范围（使用 long 而不是 int）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(); long[] nums = new long[n]; HashMap\u0026lt;Long, List\u0026lt;Integer\u0026gt;\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; nums.length; i++) { nums[i] = in.nextInt(); List\u0026lt;Integer\u0026gt; index = map.getOrDefault(nums[i], new ArrayList\u0026lt;\u0026gt;()); index.add(i); map.put(nums[i], index); } int count = 0; for (int i = 0; i \u0026lt; n; i++) { for (int j = i + 1; j \u0026lt; n; j++) { long target = -1 * (nums[i] - 3 * nums[j]); List\u0026lt;Integer\u0026gt; index = map.get(target); if (index != null) { for (Integer k : index) { if (k \u0026gt; j) count++; } } } } System.out.println(count); } 第五题 题目描述 给一棵有 n 个节点的二叉树，节点的编号从 1 到 n。其中，节点 k 的左儿子为节点 2*k（当 2*k 大于 n 时，不存在左儿子），节点 k 的右儿子为节点 2*k+1（当 2*k+1 大于 n 时，不存在右儿子），该二叉树的根节点为节点 1。对于每个节点，节点上有一些金钱。现在你可以从根节点 1 开始，每次选择左儿子或者右儿子向下走，直到走到叶子节点停止，并获得你走过的这些节点上的金钱。你的任务是求出你可以获得的最大的金钱总量。\n测试样例 第一行是一个正整数 n，表示二叉树上总共有 n 个节点。第二行有 n 个正整数，第 i 个正整数表示节点 i 上有多少数量的金钱。1 \u0026lt;= n \u0026lt;= 100000。对所有数据保证：单个节点上的金钱数量在 [1, 1000] 之间。\n测试样例 1\n输入\n3\n5 7 8\n输出\n13\n样例解释 1\n该样例中，二叉树上有三个节点，根节点为 1 号节点，其左儿子为 2 号节点，右儿子为 3 号节点，所能获取的最大金钱为 13，为从 1 号节点走到 3 号节点，共获得 5 + 8 = 13 的金钱。\n测试样例 2\n输入\n5\n863 163 396 428 90\n输出\n1454\n思路与代码 与 leetcode 124 类似，采用递归的思路，针对根节点 root，分别求得其左右子树中的路径之和较大者，再加上当前根节点 root 节点值（形成一个单臂，即根节点 + 左子节点，或根节点 + 右子节点），最后返回到上一层递归即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(); int[] arr = new int[n + 1]; for (int i = 1; i \u0026lt;= n; i++) { arr[i] = in.nextInt(); } System.out.println(maxMoney(1, n, arr)); } public static int maxMoney(int k, int n, int[] arr) { int left, right; // 获取左子树最大值，若没有左子树，则置为 0 if (2 * k \u0026lt;= n) left = maxMoney(2 * k, n, arr); else left = 0; // 获取右子树最大值，若没有左子树，则置为 0 if (2 * k + 1 \u0026lt;= n) right = maxMoney(2 * k + 1, n, arr); else right = 0; // 返回左右子树最大值与根节点之和 return Math.max(left, right) + arr[k]; } 参考文献 1.美团 | 后端笔试第二场 | 8/13\n","date":"2022-08-15T10:00:00+08:00","permalink":"https://jinggqu.github.io/posts/meituan-java-note-1/","title":"美团 Java 岗算法笔试记录（2022/08/13）"},{"content":"LibTorch 上手教程 前言 LibTorch 简介 在 Python 深度学习圈，PyTorch 具有举足轻重的地位。同样的，C++ 平台上的 LibTorch 作为 PyTorch 的纯 C++ 接口，它遵循 PyTorch 的设计和架构，旨在支持高性能、低延迟的 C++ 深度学习应用研究。本文基于 Windows 环境与 Visual Studio 2019 开发工具，将从零开始搭建一个完整的深度学习开发环境，包括环境配置、项目演示、自定义数据集及问题排查等部分。\nLibTorch 安装 本文使用的 LibTorch 版本为 LTS(1.8.2) CPU 版，若需要使用 GPU 版，也可以在官方网站下载。\n环境配置 创建项目 首先，在 Visual Studio 中创建一个名为 libtorch-toturial 的控制台项目。创建完成后，将项目设置为 Release 模式，x64 平台，如下图。\n配置 LibTorch 依赖 本文中 LibTorch 解压后的存放目录为 D:\\Software\\libtorch-lts，后续配置过程中，读者请按照自己实际情况进行相关设置。\n在 Visual Studio 中，点击 项目 -\u0026gt; libtorch-toturial 项目属性，在左侧导航栏中找到 VC++ 目录 选项。在右侧的 包含目录 选项中将 LibTorch include 目录添加进去，详细如下。\n1 2 D:\\Software\\libtorch-lts\\include D:\\Software\\libtorch-lts\\include\\torch\\csrc\\api\\include 接着找到 库目录 选项，将 LibTorch lib 目录添加进去，详细如下。\n1 D:\\Software\\libtorch-lts\\lib 配置结果如下图，注意检查窗口顶栏 配置 是否为 Release，平台 是否为 x64。\n然后找到 链接器 -\u0026gt; 输入 -\u0026gt; 附加依赖项 选项，在其中填入 LibTorch lib 路径下（即 D:\\Software\\libtorch-lts\\lib）所有 *.lib 文件的文件名，详细如下。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 asmjit.lib c10.lib c10d.lib caffe2_detectron_ops.lib caffe2_module_test_dynamic.lib clog.lib cpuinfo.lib dnnl.lib fbgemm.lib fbjni.lib gloo.lib libprotobuf-lite.lib libprotobuf.lib libprotoc.lib mkldnn.lib pthreadpool.lib pytorch_jni.lib torch.lib torch_cpu.lib XNNPACK.lib 最后，将 D:\\Software\\libtorch-lts\\lib 路径下所有的 *.dll 文件拷贝至 项目路径 -\u0026gt; x64 -\u0026gt; Release 路径下，如下图。\n示例程序 至此，开发环境搭建就已经完成了。我们可以通过运行以下示例程序，来检验上述配置是否正确。若输出如图中所示，则配置无误。\n1 2 3 4 5 6 7 #include \u0026lt;torch/torch.h\u0026gt; #include \u0026lt;iostream\u0026gt; auto main() -\u0026gt; int { auto array = torch::rand(10); std::cout \u0026lt;\u0026lt; array \u0026lt;\u0026lt; std::endl; } 手写数字识别 数据准备 本节将以深度学习经典案例——手写数字识别来演示 LibTorch 的使用。首先需要下载 mnist 手写数字数据集，你可以在这里下载，下载完成后将其解压到 libtorch-toturial.cpp 同一目录 data 文件夹下，目录结构如下。\n1 2 3 4 5 6 7 8 9 ├─libtorch-toturial │ │ libtorch-toturial.cpp │ │ ... │ ├─data │ │ t10k-images-idx3-ubyte │ │ t10k-labels-idx1-ubyte │ │ train-images-idx3-ubyte │ │ train-labels-idx1-ubyte │ ... 源代码 手写数字识别的源代码可以在 LibTorch 官方示例 中找到，请将其拷贝到项目的 libtorch-toturial.cpp 中。\n结果 与 PyTorch 类似，LibTorch 创建深度学习应用同样包含与其相似的步骤：定义网络、初始化网络、加载数据集、训练、验证及保存模型等，详细代码可以参照上述官方示例，此处不再赘述。训练 10 个 epoch 之后，识别准确率已经达到了 98.4%.\n自定义数据集 在本节中，我们将介绍如何将已有的数据集读取到神经网络中，生成 PyTorch 张量。在这之前，需要先介绍 NumCpp 工具，它可以大幅提升数据处理的效率。\nNumCpp 简介与配置 在 Python 开发环境中，最常用的工具非 NumPy 莫属，因其极为便捷高效的特性被开发者广为使用。同样的，在 C++ 平台上，也有开发者开发出了一款与 NumPy 体验“几乎一致”的 NumCpp ———— Python NumPy 库的模板头文件 C++ 实现[2]。\n由于 NumCpp 依赖 Boost 库，因此在配置 NumCpp 之前，需要先配置 Boost 库。相关文件可以在 Boost 官方网站 与 NumCpp Github 页面 进行下载。\n与 LibTorch 配置过程类似，我们需要在 Visual Studio 项目属性中找到 VC++ 目录 -\u0026gt; 包含目录 选项，将 Boost 库与 NumCpp 库的路径添加进去，具体路径如下。\n1 2 D:\\Software\\boost D:\\Software\\NumCpp\\include 然后即可使用下述程序片段进行检查是否配置正确，若成功运行并生成了 3x4 个浮点随机数，则说明配置无误。\n1 2 3 4 5 6 7 #include \u0026#34;NumCpp.hpp\u0026#34; #include \u0026lt;iostream\u0026gt; auto main() -\u0026gt; int { auto array = nc::random::randN\u0026lt;double\u0026gt;({ 3, 4 }); std::cout \u0026lt;\u0026lt; array \u0026lt;\u0026lt; std::endl; } 接下来可以使用 NumCpp 读取本地数据集，由于 NumCpp 缺少类似于 NumPy 的 loadtxt() 方法，故只能使用 fromfile()方法，具体代码如下。\n1 auto input_data = nc::fromfile\u0026lt;double\u0026gt;(input_filepath, /*sep=*/\u0026#39;,\u0026#39;); 假设数据实际尺寸为 m×n，读取到的数据形状为 1×(m×n)，所以还需要进行 reshape() 才可以正常使用。行切片与列切片也和 NumPy 类似，代码如下。\n1 2 3 4 5 6 7 input_data = input_data.reshape(m, n); // 行切片，形如 input_data = input_data[0:2, :] input_data = input_data(nc::Slice(0, 2), input_data.cSlice()); // 列切片，形如 input_data = input_data[:, :2] input_data = input_data(input_data.rSlice(), nc::Slice(0, 2)); 若要进行矩阵与矩阵的计算，则需要保证矩阵的尺寸一致。若不一致，则可以使用 tile() 方法进行扩充，示例代码如下。\n1 2 3 4 5 6 7 8 // 按列求均值，得到的矩阵为 1×n auto input_mean = nc::mean(input_data, nc::Axis::ROW); // 按列求标准差，得到的矩阵为 1×n auto input_std = nc::stdev(input_data, nc::Axis::ROW); // 归一化，将 input_mean 与 input_std 扩充为 m×n，再进行操作 input_data = (input_data - nc::tile(input_mean, { input_data.numRows(), 1 })) / nc::tile(input_std, { input_data.numRows(), 1 }); 自定义数据集 要实现自定义数据集，首先要继承 torch::data::Dataset\u0026lt;CustomDataset\u0026gt; 类，实现 CustomDataset() 构造方法、 get() 方法与 size() 方法。示例代码如下。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class CustomDataset : public torch::data::Dataset\u0026lt;CustomDataset\u0026gt; { private: std::vector\u0026lt;torch::Tensor\u0026gt; source, target; public: // 构造函数 CustomDataset(nc::NdArray\u0026lt;double\u0026gt; input_data, nc::NdArray\u0026lt;double\u0026gt; output_data, std::string data_type) { // 一些数据读取、处理工作。最后得到的 source 与 target 是输入与输出数据的集合 // 如果要对数据集进行划分，可以在此处声明一个方法进行详细处理 source = process_data(input_data, data_type); target = process_data(output_data, data_type); }; // 复写 get() 方法以返回第 index 个位置的张量（输入与输出） torch::data::Example\u0026lt;\u0026gt; get(size_t index) override { torch::Tensor sample_source = source.at(index); torch::Tensor sample_target = target.at(index); return { sample_source.clone(), sample_target.clone() }; }; // 返回数据的数量 torch::optional\u0026lt;size_t\u0026gt; size() const override { return source.size(); }; }; 接下来调用 CustomDataset() 生成 data loader。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // 训练数据 auto train_dataset = CustomDataset(input_data, output_data, \u0026#34;train_data\u0026#34;) .map(torch::data::transforms::Stack\u0026lt;\u0026gt;()); const size_t train_dataset_size = train_dataset.size().value(); std::cout \u0026lt;\u0026lt; \u0026#34;train data size = \u0026#34; \u0026lt;\u0026lt; train_dataset_size \u0026lt;\u0026lt; std::endl; // 训练集 data loader auto train_loader = torch::data::make_data_loader(std::move(train_dataset), train_batch_size); // 验证数据 auto validate_dataset = CustomDataset(input_data, output_data, \u0026#34;validate_data\u0026#34;) .map(torch::data::transforms::Stack\u0026lt;\u0026gt;()); const size_t validate_dataset_size = validate_dataset.size().value(); std::cout \u0026lt;\u0026lt; \u0026#34;validate data size = \u0026#34; \u0026lt;\u0026lt; validate_dataset_size \u0026lt;\u0026lt; std::endl; // 验证集 data loader auto validate_loader = torch::data::make_data_loader(std::move(validate_dataset), validate_batch_size); 与手写数字识别示例类似，在调用 train() 训练方法和 validate() 验证方法时，直接将 data loader 传入即可，代码示例如下。\n1 2 3 4 for (size_t epoch = 1; epoch \u0026lt;= kNumberOfEpochs; ++epoch) { train(epoch, model, device, *train_loader, optimizer, train_dataset_size); validate(model, device, *validate_loader, validate_dataset_size); } 疑难排查 网络浮点数精度 由于上述教程中使用 NumCpp 来读取数据，得到的数据集数据类型为泛型中指定的类型。LibTorch 网络初始化后的数据类型默认为 float(float32)，若我们读取的数据类型为 double(float64) 型，则需要手动将网络数据类型指定为 double，否则程序将会抛出异常[3]。\n1 2 Net model = Net(); model-\u0026gt;to(device, torch::kDouble); 模型保存再读取异常 当读取本地保存好的模型后，进行预测产生 loss 为 nan 的情况。经过 Debug 查看权重和张量数据，可以发现其均已经溢出了。这可能是由于保存的模型是 double 类型，而重新读取后初始化的模型为 float 类型，导致数据溢出。代码如下。\n1 2 3 4 5 6 7 8 9 10 Net model = Net(); model-\u0026gt;to(device, torch::kDouble); // 数据处理及网络训练与验证，并保存模型 torch::save(model, \u0026#34;test.pt\u0026#34;); Net new_model = Net(); // 首先将网络初始化为 double 类型 new_model-\u0026gt;to(device, torch::kDouble); // 从本地加载保存好的模型 torch::load(new_model, \u0026#34;test.pt\u0026#34;); C10 Error 如果在程序运行过程中抛出了 C10 Error，控制台也没有打印出错误信息，这是 LibTorch 一个已知的问题，详见参考文献[4]。为了得到实际的错误信息，此时我们可以使用 try catch 来手动捕获异常，代码如下。\n1 2 3 4 5 6 try { // 导致异常的代码块 } catch (std::exception \u0026amp;e) { std::cout \u0026lt;\u0026lt; e.what() \u0026lt;\u0026lt; std::endl; } 参考文献 LibTorch 教程 - Allent Dan NumCpp 官方文档 Does LibTorch not support float64 data training? After torch::load model and predict, then got NaN ","date":"2021-10-27T15:00:00+08:00","permalink":"https://jinggqu.github.io/posts/libtorch-toturial/","title":"LibTorch 上手教程"},{"content":"基于 Isight 的实验设计 实验设计 实验设计（Design Of Experiments，DOE），一种安排实验和分析实验数据的数理统计方法。通过实验，研究并确定影响因子，通过改变因子设置以使输出达到最佳值；要严格按照计划进行实验，并进行分析，从而找到改进的途径。\n通过有效的实验方案的设计，找到可行的工艺方法和产品指标。 筛选重要因子，确定真正的影响因素，从而有目标地进行质量改进。 Isight 版本 本文采用 Isight 2016 版本，下载链接：https://dl.downloadly.ir/Files/Software2/DS_SIMULIA_Isight_2016_HF4_Downloadly.ir.rar。\n安装 安装过程不再赘述，值得一提的是，在设置 Isight FlexNet License 时，首选 License 服务器名应填写 27011@localhost，其余留空即可。 完成安装后，启动 Isight 2016.HF4 Design Gateway。\n实验 数据准备 如下图所示，定义了三个输入参数以及两个输出参数，并在 excel 中使用名称管理器为这 5 个变量设置了名称。两个输出参数的计算方法分别如下。\n$$ volume = length \\times height \\times width $$ $$ costs = 2 \\times length^2 + 3 \\times width^2 + 0.05 \\times height^3 $$\n新建任务 打开 Isight 任务界面，将 Application Components 中的 Excel 组件拖拽到如下图所示的位置。\n此时会发现底部有一黄色警告，这是因为我们还没有正确配置 Excel 文件地址。双击刚刚添加的 Excel 组件即可打开配置界面，在顶部选中数据文件。由于我们在 Excel 文件中提前对变量进行命名，故此时 Isight 会提示并自动生成变量，如下图所示。确认无误后点击下方 OK 即可回到任务界面。\n配置 DOE 将 Process Components 选项组中的 DOE 组件拖拽到如下图所示的位置上。\n双击 DOE 组件进行详细配置。首先配置 Factors，我们选中所有的输入参数，并将这些参数的实验区间调整为基数 -20%~20%。配置如下所示。\n切换到 General 选项卡，实验技术选择拉丁方，实验点数输入 50，点击下方的 Apply 生成实验次序，此时可以在 Design Matrix 中查看样本矩阵。\n切换到 Postprocessing 选项卡，在这里配置两个输出参数的目标，即 costs 最小化，volume 最大化，如下图所示。\n进行实验 在实验之前，首先在 Graph Templates 选项卡中添加图标，方便后续的数据可视化。\n在此例中，我们选择添加 Main Effect Graph 用来观察各个输入参数都两个输出参数的影响权重。\n点击菜单栏中的三角形按钮，运行实验。\n结果分析 在 Isight Runtime Gateway 中的 History 选项卡中，如下图所示，表格中的绿色行表示最优解。\n在 Graphs 选项卡中，可以看到之前添加的图表已经填充了本次运行的结果。\n代理模型 代理模型是工程问题中常用的一个优化方法。当实际问题（高精度模型）计算量很大、不容易求解时，可以使用计算量较小、求解迅速的简化模型来替代原模型，加速优化过程。在 Isight Runtime Gateway 中，我们切换到 Visual Design 选项卡，点击 Create Approximation 创建一个代理模型。\n一阶响应曲面模型 由于 volume 项是由三个输入参数相乘得到，故我们可以先创建一阶（线性）响应曲面模型对 volume 进行拟合。\nPolynomial term Coefficient Constant -8061.40938210593 x01 273.292484060334 x02 398.117601806449 x03 149.130521907 即拟合曲线如下，R Square = 0.8935。\n$$ costs = 273.292484060334 \\times length + 398.117601806449 \\times width + 149.130521907 \\times height $$\n三阶响应曲面模型 由于 costs 项是由三个输入参数高阶相乘得到，最高为 height 三阶，故我们创建一个三阶响应曲面模型对 costs 进行拟合。\nPolynomial term Coefficient Constant -4.23710301832585e-010 x01 -2.10336177869166e-010 x02 3.51571404416191e-010 x03 3.616047694285e-011 x01**2 1.44086437819406e-011 x02**2 1.99999999996241 x03**2 2.99999999999869 x01*x02 3.92239176204056e-013 x01*x03 -1.74648188795555e-013 x02*x03 3.35578146664684e-013 x01**3 0.0499999999996755 x02**3 1.27267233735159e-012 x03**3 1.52574123633321e-014 即拟合曲线如下，R Square = 1。\n$$ costs = 1.99999999996241 \\times length^2 + 2.99999999999869 \\times width^2 \\\\\\\\ + 0.0499999999996755 \\times height^3 $$\n参考文献 代理模型 近似模型之响应面建模 DS SIMULIA Isight 2020 Windows/Linux DOE 实验设计必备基础知识理解 半导体工业软件（八）：DOE（实验设计） 常用实验设计方法（DOE）介绍 【YouTube 转载】Isight 参数优化（士盟科技出品）（中文讲解） Abaqus Isight 视频合集 ","date":"2021-07-28T19:00:00+08:00","permalink":"https://jinggqu.github.io/posts/isight-doe/","title":"基于 Isight 的实验设计"},{"content":"Docker 搭建 ShareLaTeX 安装 ShareLaTeX 1 2 sudo su docker pull kingsleyluoxin/sharelatex:full 编辑配置 在 ~/sharelatex 中放入配置文件 docker-compose.yml，并进行编辑。\n1 2 cd ~/sharelatex vim docker-compose.yml 完整配置如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 version: \u0026#34;2.2\u0026#34; services: sharelatex: restart: always image: kingsleyluoxin/sharelatex:full container_name: sharelatex depends_on: mongo: condition: service_healthy redis: condition: service_started ports: - 80:80 links: - mongo - redis volumes: - ~/sharelatex_data:/var/lib/sharelatex environment: SHARELATEX_APP_NAME: Overleaf Community Edition SHARELATEX_MONGO_URL: mongodb://mongo/sharelatex SHARELATEX_REDIS_HOST: redis REDIS_HOST: redis ENABLED_LINKED_FILE_TYPES: \u0026#34;url,project_file\u0026#34; ENABLE_CONVERSIONS: \u0026#34;true\u0026#34; ## Disables email confirmation requirement EMAIL_CONFIRMATION_DISABLED: \u0026#34;true\u0026#34; TEXMFVAR: /var/lib/sharelatex/tmp/texmf-var ### Set for SSL via nginx-proxy #VIRTUAL_HOST: 103.112.212.22 SHARELATEX_SITE_URL: http://172.23.253.113 SHARELATEX_ADMIN_EMAIL: username@qq.com SHARELATEX_EMAIL_FROM_ADDRESS: \u0026#34;username@qq.com\u0026#34; SHARELATEX_EMAIL_SMTP_HOST: smtp.qq.com SHARELATEX_EMAIL_SMTP_PORT: 465 SHARELATEX_EMAIL_SMTP_SECURE: \u0026#34;true\u0026#34; SHARELATEX_EMAIL_SMTP_USER: username@qq.com SHARELATEX_EMAIL_SMTP_PASS: SMTP 授权码 SHARELATEX_EMAIL_SMTP_TLS_REJECT_UNAUTH: \u0026#34;true\u0026#34; SHARELATEX_EMAIL_SMTP_IGNORE_TLS: \u0026#34;false\u0026#34; ## SHARELATEX_CUSTOM_EMAIL_FOOTER: \u0026#34;This system is run by department x\u0026#34; mongo: restart: always image: mongo:4.0 container_name: mongo expose: - 27017 volumes: - ~/mongo_data:/data/db healthcheck: test: echo \u0026#39;db.stats().ok\u0026#39; | mongo localhost:27017/test --quiet interval: 10s timeout: 10s retries: 5 redis: restart: always image: redis:5 container_name: redis expose: - 6379 volumes: - ~/redis_data:/data 初始化容器 1 2 cd ~/sharelatex docker-compose up -d 停止、重启服务 可以先使用以下命令查看正在运行的 docker 服务\n1 docker ps -a 输出如下\n1 2 3 4 CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 471e68a315b9 kingsleyluoxin/sharelatex:full \u0026#34;/sbin/my_init\u0026#34; 2 hours ago Up 2 hours 0.0.0.0:80-\u0026gt;80/tcp sharelatex 86213089675e redis:5 \u0026#34;docker-entrypoint.s…\u0026#34; 2 hours ago Up 2 hours 6379/tcp redis 2ae87df3ecc0 mongo:4.0 \u0026#34;docker-entrypoint.s…\u0026#34; 2 hours ago Up 2 hours (healthy) 27017/tcp mongo 使用以下命令可以停止服务\n1 2 3 docker stop sharelatex docker stop redis docker stop mongo 使用以下命令可以重启服务\n1 2 3 docker restart sharelatex docker restart redis docker restart mongo ","date":"2021-03-11T08:00:00+08:00","permalink":"https://jinggqu.github.io/posts/docker-host-sharelatex/","title":"Docker 搭建 ShareLaTeX"},{"content":"SAE 入门（二）——基于 tiny_dnn 的手写数字重建 前言 在上一篇文章中，我们使用 Python 使用 SAE 网络实现了手写数字的重建。在本文中，我们将尝试使用 tiny_dnn 库实现手写数字重建。\ntiny_dnn 简介 tiny-dnn 项目地址：https://github.com/tiny-dnn/tiny-dnn，这是深度学习的一个 C ++ 14 实现。它适合在有限的计算资源，嵌入式系统和 IoT 设备上进行深度学习。整个项目仅由头文件构成，使用时无需编译，直接引用即可。\n搭建环境 版本要求 需要一个 C++ 14 编译器，例如 gcc 4.9+，clang 3.6+ 或者 VS 2015+。本文中使用 Visual Studio 2019 为例进行配置。\n创建项目 打开 VS，创建一个名为 testTinyDNN 的控制台应用。将 tiny_dnn 下载解压之后，放置到如下图所示的位置，与 testTinyDNN.cpp 属于同一层级。\n编辑配置 编辑 config.h 文件第 61 行，将其取消注释；这样我们才可以将栈式自编码器预测的图片保存到本地。涉及内容如下： 1 2 3 4 5 /** * Enable Image API support. * Currently we use stb by default. **/ #define DNN_USE_IMAGE_API 编辑 image.h 文件第 378 行，将 border_width 值设置为 0，这样保存的图片每个像素周围就不会存在白色边框。涉及内容如下： 1 const size_t border_width = 0; 编写代码 打开 testTinyDNN.cpp 文件，将下列代码粘贴进去。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 #include \u0026lt;iostream\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026#34;tiny_dnn/tiny_dnn.h\u0026#34; using namespace tiny_dnn; using namespace tiny_dnn::activation; using namespace tiny_dnn::layers; using namespace std; #define EPOCHS 50 #define BATCH_SIZE 256 void sae() { // define network, optimizer and engine network\u0026lt;sequential\u0026gt; net; adam optimizer; core::backend_t backend_type = core::default_engine(); // construct network layers, include 3 encoder layers and 3 decoder layers net \u0026lt;\u0026lt; fully_connected_layer(784, 128, true, backend_type) \u0026lt;\u0026lt; relu() \u0026lt;\u0026lt; fully_connected_layer(128, 64, true, backend_type) \u0026lt;\u0026lt; relu() \u0026lt;\u0026lt; fully_connected_layer(64, 32, true, backend_type) \u0026lt;\u0026lt; relu() \u0026lt;\u0026lt; fully_connected_layer(32, 64, true, backend_type) \u0026lt;\u0026lt; relu() \u0026lt;\u0026lt; fully_connected_layer(64, 128, true, backend_type) \u0026lt;\u0026lt; sigmoid() \u0026lt;\u0026lt; fully_connected_layer(128, 784, true, backend_type); // load MNIST dataset vector\u0026lt;vec_t\u0026gt; train_images, test_images; string data_dir_path = \u0026#34;tiny_dnn/data\u0026#34;; parse_mnist_images(data_dir_path + \u0026#34;/train-images.idx3-ubyte\u0026#34;, \u0026amp;train_images, -1.0, 1.0, 0, 0); parse_mnist_images(data_dir_path + \u0026#34;/t10k-images.idx3-ubyte\u0026#34;, \u0026amp;test_images, -1.0, 1.0, 0, 0); cout \u0026lt;\u0026lt; \u0026#34;start training\u0026#34; \u0026lt;\u0026lt; endl; // define learning rate (0.05) optimizer.alpha *= static_cast\u0026lt;tiny_dnn::float_t\u0026gt;(0.05); // display training progress bar, and show training duration progress_display disp(static_cast\u0026lt;unsigned long\u0026gt;(train_images.size())); timer t; // create callback int epoch = 0; auto on_enumerate_epoch = [\u0026amp;]() { epoch++; cout \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34; \u0026lt;\u0026lt; t.elapsed() \u0026lt;\u0026lt; \u0026#34;s elapsed.\u0026#34; \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; \u0026#34;epoch=\u0026#34; \u0026lt;\u0026lt; epoch \u0026lt;\u0026lt; \u0026#34;/\u0026#34; \u0026lt;\u0026lt; EPOCHS \u0026lt;\u0026lt; endl; disp.restart(static_cast\u0026lt;unsigned long\u0026gt;(train_images.size())); t.restart(); }; auto on_enumerate_minibatch = [\u0026amp;]() { disp += BATCH_SIZE; }; // training net.fit\u0026lt;mse\u0026gt;(optimizer, train_images, train_images, BATCH_SIZE, EPOCHS, on_enumerate_minibatch, on_enumerate_epoch); // save model net.save(\u0026#34;sae-net\u0026#34;); cout \u0026lt;\u0026lt; \u0026#34;end training.\u0026#34; \u0026lt;\u0026lt; endl; // if the model already exists, you can read it directly //net.load(\u0026#34;sae-net\u0026#34;); // save layers to image //for (size_t i = 0; i \u0026lt; net.depth(); i++) { // auto out_img = net[i]-\u0026gt;output_to_image(); // auto filename = \u0026#34;layer_\u0026#34; + to_string(i) + \u0026#34;.bmp\u0026#34;; // out_img.save(filename); //} // test and show results for (int i = 0; i \u0026lt; 10; i++) { // get predicted result image auto predict = net.predict(test_images[i]); // save predicted result image to file auto image = vec2image\u0026lt;float\u0026gt;(predict, 10, 28); auto filename = \u0026#34;image_predicted_\u0026#34; + to_string(i) + \u0026#34;.bmp\u0026#34;; image.save(filename); // save the origin test image to file image = vec2image\u0026lt;float\u0026gt;(test_images[i], 10, 28); filename = \u0026#34;image_test_\u0026#34; + to_string(i) + \u0026#34;.bmp\u0026#34;; image.save(filename); } } int main() { sae(); } 在代码中，我们定义了每批次训练数据量为 256 条，总共训练 50 个批次。\n网络结构为 3 个编码层 + 3 个解码层。编码层将数据从 784（28 * 28） 维分别编码（降维）到 128、64、32 维，解码器再将 32 维的编码结果解码（升维）到 64、128、784 维，完成手写数字重建。各层之间的激活函数选用 relu() 与 sigmoid()。\n结果展示 从上到下，第一行为测试图像，第二行为 keras 搭建的 SAE 网络重建图像，第三行为 tiny_dnn 搭建的 SAE 网络重建图像。下面展示数字 2 和 5 重建的详细效果，左侧为 Python 平台重建结果，右侧为 C++ 平台重建结果。\n数字 2\n数字 5\n性能对比 测试使用的 CPU 型号为 Intel i5-4200H，基准频率为 2.80GHz。\n基于 tiny_dnn 的 C++ 平台训练时长为 2624.95 秒，基于 keras 的 Python 平台训练时长为 135.70 秒。在 50 个 epoch 测试中，Python 平台比 C++ 平台快了大约 19 倍，Python 平台 loss 大约为 0.08。由重建图片结果不难看出，Python 平台效果明显优于 C++ 平台。\n存在的不足 C++ 平台目前无法计算每个 epoch 的 loss； 将在 C++ 平台测试更多的 epoch，观察图像重建效果是否会有改善。 参考文献 A simple and basic tutorial of tiny-dnn A quick introduction to tiny-dnn Details about tiny-dnn\u0026rsquo;s API and short examples ","date":"2021-01-28T18:00:00+08:00","permalink":"https://jinggqu.github.io/posts/sae-2/","title":"SAE 入门（二）——基于 tiny_dnn 的手写数字重建"},{"content":"SAE 入门（一） Autoencoder 简介 自编码器（Autoencoder，AE），是一种利用反向传播（backpropagation，BP）算法使得输出值等于输入值的神经网络，它先将输入压缩成潜在空间表征，然后通过这种表征来重构输出。其中，空间表征可以看作是输入数据的高级抽象，通常是将高维度的数据抽象为低维度的数据。\n自编码器由两部分组成：\n编码器：这部分能将输入压缩成潜在空间表征，可以用编码函数 $h=f(x)$ 表示;\n解码器：这部分能重构来自潜在空间表征的输入，可以用解码函数 $r=g(h)$ 表示。\n因此，整个自编码器可以用函数 $g(f(x)) = r$ 来描述，其中输出 $r$ 与原始输入 $x$ 相近。\n自动编码器的目标是最大程度地减少输入和输出之间的重构误差。这有助于自动编码器学习数据中存在的重要功能。当表征很好地重建其输入时，则表示这个表征很好地保留了输入中存在的许多信息。整个过程如下图。\nStacked Autoencoder 简介 Stacked Autoencoder 简写作 SAE。SAE 与 AE 的主要区别在于编码器与解码器的层数，栈式自编码器包含多层隐藏层。具体网络结构如下图所示，图中有两层编码层，两层解码层。\n代码实现 代码环境配置，请参考 GAN 网络之手写数字生成 第一小节——环境搭建。\n自编码器只是一种思想，在具体实现中，编码器和解码器可以由多种深度学习模型构成，例如全连接层、卷积层和 LSTM 等，以下使用 Keras 来实现栈式自编码器。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 from keras.datasets import mnist from keras.layers import Input, Dense from keras.models import Model import numpy as np import matplotlib.pyplot as plt EPOCHS = 50 BATCH_SIZE = 256 def train(x_train, x_test): input_img = Input(shape=(784,)) # 三个编码层，将数据从 784 维向量编码为 128、64、32 维向量 encoded = Dense(units=128, activation=\u0026#39;relu\u0026#39;)(input_img) encoded = Dense(units=64, activation=\u0026#39;relu\u0026#39;)(encoded) encoded = Dense(units=32, activation=\u0026#39;relu\u0026#39;)(encoded) # 三个解码层，将数据从 32 维向量解码成 64、128、784 维向量 decoded = Dense(units=64, activation=\u0026#39;relu\u0026#39;)(encoded) decoded = Dense(units=128, activation=\u0026#39;relu\u0026#39;)(decoded) decoded = Dense(units=784, activation=\u0026#39;sigmoid\u0026#39;)(decoded) autoencoder = Model(input_img, decoded) encoder = Model(input_img, encoded) autoencoder.summary() encoder.summary() autoencoder.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) autoencoder.fit(x_train, x_train, epochs=EPOCHS, batch_size=BATCH_SIZE, shuffle=True, validation_data=(x_test, x_test)) return encoder, autoencoder def plot(encoded_imgs, decoded_imgs): plt.figure(figsize=(40, 4)) for i in range(10): # 展示原始输入图像 ax = plt.subplot(3, 20, i + 1) plt.imshow(x_test[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) # 展示编码后的图像 ax = plt.subplot(3, 20, i + 1 + 20) plt.imshow(encoded_imgs[i].reshape(8, 4)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) # 展示解码后的输入图像 ax = plt.subplot(3, 20, 2 * 20 + i + 1) plt.imshow(decoded_imgs[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) plt.show() if __name__ == \u0026#39;__main__\u0026#39;: # 加载数据，训练数据 60000 条，测试数据 10000 条，数据灰度值 [0, 255] (x_train, _), (x_test, _) = mnist.load_data() # 正则化数据，将灰度值区间转换为 [0, 1] x_train = x_train.astype(\u0026#39;float32\u0026#39;) / 255 x_test = x_test.astype(\u0026#39;float32\u0026#39;) / 255 # 将数据集从二维 (28, 28) 矩阵转换为长度为维度是 784 的向量 x_train = x_train.reshape(len(x_train), np.prod(x_train.shape[1:])) x_test = x_test.reshape(len(x_test), np.prod(x_test.shape[1:])) print(x_train.shape) print(x_test.shape) # 训练数据 encoder, autoencoder = train(x_train, x_test) # 获取编码后和解码后的图像 encoded_imgs = encoder.predict(x_test) decoded_imgs = autoencoder.predict(x_test) # 绘制图像 plot(encoded_imgs, decoded_imgs) 运行上述代码，可以从输出内容中得到以下信息：\n输入数据是 60000 张手写数字的灰度图像，灰度取值范围是 [0, 255]，我们将其灰度值按行依次存储到一个 1 * 784 的数组中； 输入数据形如 (0, 0, 0,\u0026hellip;, 84, 185, 159,\u0026hellip;, 170, 52,\u0026hellip;, 0, 0)，我们可以将每张图片（每个向量）理解为一个 784 维空间的中向量； 通过正则化后，输入数据每个维度区间变为 [0, 1]； 编码层将输入的 784 维向量抽象为 128、64、32 维向量（dense，dense_1，dense_2）； 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Model: \u0026#34;functional_3\u0026#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 784)] 0 _________________________________________________________________ dense (Dense) (None, 128) 100480 _________________________________________________________________ dense_1 (Dense) (None, 64) 8256 _________________________________________________________________ dense_2 (Dense) (None, 32) 2080 ================================================================= Total params: 110,816 Trainable params: 110,816 Non-trainable params: 0 _________________________________________________________________ 解码层将抽象后的 32 维向量还原维 64、128、784 维向量（dense_3，dense_4，dense_5）； 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Model: \u0026#34;functional_1\u0026#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 784)] 0 _________________________________________________________________ dense (Dense) (None, 128) 100480 _________________________________________________________________ dense_1 (Dense) (None, 64) 8256 _________________________________________________________________ dense_2 (Dense) (None, 32) 2080 _________________________________________________________________ dense_3 (Dense) (None, 64) 2112 _________________________________________________________________ dense_4 (Dense) (None, 128) 8320 _________________________________________________________________ dense_5 (Dense) (None, 784) 101136 ================================================================= Total params: 222,384 Trainable params: 222,384 Non-trainable params: 0 _________________________________________________________________ 训练完成之后，可以在输出内容中看到详细的训练数据，在 50 次训练之后，loss 已经降低到了 0.08。得到的输出图像如下图所示。\n1 2 3 4 5 6 7 8 ...... Epoch 48/50 235/235 [==============================] - 3s 12ms/step - loss: 0.0848 - accuracy: 0.0130 - val_loss: 0.0844 - val_accuracy: 0.0147 Epoch 49/50 235/235 [==============================] - 3s 12ms/step - loss: 0.0846 - accuracy: 0.0130 - val_loss: 0.0845 - val_accuracy: 0.0115 Epoch 50/50 235/235 [==============================] - 3s 12ms/step - loss: 0.0845 - accuracy: 0.0139 - val_loss: 0.0840 - val_accuracy: 0.0165 参考文献 Sparse, Stacked and Variational Autoencoder Deep Learning Autoencoders Deep Autoencoder using Keras 自编码器是什么？有什么用？这里有一份入门指南（附代码） 反向传播算法 - 维基百科 Autoencoder - Github ","date":"2021-01-20T18:00:00+08:00","permalink":"https://jinggqu.github.io/posts/sae-1/","title":"SAE 入门（一）"},{"content":"并行计算总结 1. 概念 1.1 摩尔定律（Moore\u0026rsquo;s law） 摩尔定律是由英特尔（Intel）创始人之一戈登·摩尔提出的。其内容为：集成电路上可容纳的晶体管数目，约每隔两年便会增加一倍；经常被引用的“18 个月”，是由英特尔首席执行官大卫·豪斯（David House）提出：预计 18 个月会将芯片的性能提高一倍（即更多的晶体管使其更快），是一种以倍数增长的观测。\n通常认为摩尔定律具体的内容：每 18 个月，芯片的性能将提高一倍。\n1.2 新摩尔定律（存疑） 由于单个核心性能提升有着严重的瓶颈问题，未来的计算机硬件不会更快，但会更“宽”。\n1.3 常见并行模式 进程 + 线程 硬件组织通常是多机+多核，编程环境 MPI+OpenMP\n线程 + GPU 线程 硬件组织通常是多核+多 GPU，编程环境 OpenMP+CUDA/OpenCL\n进程 + 线程 + GPU 线程 硬件组织通常是多机+多核+多 GPU，编程环境 MPI+OpenMP+CUDA/OpenCL\n1.4 CPU 与 GPU 区别 上图中，绿色的是计算单元，橙红色的是存储单元，橙黄色的是控制单元。其中，各个部件详细释义：\nALU：算术逻辑单元（Arithmetic Logic Unit），是一种可对二进制整数执行算术运算或位运算的组合逻辑数字电路； Control：控制单元，负责指挥 CPU 工作，控制其他设备的活动； Cache：用于减少处理器访问内存所需平均时间的部件； DRAM：动态随机存取存储器（Dynamic Random Access Memory），即内存。 简而言之，CPU 擅长于复杂逻辑控制，GPU 擅长于简单重复运算。\n2. CUDA 简介及架构 CUDA（Compute Unified Device Architecture）是 NVIDIA 推出的的通用并行计算架构，该架构使 GPU 能够解决大量重复性的计算问题。\n2.1 物理架构 2.2 kernel 函数启动参数 2.2.1 blocksPerGrid 用于定义 kernel 函数使用的 block 数量，可以定义为一、二与三维结构，示例如下；\n1 2 3 4 5 6 7 8 // 使用 8 个 block int blocksPerGrid = 8; // 使用 2 * 2 个 block dim3 blocksPerGrid(2, 2); // 使用 2 * 2 * 2 个 block dim3 blocksPerGrid(2, 2, 2); 2.2.2 threadsPerBlock 用于定义每个 block 中使用的线程数量，与 block 类似，同样可以定义为一、二与三维结构。在目前的 GPU 上，一个线程块可以包含多达 1024 个线程。示例如下；\n1 2 3 4 5 6 7 8 // 使用 1024 个线程 int threadsPerBlock = 1024; // 使用 32 * 32 个线程 dim3 threadsPerBlock(32, 32); // 使用 8 * 8 * 8 个线程 dim3 threadsPerBlock(8, 8, 8); 2.2.3 调用 kernel 函数 1 2 3 4 5 // 官方示例，sharedMemBytes 表示指定共享内存大小，单位为字节 kernelFunction \u0026lt;\u0026lt;\u0026lt; dimGrid, dimBlock, sharedMemBytes \u0026gt;\u0026gt;\u0026gt; (...); // 实际调用示例，sharedMemBytes 参数可省略 kernelFunction \u0026lt;\u0026lt;\u0026lt; blocksPerGrid, threadsPerBlock \u0026gt;\u0026gt;\u0026gt; (...); 2.3 常用 CUDA 关键字 函数定义方式 执行方 调用方 __device__ float DeviceFunc() GPU GPU __global__ void KernelFunc() GPU CPU __host__ float HostFunc() CPU CPU __device__函数没有函数地址，也没有指向它的函数指针。在 device 端执行的函数有下面的限制:\n没有递归； 函数内部没有静态变量； 参数的数量是固定的。 2.3.1 __shared__ 存储于 GPU 上的 thread block 内的共享存储器； 和 thread block 具有相同的生命期； 只能被 thread block 内的线程存取。 常用于声明变量，声明后的变量将会存储在 shared memory 中，例如\n1 2 3 4 5 // 存储在 shared memory 中 __shared__ float array[N]; // 存储在 global memory 中 float array[N]; 2.4 内存类型 2.4.1 Register 与 Local Memory 对每个线程来说，寄存器都是线程私有的； 如果寄存器被消耗完，数据将被存储在 local memory。Local memory 是私有的，但是 local memory 中的数据是被保存在显存中，速度很慢； 输入和中间输出变量将被保存在 register 或者 local memory 中。 2.4.2 Shared Memory 用于线程间通信的 shared memory。shared memory 是一块可以被同一 block 中的所有 thread 访问的可读写存储器； 访问 shared memory 几乎和访问 register 一样快，是实现线程间通信的延迟最小的方法； shared memory 可以实现许多不同的功能，如用于保存公用的计数器或者 block 的公用结构。 2.5 内存分配 通过如下语句可以实现 CUDA 内存分配，分配显存中的 global memory。\n1 cudaMalloc(void** devPtr, size_t size) 其中\ndevPtr：对象指针； size：分配的内存大小。 例如\n1 cudaMalloc((void**) \u0026amp;device_array, N * sizeof(float)); 通过如下语句可以实现 CUDA 释放 global memory 中分配出的内存。\n1 cudaFree(void* devPtr) 2.6 数据交换 通过如下语句可以实现 CUDA 显存中数据与 CPU 内存端数据的交换。\n1 cudaMemcpy(void *dst, const void *src, size_t count, enum cudaMemcpyKind kind) 其中\ndst：目的存储器地址； src：源存储器地址； count：拷贝数据的大小； kind：数据传输类型，常用的包括以下两种： cudaMemcpyDeviceToHost：将显存中的数据拷贝到内存中； cudaMemcpyHostToDevice：将内存中的数据拷贝到显存中。 例如\n1 cudaMemcpy(device_array, host_array, N * sizeof(float), cudaMemcpyHostToDevice); 2.7 线程同步 2.7.1 block 内线程同步 通过调用以下方法，实现同一个 block 内所有线程同步。\n1 __syncthreads(); 例如\n1 2 3 4 5 __global__ void function(...) { ... __syncthreads(); ... } 2.7.2 CPU 与 GPU 线程同步 通常情况下，CPU 端调用 kernel 函数，并不会阻塞后续 CPU 端的代码执行，也即调用 kernel 函数是异步的。若要实现同步的效果，只需在调用 kernel 函数后调用以下方法，进行线程同步即可。\n1 2 cudaDeviceSynchronize() cudaThreadSynchronize() // 已过时，应避免使用 例如\n1 2 kernel \u0026lt;\u0026lt;\u0026lt; blocksPerGrid, threadsPerBlock \u0026gt;\u0026gt;\u0026gt; (...); cudaDeviceSynchronize(); 3 CUDA 编程 3.1 矩阵相加 https://github.com/jinggqu/ParallelComputing/blob/main/classwork/01/matrix_addition.cu\n3.2 矩阵相乘 https://github.com/jinggqu/ParallelComputing/blob/main/classwork/02/matrix_multiplication.cu\n3.3 并行规约 https://github.com/jinggqu/ParallelComputing#%E4%BA%8C%E8%8E%B7%E5%8F%96%E7%9F%A9%E9%98%B5%E6%AF%8F%E4%B8%80%E8%A1%8C%E7%9A%84%E6%9C%80%E5%A4%A7%E5%80%BC\n","date":"2021-01-11T15:00:00+08:00","permalink":"https://jinggqu.github.io/posts/parallel-computing-summary/","title":"并行计算总结"},{"content":"GAN 网络之手写数字生成 环境搭建 本例中，所涉及的系统与软件版本列表如下。\n名称 版本 操作系统 Windows 20H2 Anaconda Anaconda3-2020.11 python 3.6 tensorflow 1.8.0 本例代码存放于 https://github.com/jinggqu/MachineLearning。\nAnaconda 安装 通过清华大学开源软件镜像站，我们可以直接下载最新版本的 Anaconda，本例中使用的 Anaconda 下载链接： https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2020.11-Windows-x86_64.exe\nAnaconda 安装教程网络上已经有很多，故此处不再赘述。\n安装完成后，我们需要手动配置 Anaconda 的环境变量，在用户变量的 Path 中添加 Anaconda 的安装路径以及其子文件夹，具体内容如下。\n1 2 3 C:\\Users\\xvyn\\anaconda3 C:\\Users\\xvyn\\anaconda3\\Scripts C:\\Users\\xvyn\\anaconda3\\Library\\bin 上述配置请根据 Anaconda 实际安装路径进行调整，配置完成的效果如下图所示。\n完成后打开 cmd 输入下列命令，如果输出内容与下列内容类似，则表示配置正确，可继续后面的步骤。\n1 conda --version 输出 conda 4.9.2 创建虚拟环境 通过如下命令进行创建一个虚拟环境。\n1 conda create -n handwrittendigits -n handwrittendigits 的作用是指定虚拟环境的名称，本例中指定为 handwrittendigits。\n执行结束后，可通过下列命令查看 Anaconda 中所有的虚拟环境。\n1 conda info --evns 输出如下\n(base) PS C:\\Users\\xvyn\u0026gt; conda info --envs conda environments: base * C:\\Users\\xvyn\\anaconda3 handwrittendigits C:\\Users\\xvyn\\anaconda3\\envs\\handwrittendigits 其中，标记 * 的表示目前已启用，命令行前半部分的 (base) 也表示目前启用的是哪个虚拟环境，此例中为 base 环境。\n切换虚拟环境 如果使用 PowerShell 进行 Anaconda 的一些操作，需要以 管理员 身份运行 PowerShell，然后执行下列命令。\n1 set-executionpolicy remotesigned 执行完成后可通过下列命令进行切换虚拟环境。若使用其他 Shell 工具进行操作，则可直接执行下列命令。\n1 conda activate handwrittendigits 如果执行时报错如下，则可以通过 https://github.com/conda/conda/issues/7980 来解决。\nCan't execute `conda activate` from batch script 详细操作为：\n安装并打开 Git Bash 执行 source ~/anaconda3/etc/profile.d/conda.sh 执行 conda init 重启 PowerShell 切换环境操作结束后，可以注意到命令行左侧的括号内容由 (base) 变为 (handwrittendigits)，表明切换成功，后面的操作均在此虚拟环境中进行。\n实际操作过程 (base) PS C:\\Users\\xvyn\u0026gt; conda activate handwrittendigits (handwrittendigits) PS C:\\Users\\xvyn\u0026gt; 再次查看所有虚拟环境 (handwrittendigits) PS C:\\Users\\xvyn\u0026gt; conda info --envs conda environments: base C:\\Users\\xvyn\\anaconda3 handwrittendigits * C:\\Users\\xvyn\\anaconda3\\envs\\handwrittendigits 更换镜像源（不推荐） 由于 Anaconda 和 pip 官方镜像源访问缓慢，故需要将镜像源更换为国内镜像源，例如清华大学、中科大与阿里云镜像源。使用下列命令可以查看当前 Anaconda 镜像源。\n1 conda config --show 在输出中找到 channel 部分，有如下内容。\nchannels: - defaults default_channels: - https://repo.anaconda.com/pkgs/main - https://repo.anaconda.com/pkgs/r - https://repo.anaconda.com/pkgs/msys2 更换 Anaconda 镜像源 以清华大学镜像源为例，执行下列命令即可完成更换。\n1 2 3 4 5 6 7 conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/menpo/ conda config --set show_channel_urls yes 恢复默认源\n1 conda config --remove-key channels 除了上述命令行操作方式外，也可以直接修改 C:\\Users\u0026lt;USER\u0026gt;\\.condarc 文件来实现换源。参考 Anaconda 镜像使用帮助 修改后的文件内容如下所示。\nssl_verify: false show_channel_urls: true channels: - defaults default_channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 custom_channels: conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud 更换 pip 镜像源（不推荐） 以清华大学镜像源为例，执行下列命令即可完成更换。\n1 pip config set global.index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple 安装 tensorflow 手写数字生成例子所需要的 tensorflow 版本为 1.x，本例中我们使用的实际版本为 1.8.0。将虚拟环境切换到 handwrittendigits 后，执行以下命令开始安装。\n1 conda install tensorflow-gpu=1.8.0 上述命令中 tensorflow-gpu 表示安装的 tensorflow 为 GPU 版本，=1.8.0 指定了安装的版本号。若需要安装 CPU 版 tensorflow 1.8.0，执行以下命令即可。\n1 conda install tensorflow=1.8.0 安装 Python 由于需要 1.8.0 版本的 tensorflow，此版本仅兼容 3.5 到 3.7 版本的 Python，故需要先删除 conda 环境中默认安装的 Python，并安装 3.6 版本。\n1 2 3 4 5 ## 移除自带 Python conda remove python ## 安装 3.6 版本 conda install python=3.6 测试 Demo 使用 PyCharm 创建项目 在创建项目时，需要将虚拟环境（图中 Location 项）配置为前文中创建的虚拟环境所在目录，然后点击创建项目。\n由于此前作者已经创建过项目，故创建窗口下方会提示虚拟环境目录不为空，忽略即可。\n运行项目 将以下代码置于项目 main.py 中，运行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data import numpy as np import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec import logging import os if not os.path.exists(\u0026#39;./log\u0026#39;): os.mkdir(\u0026#39;./log\u0026#39;) if not os.path.exists(\u0026#39;./out\u0026#39;): os.mkdir(\u0026#39;./out\u0026#39;) def get_logger(filepath, level=logging.INFO): logger = logging.getLogger(__name__) logger.setLevel(level) ## create a file handler handler = logging.FileHandler(filepath) handler.setLevel(logging.INFO) ## create a logging format ## formatter = logging.Formatter(\u0026#39;%(asctime)s - %(name)s - %(levelname)s - %(message)s\u0026#39;) ## handler.setFormatter(formatter) ## add the handlers to the logger logger.addHandler(handler) return logger def plot(samples): fig = plt.figure(figsize=(4, 4)) gs = gridspec.GridSpec(4, 4) gs.update(wspace=0.05, hspace=0.05) for i, sample in enumerate(samples): ax = plt.subplot(gs[i]) plt.axis(\u0026#39;off\u0026#39;) ax.set_xticklabels([]) ax.set_yticklabels([]) ax.set_aspect(\u0026#39;equal\u0026#39;) plt.imshow(sample.reshape(28, 28), cmap=\u0026#39;Greys_r\u0026#39;) return fig def random_data(row, column): return np.random.uniform(-1., 1., size=[row, column]) def weight_variable(shape, stddev=0.1): initial = tf.truncated_normal(shape, stddev=stddev) return tf.Variable(initial) def bias_variable(shape, bais=0.1): initial = tf.constant(bais, shape=shape) return tf.Variable(initial) ## 鉴别网络weights d_w1 = weight_variable([784, 128]) d_b1 = bias_variable([128]) d_w2 = weight_variable([128, 1]) d_b2 = bias_variable([1]) param_d = [d_w1, d_w2, d_b1, d_b2] ## 生成网络weights g_w1 = weight_variable([100, 128]) g_b1 = bias_variable([128]) g_w2 = weight_variable([128, 784]) g_b2 = bias_variable([784]) param_g = [g_w1, g_w2, g_b1, g_b2] ## 鉴别网络 def d_network(x): d1 = tf.nn.relu(tf.matmul(x, d_w1) + d_b1) d_out = tf.matmul(d1, d_w2) + d_b2 return tf.nn.sigmoid(d_out) ## 生成网络 def g_network(x): g1 = tf.nn.relu(tf.matmul(x, g_w1) + g_b1) g_out = tf.matmul(g1, g_w2) + g_b2 return tf.nn.sigmoid(g_out) x = tf.placeholder(tf.float32, shape=[None, 784]) z = tf.placeholder(tf.float32, shape=[None, 100]) d_out_real = d_network(x) g_out = g_network(z) d_out_fake = d_network(g_out) d_loss = -tf.reduce_mean(tf.log(d_out_real) + tf.log(1. - d_out_fake)) g_loss = -tf.reduce_mean(tf.log(d_out_fake)) d_optimizer = tf.train.AdamOptimizer().minimize(d_loss, var_list=param_d) g_optimizer = tf.train.AdamOptimizer().minimize(g_loss, var_list=param_g) batch_size = 256 max_step = 1000000 mnist = input_data.read_data_sets(\u0026#39;../mnist\u0026#39;, one_hot=True) logger = get_logger(\u0026#34;./log/info.log\u0026#34;) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(\u0026#34;training\u0026#34;) i = 0 for step in range(max_step): batch_real, _ = mnist.train.next_batch(batch_size) _, d_loss_train = sess.run([d_optimizer, d_loss], feed_dict={x: batch_real, z: random_data(batch_size, 100)}) _, g_loss_train = sess.run([g_optimizer, g_loss], feed_dict={z: random_data(batch_size, 100)}) if step % 1000 == 0: samples = sess.run(g_out, feed_dict={z: random_data(16, 100)}) fig = plot(samples) plt.savefig(\u0026#39;out/{}.png\u0026#39;.format(str(i).zfill(4)), bbox_inches=\u0026#39;tight\u0026#39;) i += 1 plt.close(fig) logger.info(\u0026#34;step %s: d_loss is %s, gan_loss is %s\u0026#34; % (step, d_loss_train, g_loss_train)) print(\u0026#34;step %s: d_loss is %s, g_loss is %s\u0026#34; % (step, d_loss_train, g_loss_train)) 运行时的截图如下，可以看到已经生成了多张手写数字的图片。\n至此，GAN 网络手写数字生成环境搭建已经完成，后续将进行更加深入的学习。\n备注 为 jupyter lab 指定 conda 环境，在 conda 环境中执行以下命令后再启动 jupyter lab 即可\n1 conda install nb_conda 参考文章 Anaconda 源使用帮助 gan_practice Can\u0026rsquo;t execute conda activate from bash script python 安装 TensorFlow 吐血整理 conda 安装指定版本的指定包 Python pip 命令行设置国内镜像源 ","date":"2020-12-08T10:00:00+08:00","permalink":"https://jinggqu.github.io/posts/gan-for-hand-written-digits/","title":"GAN 网络之手写数字生成"},{"content":"并行计算课程实践 写在前面 Vim 基本操作 简介 vim 共分为三种模式，分别是命令模式（Command mode），输入模式（Insert mode）和底线命令模式（Last line mode）。 详见 Linux vi/vim | 菜鸟教程。\n编辑文件 编辑文本文件可以通过 vim 文件名 实现，如果文件原本不存在，则 Vim 会自动新建该文件。例如新建或编辑名为 test.txt 的文件，可以通过下面的命令实现。\n1 vim test.txt 进入编辑界面之后，按下 i 键或 Insert 键，即可进入输入模式开始编辑文件。\n常用命令 命令 操作说明 Esc 退出编辑模式 :wq 保存文件并退出 Vim :q! 不保存文件并强制退出 Vim（慎用） :set nu 显示行号 i / Insert 切换到输入模式 x 删除当前光标所在处的字符 d 剪切光标所在行 dd 删除光标所在行 yy 复制光标所在行 p 粘贴已复制或已剪切的数据 u 撤销上一步操作 gg 跳转到文件开头 G(shift+g) 跳转到文件末尾 /word 搜索名称为 word 的字符串 ^ 跳转到光标所在行首 $ 跳转到光标所在行尾 代码及数据 本文的代码均存放在 Github，地址 https://github.com/jinggqu/ParallelComputing。\n本例中所设计到的输入矩阵有两个：a 矩阵与 b 矩阵，其值相同，如下：\na、b 矩阵：\n1 2 3 4 5 6 7 8 9 10 18.04\t8.47\t16.82\t17.15\t19.58\t4.24\t7.20\t16.50\t5.97\t11.90 10.25\t13.50\t7.83\t11.03\t20.45\t19.68\t13.65\t15.40\t3.04\t13.03 0.35\t5.22\t2.95\t17.27\t3.36\t8.61\t2.79\t2.34\t21.45\t4.69 11.02\t18.02\t13.16\t6.36\t13.69\t11.26\t10.60\t20.89\t6.28\t16.56 11.31\t16.53\t8.59\t19.15\t6.08\t7.57\t17.35\t19.74\t1.50\t20.39 11.30\t1.85\t4.13\t14.24\t19.12\t7.49\t1.38\t0.43\t9.83\t1.35 5.12\t20.84\t19.37\t18.27\t5.73\t11.59\t8.06\t16.33\t11.01\t14.34 11.42\t0.84\t9.40\t20.01\t19.99\t15.48\t6.11\t15.86\t13.74\t7.60 14.77\t3.56\t9.45\t18.90\t17.81\t7.09\t4.92\t19.19\t7.52\t14.75 20.54\t12.64\t14.12\t18.44\t9.44\t19.84\t8.56\t17.50\t14.69\t19.56 矩阵相加 上传文件 在命令行中使用 rz 命令开始上传文件，选中相应文件之后点击打开即可开始上传。\n编译 执行 nvcc 命令编译上述文件，生成可执行文件。\n1 nvcc -o matrixaddition matrixaddition.cu 其中，-o matrixaddition 表示将可执行文件的文件名指定为 matrixaddition。若不指定，则默认为 a.out。\n执行完上述操作之后，使用 ll 命令可以查看现有的所有文件，输出如下。\n1 2 -rwxrwxr-x. 1 cuda01 cuda01 569416 11 月 24 09:50 matrixaddition -rw-r--r--. 1 cuda01 cuda01 2023 11 月 24 09:49 matrixaddition.cu 运行 上述过程中生成了 matrixaddition 可执行文件，执行下述命令即可开始运行。\n1 ./matrixaddition 输出结果如下：\n1 2 3 4 5 6 7 8 9 10 36.09\t16.94\t33.63\t34.29\t39.15\t8.48\t14.40\t33.00\t11.93\t23.79 20.50\t27.01\t15.67\t22.05\t40.90\t39.35\t27.30\t30.81\t6.08\t26.07 0.70\t10.43\t5.89\t34.54\t6.73\t17.22\t5.57\t4.67\t42.90\t9.37 22.03\t36.04\t26.31\t12.71\t27.38\t22.52\t21.20\t41.78\t12.56\t33.13 22.62\t33.07\t17.19\t38.29\t12.17\t15.14\t34.69\t39.47\t3.00\t40.77 22.59\t3.70\t8.26\t28.49\t38.24\t14.98\t2.76\t0.86\t19.66\t2.71 10.23\t41.69\t38.75\t36.55\t11.45\t23.18\t16.12\t32.65\t22.01\t28.68 22.83\t1.69\t18.80\t40.02\t39.98\t30.96\t12.21\t31.72\t27.49\t15.21 29.54\t7.13\t18.90\t37.80\t35.61\t14.19\t9.83\t38.37\t15.05\t29.49 41.08\t25.28\t28.23\t36.88\t18.88\t39.68\t17.11\t34.99\t29.39\t39.13 矩阵相乘 与上述过程类似，将编写好的代码上传到服务器中，准备进行编译。\n编译 执行 nvcc 命令编译代码文件，生成可执行文件。\n1 nvcc -o matrixmultiplication matrixmultiplication.cu 运行 上述过程中生成了 matrixmultiplication 可执行文件，执行下述命令即可开始运行。\n1 ./matrixmultiplication 相乘结果：\n1 2 3 4 5 6 7 8 9 10 1434.15\t1331.02\t1349.52\t2031.20\t1607.49\t1378.23\t1109.41\t1915.59\t1198.52\t1642.11 1459.53\t1356.36\t1325.31\t2057.32\t1718.01\t1479.91\t1109.21\t1791.41\t1139.91\t1573.60 840.46\t667.29\t691.96\t1000.79\t1033.85\t727.56\t517.53\t1100.00\t572.38\t918.05 1465.92\t1237.35\t1307.85\t2113.54\t1760.48\t1593.65\t1067.92\t1812.61\t1268.27\t1554.78 1106.95\t1007.73\t1027.50\t1761.49\t1474.07\t1238.15\t1092.06\t1966.31\t1278.88\t1661.49 866.95\t809.99\t741.98\t1093.15\t931.67\t600.26\t684.60\t1144.12\t468.24\t1008.11 1394.52\t1273.50\t1259.89\t1989.42\t1802.07\t1643.34\t1025.81\t1803.05\t1353.40\t1541.02 1410.82\t1162.53\t1229.82\t1926.65\t1632.43\t1203.22\t978.71\t1790.40\t1080.38\t1510.35 1338.49\t1253.85\t1171.30\t1826.25\t1491.70\t1298.11\t1020.23\t1856.90\t1107.89\t1567.99 1901.57\t1436.10\t1636.91\t2460.80\t2242.40\t1747.59\t1161.72\t2187.99\t1521.42\t1855.17 ","date":"2020-11-24T10:00:00+08:00","permalink":"https://jinggqu.github.io/posts/parallel-computing/","title":"并行计算课程实践"},{"content":"FastDFS 搭建分布式文件管理系统 FastDFS 简介 FastDFS 是一个开源的高性能分布式文件系统（Distributed File System）。它的主要功能包括：文件存储、文件同步和文件访问以及高容量和负载平衡。主要解决了海量数据存储问题，特别适合以中小文件（建议范围：4KB \u0026lt; file_size \u0026lt; 500MB）为载体的在线服务。\nFastDFS 开源地址：https://github.com/happyfish100/fastdfs\n由于网络上已有很多详细的关于 FastDFS 的介绍，故此处不再赘述。请查看参考文章中的第 1、2 条。\nFastDFS 架构图 FastDFS 上传流程 FastDFS 下载流程 安装 FastDFS 配置防火墙 本篇文章是基于 CentOS v8.2.2004 版本，以下操作均为单机环境，单机 IP 地址为 192.168.61.128。在安装 FastDFS 之前，需要先进行防火墙的设置。防火墙的相关命令如下：\n1 2 3 4 5 6 7 8 ## 暂时关闭防火墙 systemctl stop firewalld ## 永久关闭防火墙 systemctl disable firewalld ## 启用防火墙 systemctl enable firewalld 下载安装 libfastcommon libfastcommon 是从 FastDFS 抽取出来的公共 c 函数库。\n1 2 3 4 5 6 7 8 9 10 ## 下载 wget https://github.com/happyfish100/libfastcommon/archive/V1.0.43.tar.gz ## 解压 tar -zxvf V1.0.43.tar.gz cd libfastcommon-1.0.43 ## 编译安装 ./make.sh ./make.sh install libfastcommon.so 安装到了 /usr/lib64/libfastcommon.so，但是 FastDFS 主程序设置的 lib 目录是 /usr/local/lib，所以需要创建软链接。\n1 2 3 4 ln -s /usr/lib64/libfastcommon.so /usr/local/lib/libfastcommon.so ln -s /usr/lib64/libfastcommon.so /usr/lib/libfastcommon.so ln -s /usr/lib64/libfdfsclient.so /usr/local/lib/libfdfsclient.so ln -s /usr/lib64/libfdfsclient.so /usr/lib/libfdfsclient.so 下载安装 FastDFS 1 2 3 4 5 6 7 8 9 10 ## 下载 wget https://github.com/happyfish100/fastdfs/archive/V6.06.tar.gz ## 解压 tar -zxvf V6.06.tar.gz cd fastdfs-6.06 ## 编译安装 ./make.sh ./make.sh install 安装完成之后，服务脚本存储在 /etc/init.d/ 中，详细文件如下：\n/etc/init.d/fdfs_storaged /etc/init.d/fdfs_tracker 默认配置文件存储在 /etc/fdfs/ 中，详细文件如下：\n/etc/fdfs/client.conf.sample /etc/fdfs/storage.conf.sample /etc/fdfs/tracker.conf.sample 命令工具存储在 /usr/bin/ 中，详细文件如下：\nfdfs_appender_test fdfs_appender_test1 fdfs_append_file fdfs_crc32 fdfs_delete_file fdfs_download_file fdfs_file_info fdfs_monitor fdfs_regenerate_filename fdfs_storaged fdfs_test fdfs_test1 fdfs_trackerd fdfs_upload_appender fdfs_upload_file stop.sh restart.sh 配置 FastDFS Tracker 服务 修改配置文件 从上文可知，配置文件在 /etc/fdfs/ 中，我们需要拷贝一份并进行一些修改。\n1 2 3 cd /etc/fdfs cp tracker.conf.sample tracker.conf vim tracker.conf 需要修改的部分如下：\n1 2 3 4 5 ## Tracker 数据和日志存储目录 base_path = /home/fastdfs/tracker ## HTTP 服务端口 http.server_port = 80 根据上述配置，创建配置中的目录\n1 mkdir -p /home/fastdfs/tracker 启动 Tracker 服务 初次成功启动，会在 base_path 即 /home/fastdfs/tracker 下创建 data、logs 两个目录。启动命令如下：\n1 2 3 /etc/init.d/fdfs_trackerd start ## 或 service fdfs_trackerd start 启动后，可以通过 netstat 命令查看是都启动成功，若得到以下类似输出，22122 端口处于监听状态，则表示 Tracker 服务安装并启动成功。\n1 netstat -unltp | grep fdfs ## 输出内容 tcp 0 0 0.0.0.0:22122 0.0.0.0:* LISTEN 6220/fdfs_trackerd 同理，可以通过下列命令关闭 Tracker 服务或者设置 Tracker 开机自启：\n1 2 3 4 5 ## 关闭服务 service fdfs_trackerd stop ## 开机自启 chkconfig fdfs_trackerd on 配置 FastDFS Storage 服务 修改配置文件 与配置 Tracker 服务类似，首先我们也需要拷贝样例配置文件并进行相应修改。\n1 2 3 cd /etc/fdfs cp storage.conf.sample storage.conf vim storage.conf 需要修改的部分如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ## Tracker 数据和日志存储目录 base_path = /home/fastdfs/storage ## 存放文件时 storage server 支持多个路径。这里配置存放文件的基路径数目，通常只配一个目录 store_path_count=1 ## 逐一配置 store_path_count 个路径，索引号基于 0 ## 如果不配置 store_path0，那它就和 base_path 对应的路径一样 store_path0=/home/fastdfs/file ## FastDFS 存储文件时，采用了两级目录。这里配置存放文件的目录个数。 ## 如果本参数为 N（默认 256），那么 storage server 在初次运行时，会在 store_path 下自动创建 N*N 个存放文件的子目录 subdir_count_per_path=256 ## tracker_server 的列表 ，会主动连接 tracker_server ## 有多个 tracker server 时，每个 tracker server 写一行 tracker_server=192.168.61.128:22122 ## HTTP 服务端口 http.server_port = 80 根据上述配置，创建配置中的目录\n1 2 3 4 5 ## base_path mkdir -p /home/fastdfs/storage ## store_path0 mkdir -p /home/fastdfs/file 启动 Storage 服务 启动 Storage 前确保 Tracker 是启动的。初次启动成功，会在 base_path 即 /home/fastdfs/storage/ 目录下创建 data、logs 两个目录。启动命令如下：\n1 2 3 /etc/init.d/fdfs_storaged start ## 或 service fdfs_storaged start 启动后，可以通过下列命令查看 Storage 服务是否启动成功。若输出结果与下列输出类似，23000 端口正处于监听状态，则 Storage 服务启动成功。\n1 netstat -unltp | grep fdfs ## 输出内容 tcp 0 0 0.0.0.0:23000 0.0.0.0:* LISTEN 6257/fdfs_storaged tcp 0 0 0.0.0.0:22122 0.0.0.0:* LISTEN 6220/fdfs_trackerd 同理，可以通过下列命令关闭 Tracker 服务或者设置 Tracker 开机自启：\n1 2 3 4 5 ## 关闭服务 service fdfs_storaged stop ## 开机自启 chkconfig fdfs_storaged on 此时我们可以在 store_path0 目录下看到 Storage 服务自动创建的 N*N 个子目录\n1 2 3 4 5 6 7 ls /home/fastdfs/file/data/ ## 统计文件夹数量 ls -l | grep \u0026#34;^d\u0026#34; | wc -l cd 00 ls -l | grep \u0026#34;^d\u0026#34; | wc -l ## 输出内容 00 0A 14 1E 28 32 3C 46 50 5A 64 6E 78 82 8C 96 A0 AA B4 BE C8 D2 DC E6 F0 FA 01 0B 15 1F 29 33 3D 47 51 5B 65 6F 79 83 8D 97 A1 AB B5 BF C9 D3 DD E7 F1 FB 02 0C 16 20 2A 34 3E 48 52 5C 66 70 7A 84 8E 98 A2 AC B6 C0 CA D4 DE E8 F2 FC 03 0D 17 21 2B 35 3F 49 53 5D 67 71 7B 85 8F 99 A3 AD B7 C1 CB D5 DF E9 F3 FD 04 0E 18 22 2C 36 40 4A 54 5E 68 72 7C 86 90 9A A4 AE B8 C2 CC D6 E0 EA F4 FE 05 0F 19 23 2D 37 41 4B 55 5F 69 73 7D 87 91 9B A5 AF B9 C3 CD D7 E1 EB F5 FF 06 10 1A 24 2E 38 42 4C 56 60 6A 74 7E 88 92 9C A6 B0 BA C4 CE D8 E2 EC F6 07 11 1B 25 2F 39 43 4D 57 61 6B 75 7F 89 93 9D A7 B1 BB C5 CF D9 E3 ED F7 08 12 1C 26 30 3A 44 4E 58 62 6C 76 80 8A 94 9E A8 B2 BC C6 D0 DA E4 EE F8 09 13 1D 27 31 3B 45 4F 59 63 6D 77 81 8B 95 9F A9 B3 BD C7 D1 DB E5 EF F9 256 256 文件上传测试 修改配置文件 与配置 Storage 服务类似，首先我们也需要拷贝客户端样例配置文件并进行相应修改。\n1 2 3 cd /etc/fdfs cp client.conf.sample client.conf vim client.conf 需要修改的部分如下：\n1 2 3 4 5 ## Client 的数据和日志目录 base_path=/home/fastdfs/client ## Tracker端口 tracker_server=192.168.61.128:22122 上传图片测试 执行下列命令，尝试上传一张图片到 FastDFS 中：\n1 fdfs_upload_file /home/Pictures/1989.jpg 上传成功后，会输出文件 ID：group1/M00/00/00/wKg9gF-f0aKAUJAlAARra4mLMhc390.jpg\n返回的文件 ID 由 group、存储目录、两级子目录、fileid、文件后缀名（由客户端指定，主要用于区分文件类型）拼接而成。\n配置 Nginx 安装 Nginx 的相关操作请查看文章 初识 Nginx，此处不再赘述。\n修改配置 经过上述操作之后，文件已经可以通过命令行的方式上传到 FastDFS 中，但还无法下载，此时我们需要使用 Nginx 来实现下载功能。修改 Nginx 配置文件，在 server 组内添加以下内容：\n1 2 3 4 5 6 7 8 9 server { listen 80; server_name localhost; ## 添加以下部分 location /group1/M00 { alias /home/fastdfs/file/data; } } 然后重启 Nginx，访问服务器 ip/fileid 进行测试，根据上述例子，此处访问 http://192.168.61.128/group1/M00/00/00/wKg9gF-f0aKAUJAlAARra4mLMhc390.jpg\n测试结果如下图，可以看到已经访问成功，HTTP 状态码返回 200。\n配置 fastdfs-nginx-module 模块 模块简介 fastdfs-nginx-module 可以重定向文件链接到源服务器，避免由于 Storage 服务器复制延迟导致文件无法访问而产生的错误。\n下载解压 1 2 3 wget https://github.com/happyfish100/fastdfs-nginx-module/archive/V1.22.tar.gz tar -zxvf V1.22.tar.gz cd fastdfs-nginx-module 为 Nginx 添加模块 首先进入 Nginx 源目录，然后执行 ./configure 命令，具体命令如下：\n1 2 3 4 nginx -s stop cd /usr/local/software/nginx-1.18.0 ./configure --prefix=/usr/local/nginx/ --add-module=$YOUR_PATH/fastdfs-nginx-module/src make \u0026amp;\u0026amp; make install 其中，/usr/local/software/nginx-1.18.0 为 Nginx 源目录，--prefix 参数指定新版本生成的目录，--add-module 参数表示添加模块，$YOUR_PATH 需要手动替换为 fastdfs-nginx-module 模块的文件路径，本例中为 /usr/local/software/fastdfs-nginx-module。\n安装好之后可以通过以下命令查看 Nginx 模块，如果输出与下列内容类似，则表示模块添加成功。\n1 nginx -V ## 输出内容 nginx version: nginx/1.18.0 built by gcc 8.3.1 20191121 (Red Hat 8.3.1-5) (GCC) configure arguments: --prefix=/usr/local/nginx/ --add-module=/usr/local/software/fastdfs-nginx-module/src 修改 Nginx 配置 修改 nginx.conf，在 server 中做出如下修改：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 server { listen 80; server_name localhost; ## 删除以下部分 location /group1/M00 { alias /home/fastdfs/file/data; } ## 添加以下部分 location ~/group[0-9]/ { ngx_fastdfs_module; } } 修改 fastdfs-nginx-module 配置 复制 fastdfs-nginx-module 源码中的配置文件到 /etc/fdfs 目录，兵做以下修改。\n1 2 cd /usr/local/software/fastdfs-nginx-module/src cp mod_fastdfs.conf /etc/fdfs/ 1 2 3 4 5 6 7 8 9 10 11 ## Tracker Server tracker_server=192.168.61.128:22122 ## Storage Server 默认端口 storage_server_port=23000 ## 如果文件 ID 的 uri 中包含 /group**，则要设置为 true url_have_group_name = true ## Storage 配置的 store_path0 路径，必须和 storage.conf 中的一致 store_path0=/home/fastdfs/file 复制 FastDFS 的部分配置文件到 /etc/fdfs 目录\n1 2 cd /usr/local/software/fastdfs-5.05/conf/ cp anti-steal.jpg http.conf mime.types /etc/fdfs/ 测试 配置完成之后，我们先启动 Nginx 服务，并观察输出内容，如果与下列输出类似，则表示配置成功。\n1 nginx ## 输出内容 ngx_http_fastdfs_set pid=11648 此时我们可以在浏览器中访问前文中上传的文件，能下载文件则表示 fastdfs-nginx-module 模块安装成功。注意和和前文中直接使用 Nginx 路由访问不同的是，这里配置 fastdfs-nginx-module 模块，可以重定向文件链接到源服务器取文件。访问结果如下图所示。\n至此，FastDFS 搭建分布式文件管理系统就初步完成了，在下一篇文章中，将介绍如何在客户端上通过 Java 来实现文件的上传和下载。\n参考文章 用 FastDFS 一步步搭建文件管理系统 FastDFS 简介 FastDFS Distributed File Storage FastDFS 配置文件详解（修订版 1） 初识 Nginx fastdfs nginx module installation introduction ","date":"2020-11-04T17:00:00+08:00","permalink":"https://jinggqu.github.io/posts/getting-to-know-fastdfs/","title":"FastDFS 搭建分布式文件管理系统"},{"content":"Spark 分布式内存计算框架 Spark 简介 Spark 是一种基于内存的、用以实现高效集群计算的平台。准确地讲，Spark 是一个大数据并行计算框架，是对广泛使用的 MapReduce 计算模型的扩展。Spark 有着自己的生态系统，但同时兼容 HDFS、Hive 等分布式存储系统，可以完美融入 Hadoop 的生态圈中，代替 MapReduce 去执行更为高效的分布式计算。两者的区别在于：基于 MapReduce 的计算引擎通常会将中间结果输出到磁盘上进行存储和容错；而 Spark 则是将中间结果尽量保存在内存中以减少底层存储系统的 I/O，以提高计算速度。\nSpark 编程模型 核心数据结构 RDD Spark 将数据抽象成弹性分布式数据集（Resilient Distributed Dataset, RDD），RDD 实际是分布在集群多个节点上数据的集合，通过操作 RDD 对象来并行化操作集群上的分布式数据。\nRDD 有两种创建方式:\n并行化驱动程序中已有的原生集合; 引用 HDFS、HBase 等外部存储系统上的数据集。 RDD 可以缓存在内存中，每次对 RDD 操作的结果都可以放到内存中，下一次操作时可直接从内存中读取，相对于 MapReduce,它省去了大量的磁盘 I/O 操作。另外，持久化的 RDD 能够在错误中自动恢复，如果某部分 RDD 丢失，Spark 会自动重算丢失的部分。\nRDD 上的操作 从相关数据源获取初始数据形成初始 RDD 后，需要根据应用的需求对得到的初始 RDD 进行必要的处理，来获取满足需求的数据内容，从而对中间数据进行计算加工，得到最终的数据。\nRDD 支持两种操作，一种是转换（Transformation）操作，另一种是行动（Action）操作。\n转换（Transformation）操作 转换操作即将一个 RDD 转换为一个新的 RDD。值得注意的是，转换操作是惰性的，这就意味着对 RDD 调用某种转换操作时，操作并不会立即执行，而是 Spark 在内部记录下所要求执行的操作的相关信息，当在行动操作中需要用到这些转换出来的 RDD 时才会被计算，下表所示为基本的转换操作。通过转换操作，可以从已有的 RDD 生成出新的 RDD, Spark 使用谱系（Lineage）记录新旧 RDD 之间的依赖关系，一旦持久化的 RDD 丢失部分数据时，Spark 能通过谱系图重新计算丢失的数据。\n输入数据为 {1, 2, 3, 3}\n函数名 目的 示例 结果 map() 将数据集中的每个元素经过用户自定义的函数转换形成一个新的 RDD rdd.map(x =\u0026gt; x * 2) {2, 4, 6, 6} flatMap() 与 map() 类似，但每个元素输入项都可以被映射到 0 个或多个的输出项，最终将结果“扁平化“后输出 rdd.flatMap(x =\u0026gt; (1 to x)) {1, 1, 2, 1, 2, 3, 1, 2, 3, 3} filter() 对 RDD 元素进行过滤，把经过指定函数后返回值为 true 的元素组成一个新的 RDD rdd.filter(x =\u0026gt; (x != 3)) {1, 2} distinct() 对数据进行去重，返回一个新的 RDD rdd.distinct() {1, 2, 3} sample(withReplacement, fraction, seed) 以指定的随机种子随机抽样出数量为 fraction 的数据，withReplacement 表示是抽出的数据是否放回，true 为有放回的抽样，false 为无放回的抽样 rdd.sample(true,0.5,3) 非确定的 行动（Action）操作 行动操作会触发 Spark 提交作业，对 RDD 进行实际的计算，并将最终求得的结果返回到驱动器程序，或者写入外部存储系统中。由于行动操作会得到一个结果，所以 Spark 会强制对 RDD 的转换操作进行求值，下表所示为基本的行动操作。\n输入数据为 {1, 2, 3, 3}\n函数名 目的 示例 结果 collect() 返回 RDD 中的所有元素 rdd.collect() {1, 2, 3, 3} count() 返回 RDD 中元素的个数 rdd.count() 4 countByValue() 返回 RDD 中各元素出现的次数 rdd.countByValue() {(1, 1), (2, 1), (3, 2)} take(n) 从 RDD 中返回 n 个元素（任意位置） rdd.take(2) {2, 3} top(n) 从 RDD 中返回前 n 个元素 rdd.top(2) {1, 2} reduce(func) 并行整合 RDD 中的所有数据 rdd.reduce((x, y) =\u0026gt; x + y) 9 fold(zero)(func) 与 reduce() 类似，但需要提供初始值。加法的默认是 0；乘法的默认是 1 rdd.fold(1)((x, y) =\u0026gt; x + y) 10 aggregate() 与 reduce() 类似，但通常返回不同类型的函数 rdd.aggregate((0, 0))((x, y) =\u0026gt; (x._1 + y, x._2 + 1), (x, y) =\u0026gt; (x._1 + y._1, x._2 + y._2)) (9, 4) foreach(func) 对 RDD 中的每个元素使用给定的函数 rdd.foreach(func) 无 示例 以下两个示例的数据集与源代码均可以在下述链接中进行下载 https://github.com/jinggqu/BigDataTechnologyFoundation_SourceCodeAndDataSet/tree/main/ch08\n一、分词 WordCount（单词统计程序）是大数据领域经典的例子，与 Hadoop 实现的 WordCount 程序相比，Spark 实现的版本要显得更加简洁。\n从 MapReduce 到 Spark 在经典的计算框架 MapReduce 中，问题会被拆成两个主要阶段: map 阶段和 reduce 阶段。对单词计数来说，MapReduce 程序从 HDFS 中读取一行字符串。在 map 阶段，将字符串分割成单词，并生成 \u0026lt;word, 1\u0026gt; 这样的键值对；在 reduce 阶段，将单词对应的计数值（初始为 1）全部累加起来，最后得到单词的总出现次数。\n在 Spark 中，并没有 map/reduce 这样的划分，而是以 RDD 的转换来呈现程序的逻辑。首先，Spark 程序将从 HDFS 中按行读取的文本作为初始 RDD（即集合的每一个元素都是一行字符串）；然后，通过 flatMap 操作将每一行字符串分割成单词，并收集起来作为新的单词 RDD；接着，使用 map 操作将每一个单词映射成 \u0026lt;word, 1\u0026gt;这样的键值对，转换成新的键值对 RDD；最后，通过 reduceByKey 操作将相同单词的计数值累加起来，得到单词的总出现次数。\nJava 实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import org.apache.spark.SparkConf; import org.apache.spark.api.java.JavaPairRDD; import org.apache.spark.api.java.JavaRDD; import org.apache.spark.api.java.JavaSparkContext; import scala.Tuple2; import java.util.Arrays; import java.util.List; import java.util.regex.Pattern; public class SparkDemo { private static final Pattern kSpace = Pattern.compile(\u0026#34; \u0026#34;); public static void main(String[] args) { SparkConf conf = new SparkConf().setAppName(\u0026#34;WordCount\u0026#34;); JavaSparkContext sc = new JavaSparkContext(conf); JavaRDD\u0026lt;String\u0026gt; lines = sc.textFile(args[0]).rdd().toJavaRDD(); JavaRDD\u0026lt;String\u0026gt; words = lines.flatMap(s -\u0026gt; Arrays.asList(kSpace.split(s)).iterator()); JavaPairRDD\u0026lt;String, Integer\u0026gt; ones = words.mapToPair(s -\u0026gt; new Tuple2\u0026lt;\u0026gt;(s, 1)); JavaPairRDD\u0026lt;String, Integer\u0026gt; counts = ones.reduceByKey(Integer::sum); List\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; output = counts.collect(); for (Tuple2\u0026lt;String, Integer\u0026gt; tuple : output) { System.out.println(tuple._1() + \u0026#34; : \u0026#34; + tuple._2()); } sc.close(); } } 运行过程分析 初始化 创建配置文件 SparkConf，这里仅设置应用名称；再创建 JavaSparkContext，在程序中主要通过 JavaSparkContext 来访问 Spark 集群； 处理数据 根据参数使用 Spark.read().textFile() 方法按行读取输入文件，并转换成 RDD lines； 使用 flatMap 操作将所有行按空格分割切割成词，并生成新的 RDD words； 使用 map 操作( Java 中为 mapToPair )，将词映射成 \u0026lt;word, 1\u0026gt;键值对 RDD ones，其中 1 表示出现一次； 使用 reduceByKey 操作将所有相同的 word 对应的计数累加起来，得到新的 RDD counts； 使用 collect 操作将所有结果打印出来； 关闭 JavaSparkContext。 执行 将上述代码生成 Jar 包之后，将其放到服务器中，执行下面的命令即可开始运行。\n1 ./bin/spark-submit --class SparkDemo ~/Documents/SparkDemo.jar ~/Documents/sample.txt 其中\n\u0026ndash;class SparkDemo 用来指定主类名 ~/Documents/SparkDemo.jar 指定 Jar 包路径 ~/Documents/sample.txt 指定测试文本路径 sample.txt 文本内容如下所示\n1 2 3 4 5 6 7 8 9 10 11 12 13 Your want text it even a text notes having wrong even about fake want or not even but. Language way contentwise just language contentwise recipes set start are. Recipes a words than with meeting days ?looks? even than is name story more story words generator anything gone. Having story but fairly random some adequate want it set has a kind looking having. Fantasy anything you looks just copy work text random sets even fake having. Piece some recipes repetitive adequate wrong wrong way options to repetitive working some dummy repetitive copy realistic you fake. Work or just fairly with is unrelated having language about set forever not game repetitive adequate now you looks it of even dummy now. That but design language unrelated copy you text placeholder has review those of with fake. Random to want the has gardening which business some realistic that and just work. Gardening you realistic kind and name looks about name words words way which some name. The that copy story realistic the adequate text meeting options game gone piece has options has name random. Days wrong set realistic design repetitive adequate review text your or but having start about right are story fairly fairly but to language sets adequate. But work want sets right kind some having contentwise fairly convincing language notes right name from but want realistic unrelated words. From about generator not looks or fairly copy has more. Forever from gone which or that having a with some having the work wrong generator design a fantasy way convincing. Working in dummy not now happily but to it of happily story want those kind looking right words business it generator language are. A you anything sets fake sets kind notes meeting having has in copy realistic is you. Copy or fairly set story. Wrong and days not work of want piece options unrelated way random just just recipes. Your recipes gardening start fairly. Happily start game days want from in set meeting that forever random. Support has wrong than your language are random business a even design has. Way design dummy unrelated set generator game convincing. Text contentwise copy to of set kind notes a you ?looks? gone work. Way forever result you to not. Your your meeting generator way placeholder looking than has want in repetitive more kind start has you but language a. A than notes name story and a just days some with in options looking just not are want looks. Kind some from review even. Some start random meeting recipes is a a ?looks? unrelated more the about but are dummy. Words review fake now kind of you meeting it design your. To just a about to. Not realistic name from with fake is. Work even business options fake wrong result notes want the more has dummy a notes random. Gone right repetitive fairly want now it want days review. Has notes want random name that random fantasy not unrelated in is dummy work work random game design now. Business result a and piece from working. Your some recipes copy sets are has kind story support fantasy has and some fantasy a which anything are the. Language piece that kind copy right anything dummy a of copy which fantasy placeholder which the work are convincing random. Your gone way copy you copy are that game but looking gardening result is start text the words the a anything. Want piece set set fantasy generator sets a more are happily or ?looks? just the and sets not anything. Support to just start game work looks copy that in of but words placeholder support now fairly fake even now. Text adequate words not fairly looks from game that result name realistic or you fake working want. Kind you some looking of review has sets than want the way working has. Of fantasy gardening and kind just game those adequate your from or text are you story working happily. Business set way gardening more dummy want are you business ?looks? work to placeholder are design options sets having. Working from options work right not meeting story it is of which way fake meeting. Adequate story than words want the anything. Language some gone random or just fairly gone which adequate sets having and adequate or text random from review. From unrelated those a start the ?looks? game business. With copy and which set kind game contentwise which anything the set story notes about or forever. Way anything work ?looks? a contentwise adequate and meeting. Options which realistic words it of to right game random way random your those and those anything some you notes gone gardening dummy than fake. But language just a your work with that set the. Are dummy business story not gardening start wrong fantasy fake and words having text which recipes your ?looks? wrong or. Generator fake than set looking text now forever more design ?looks? text but than has than wrong. Way than fake gardening those a now it language but piece. A is even looks just result that which realistic gone are working right fake some. Which language wrong having with that looks. 执行结果如下所示\nright : 9 Fantasy : 1 review : 5 convincing : 2 is : 8 Business : 2 even : 1 Are : 1 even : 10 start : 10 // 此处省略数行 二、统计用户的视频上传数 场景分析 接下来使用 Spark 来统计 Youtube 的测试数据集中每个用户的视频上传数量。稍加分析，会发现统计每个用户的视频数量其实与 WordCount 中统计每个单词出现的次数的逻辑几乎一致，区别在于处理 Youtube 测试数据集的格式略为复杂些。将给定的数据集按行划分，每行代表一条记录，除了视频类别这一字段中间有可能出现空格之外，其他的字段都是用空格分割。可以考虑使用正则表达式来匹配记录，并提取所需要的信息。\n在测试数据集中，假定每行所代表的视频都是唯一的，所以仅仅需要用户 ID 这一条信息。在提取到用户 ID 之后，可以像 WordCount 一样，组成 \u0026lt;ID, 1\u0026gt; 这样的用来计数的键值对，这步之后的逻辑便与 WordCount 相似了。\nJava 实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import org.apache.spark.SparkConf; import org.apache.spark.api.java.JavaPairRDD; import org.apache.spark.api.java.JavaRDD; import org.apache.spark.api.java.JavaSparkContext; import scala.Tuple2; import java.util.ArrayList; import java.util.List; import java.util.regex.Matcher; import java.util.regex.Pattern; public class SparkDemo { private static final Pattern EXTRACT = Pattern.compile(\u0026#34;(\\\\S+)\\\\s+(\\\\S+)\\\\s+(\\\\d+)\\\\s+(\\\\D+[a-zA-Z])\\\\s+(\\\\d+)\\\\s+(\\\\d+)\\\\s+(\\\\d+\\\\.?\\\\d*)\\\\s+(\\\\d+)\\\\s+(\\\\d+)\\\\s+(.*)\u0026#34;); public static void main(String[] args) { SparkConf conf = new SparkConf().setAppName(\u0026#34;CountUploader\u0026#34;); JavaSparkContext sc = new JavaSparkContext(conf); JavaRDD\u0026lt;String\u0026gt; lines = sc.textFile(args[0]); JavaRDD\u0026lt;String\u0026gt; filtered = lines.filter(s -\u0026gt; EXTRACT.matcher(s).matches()); JavaPairRDD\u0026lt;String, String\u0026gt; records = filtered.mapToPair(s -\u0026gt; { Matcher m = EXTRACT.matcher(s); boolean result = m.matches(); return new Tuple2\u0026lt;\u0026gt;(m.group(2), m.group(1)); }); JavaPairRDD\u0026lt;String, List\u0026lt;String\u0026gt;\u0026gt; groups = records.groupByKey().mapToPair(t -\u0026gt; { List\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); t._2().forEach(list::add); return new Tuple2\u0026lt;\u0026gt;(t._1(), list); }); // 手动实现 sortBy 操作 JavaRDD\u0026lt;Tuple2\u0026lt;String, List\u0026lt;String\u0026gt;\u0026gt;\u0026gt; tops = groups.keyBy(t -\u0026gt; t._2().size()).sortByKey(false).values(); List\u0026lt;Tuple2\u0026lt;String, List\u0026lt;String\u0026gt;\u0026gt;\u0026gt; topList = tops.take(100); for (Tuple2\u0026lt;String, List\u0026lt;String\u0026gt;\u0026gt; t : topList) { System.out.println(\u0026#34;User: \u0026#34; + t._1() + \u0026#34;, Number of videos: \u0026#34; + t._2().size()); } sc.stop(); } } 执行 将上述代码生成 Jar 包之后，将其放到服务器中，执行下面的命令即可开始运行。\n1 ./bin/spark-submit --class SparkDemo ~/Documents/SparkDemo.jar ~/Documents/YoutubeDataSets.txt 执行结果如下所示\nUser: machinima, Number of videos: 21 User: hotforwords, Number of videos: 19 User: theevang1, Number of videos: 19 User: kushtv, Number of videos: 19 User: supermac18, Number of videos: 18 User: NBA, Number of videos: 18 User: somedia, Number of videos: 17 User: tokiohotelchannel, Number of videos: 17 User: AtheneWins, Number of videos: 16 User: davidisbetterthenyou, Number of videos: 16 // 此处省略数行 参考文章 RDD Operations Spark 函数详解系列之 RDD 基本转换 Spark 教程之 RDD 操作-转换和执行（示例） Spark 笔记-玩转 RDD 操作 RDD Aggregate in spark 利用开发工具 IntelliJ IDEA 编写 Spark 应用程序（Scala+Maven） ","date":"2020-10-23T20:00:00+08:00","permalink":"https://jinggqu.github.io/posts/spark-distributed-programming/","title":"Spark 分布式内存计算框架"},{"content":"MapReduce 分布式编程 词频统计程序示例 假设将一个英文文本大文件作为输入，统计文件中单词出现的频数。最基本的操作是把输入文件的每一行传递给 map 函数完成对单词的拆分并输出中间结果，中间结果为 \u0026lt;word, 1\u0026gt; 的形式， 表示程序对一个单词，都对应一个计数 1。使用 reduce 函数收集 map 函数的结果作为输入值，并生成最终 \u0026lt;word, count\u0026gt; 形式的结果，完成对每个单词的词频统计。它们对应 MapReduce 处理数据流程如上图所示。\nMapReduce 程序的运行过程 如图所示，MapReduce 运行阶段数据传递经过输入文件、Map 阶段、中间文件、 Reduce 阶段、输出文件五个阶段，用户程序只与 Map 阶段和 Reduce 阶段的 Worker 直接相关，其他事情由 Hadoop 平台根据设置自行完成。\n从用户程序 User Program 开始，用户程序 User Program 链接了 MapReduce 库，实现了最基本的 map 函数和 reduce 函数。\nMapReduce 库先把 User Program 的输入文件划分为 M 份，如上图左方所示，将数据分成了分片 04，每一份通常为 16MB64MB；然后使用 fork 将用户进程复制到集群内其他机器上。 User Program 的副本中有一个 Master 副本和多个 Worker 副本。Master 是负责调度的，为空闲 Worker 分配 Map 作业或者 Reduce 作业。 被分配了 Map 作业的 Worker，开始读取对应分片的输入数据, Map 作业数量与输入文件划分数 M 相同，并与分片一一对应; Map 作业将输入数据转化为键值对表示形式并传递给 map 函数，map 函数产生的中间键值对被缓存在内存中。 缓存的中间键值对会被定期写入本地磁盘，而且被分为 R 个区（R 的大小是由用户定义的），每个区会对应一个 Reduce 作业；这些中间键值对的位置会被通报给 Master, Master 负责将信息转发给 Reduce Worker。 Master 通知分配了 Reduce 作业的 Worker 负责数据分区，Reduce Worker 读取键值对数据并依据键排序，使相同键的键值对聚集在一起。同一个分区可能存在多个键的键值对，而 reduce 函数的一次调用的键值是唯一的， 所以必须进行排序处理。 Reduce Worker 遍历排序后的中间键值对，对于每个唯一的键，都将键与关联的值传递给 reduce 函数，reduce 函数产生的输出会写回到数据分区的输出文件中。 当所有的 Map 和 Reduce 作业都完成了，Master 唤醒 User Program，MapReduce 函数调用返回 User Program。 执行完毕后，MapReduce 的输出放在 R 个分区的输出文件中，即每个 Reduce 作业分别对应一个输出文件。用户可将这 R 个文件作为输入交给另一个 MapReduce 程序处理，而不需要主动合并这 R 个文件。在 MapReduce 计算过程中，输入数据来自分布式文件系统，中间数据放在本地文件系统，最终输出数据写入分布式文件系统。\n必须指出 Map 或 Reduce 作业和 map 或 reduce 函数存在以下几个区别:\nMap 或 Reduce 作业是从计算框架的角度来认识的，而 map 或 reduce 函数是需要程序员编写代码完成的，并在运行过程中被对用 Map 或 Reduce 作业调度; Map 作业处理一个输入数据的分片，可能需要多次调用 map 函数来处理输入的键值对; Reduce 作业处理一个分区的中间键值对，期间要对每个不同的键调用一次 reduce 函数，一个 Reduce 作业最终对应一个输出文件。 经典 MapReduce 任务调度模型 经典 MapReduce 任务调度模型采用主从结构（Master/Slave），包含四个组成部分：Client、JobTracker、TaskTracker、Task。支撑 MapReduce 计算框架的是 JobTracker 和 TaskTracker 两类后台进程。框架结构如下图所示。\nClient 每一个 Job 在 Ciat 端将运行 MapRecdce 程序所需要的所有 Jar 文件和类的集合，打包成一个 Jar 文件存储在 HDFS 中，并把文件路径提交到 JobTracker。 JobTracker JobTracker 主要负责资源的监控和作业调度，一个 Hadoop 集群只有一个 JobTracker，并不参与具体的计算任务。根据提交的 Job，JobTackor 会创建一系列 Task（即 MapTask、ReduceTask），分发到每个 TaskTracker 服务中去执行。常用的作业调度算法主要包括 FIFO(First In First Out) 调度器（默认）、公平调度器、容量调度器等。 TaskTracker TaskTracker 主要负责汇报心跳和执行 JobTracker 分发的任务。TaskTracker 会周期性地通过 HeartBeat 将本节点上资源的使用情况和任务的运行进度汇报给 JobTracker，JobTracker 会根据心跳信息和当前作业运行情况为 TaskTracker 下达任务，主要包括启动任务、提交任务、杀死任务和重新初始化命令等。 Task Task 分为 MapTask 和 ReduceTask 两种，均由 TaskTracker 启动，执行 JobTracker 分发的任务。MapTask 解析每条数据记录，传递给用户编写的 map 函数并执行，最后将输出结果写入 HDFS；ReduceTask 从 MapTask 的执行结果中，对数据进行排序，将数据按分组传递给用户编写的 reduce 函数执行。 TaskTracker 分布在 Map-Reduce 集群每个节点上，主要是监视所在机器的资源情况和当前机器的 tasks 运行状况。TaskTracker 通过 HeartBeat 发送给 JobTracker，JobTracker 会根据这些信息给新提交的 job 分配计算节点。经典 MapReduce 框架 MR V1 模型简单直观，但是不能满足大规模集群任务调度的需要。主要表现为以下四点:\nJobTracker 是 MapReduce 的集中处理点，存在单点故障问题； 当 MapRcduce job 非常多的时候，会造成很大的内存开销，就增加了 JobTracker 失败的风险，业界普遍认为该调度模型支持的上限为 4000 个节点; 在 TaskTracker 端，以 Map/Reduce Task 的数目作为资源的表示过于简单，没有考虑到 CPU/内存的占用情况，如果两个大内存消耗的 Task 被调度到一起， 就很容易出现内存消耗殆尽的问题; TaskTracker 把资源强制划分为 Map Task Slot 和 Reduce Task Slot，如果当系统中只有 Map Task 或者只有 Reduce Task 时，会造成资源的浪费，导致集群资源利用不足。 YARN 框架原理及运行机制 为了从根本上解决经典 MapReduce 框架的性能瓶颈，Hadoop 的 MapReduce 框架完全重构，叫做 YARN 或者 MR V2。\nYARN 的基本思想就是将经典调度框架中 JobTracker 的资源管理和任务调度/监控功能分离成两个单独的组件，即一个全局的资源管理器 ResoureManager 和每个应用程序特有的 ApplicationMaster。ResoureManager 负责整个系统资源的管理和分配，而 ApplicationMaster 则负责单个应用程序的资源管理。\nYARN 调度框架包括 ResourceManager、ApplicationMaster、NodeMananger 及 Container 等组件概念。\nResourceManager 是基于应用程序对资源的需求进行调度的。每一个应用程序需要不同类型的资源，因此就需要不同的容器。这些资源包括内存、CPU、磁盘、网络等。 ApplicationMaster 负责向调度器申请、释放资源，清求 NodeManager 运行任务、跟踪应用程序的状态和监控它们的进程。\nNodeManager 是 YARN 中单个节点的代理，负责与应用程序的 ApplicationMaster 和集群管理者 ResourceManager 交互；从 ApplicationMaster 上接收有关 Container 的命令并执行（例如，启动、停止 Container）；向 ResourceManager 汇报各个 Container 执行状态和节点健康状况，并读取有关 Container 的命令；执行应用程序的容器、监控应用程序的资源使用情况并且向 ResourceManager 调度器汇报。\nContainer 是 YARN 中资源的抽象，它封装了节点上一定量的资源（CPU 和内存等）。一个应用程序所需的 Container 分为两类：一类是运行 ApplicationMaster 的 Container，是由 ResourceManager（向内部的资源调度器）申请和启动的，用户提交应用程序时，可指定唯一的 ApplicationMaster 所需的资源；另一类是运行各类任务的 Container，是由 ApplicationMaster 向 ResourceManager 申请的，并由 ApplicationMaster 与 NodeManager 通信后启动。\n用户向 YARN 提交一个应用程序后，YARN 将分为两个阶段运行该应用程序：第一个阶段是启动 ApplicationMaster；第二个阶段是由 ApplicationMaster 创建应用程序，为它申请资源，并监控它的整个运行过程，直到运行成功。\nYARN 任务调度流程如下图所示。\n用户向 YARN 提交应用程序； ResourceManager 为该应用程序在某个 NodeManagr 分配一个 Container，并要求 NodeManager 启动应用程序的 ApplicationMaster； ApplicationMaster 启动后立即向 ResourceManager 注册，此时用户可以直接通过 ResourceManager 查看应用程序的运行状态，然后它将为各个任务申请分布在某些 NodeManager 上的容器资源，并监控它的运行状态（步骤 4~7），直到运行结束； ApplicationMaster 采用轮询的方式向 ResourceManager 申请和领取资源； ApplicationMaster 申请到资源后，即与资源容器所在的 NodeManager 通信，要求其在容器内启动任务; NodeManager 为任务初始化运行环境（包括环境变量、jar 包、二进制程序等)，启动任务； 运行各个任务的容器通过向 ApplicationMaster 汇报自己的状态和进度，使 ApplicationMaster 随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。用户可以向 ApplicationMaster 查询应用程序的当前运行状态； 应用程序运行完成后，ApplicationMaster 向 ResourceManager 注销并关闭。 YARN 框架和经典的 MRV1 调度框架相比，主要有以下优化：\nApplicationMaster 使得检测每一个 Job 子任务状态的程序分布式化，减少了 JobTracker 资源消耗； 在 YARN 中，用户可以对不同的编程模型写自己的 ApplicationMaster, 可以让更多类型的编程模型运行在 Hadoop 集群上，如 Spark 基于内存的计算模型； Container 提供 Java 虚拟机内存的隔离，优化了经典调度框架中 Map Slot 和 Reduce Slot 分开造成集群资源闲置的不足。 Youtube 数据集统计分析 本例的数据来自于 Youtube 的数据集，完整的数据集以及源代码下载地址请点击以下链接 https://github.com/jinggqu/BigDataTechnologyFoundation-SourceCodeAndDataSet/blob/main/ch04\n该数据集各字段的具体含义如表所示：\n字段名 解释及数据类型 video ID 视频 ID：每个视频存在唯一的 11 位字符串 uploader 上传者用户名：字符串类型 age 视频上传日期与 2007 年 2 月 15 日（YouTube 创立日）的间隔天数：整数值 category 视频类别：字符串类型 length 视频长度：整数值 views 浏览量：整数值 rate 视频评分：浮点值 ratings 评分次数：整数值 comments 评论数：整数值 related IDs 相关视频 ID，每个相关视频的 ID 均为单独的一列：字符串类型 视频类型统计 场景：从已经上传的视频中，统计每一个视频类型下的视频数量。下表所示为数据集数据格式示例。category 列代表了视频类型，因而 map 函数只需逐行读取，返回视频类型为键和数字 1 为值的键值对，再传给 reduce 函数处理即可。map 函数的输入键依然为文本文件中行的偏移量，值为行内容。reduce 函数输出键值对为视频类型和该视频类型中的视频数量。\nvideo ID uploader age category length views rate ratings comments Related IDs PRGUU_ggO3k tom 704 Entertainment 262 11235 3.86 247 280 tpAL3iOurl4\u0026hellip;ifn1njiY4s RX24KLBhwMI jsack 687 Blogs 512 24149 4.22 315 474 PkGUU_ggO3k\u0026hellip;tpAl3iOurl4 Mapper 类代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 public static class Map extends Mapper\u0026lt;LongWritable, Text, Text, IntWritable\u0026gt; { private static final IntWritable ONE = new IntWritable(1); private final Text tx = new Text(); public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); String[] str = line.split(\u0026#34;\\t\u0026#34;); if (str.length \u0026gt; 3) { this.tx.set(str[3]); } context.write(this.tx, ONE); } } 第 2 行构造 IntWritable 可持久化对象并赋值为 1；第 8~10 行过滤字段，将一条记录中的分类 category 作为 map 函数的 value 输出。\nReduce 类代码实现 1 2 3 4 5 6 7 8 9 public static class Reduce extends Reducer\u0026lt;Text, IntWritable, Text, IntWritable\u0026gt; { public void reduce(Text key, Iterable\u0026lt;IntWritable\u0026gt; values, Context context) throws IOException, InterruptedException { int sum = 0; for (IntWritable v : values) { sum += v.get(); } context.write(key, new IntWritable(sum)); } } reduce 函数接收 Map 阶段传来的键值对，第 3~6 行遍历每一组记录，累加同一视频类型下的视频数量，第 7 行通过 context 输出计算结果。\n运行 通过 IDEA-Build-Build Artifacts 功能将代码打包为 jar 文件，命名为 CategoryCount.jar 登录 Hadoop 集群，将数据集文件 YoutubeDataSets.txt 传到 HDFS 下 /tmp 目录下 执行如下命令，开始运行程序 1 hadoop jar CategoryCount.jar CategoryCount /tmp/YoutubeDataSets.txt /tmp/output 执行如下命令，查看各类别视频数量 1 hadoop fs -cat /tmp/output/part-r-00000 可以得到如下输出\nUNA\t32 Autos \u0026amp; Vehicles\t77 Comedy\t420 Education\t65 Entertainment\t911 Film \u0026amp; Animation\t261 Howto \u0026amp; Style\t138 Music\t870 News \u0026amp; Politics\t343 Nonprofits \u0026amp; Activism\t43 People \u0026amp; Blogs\t399 Pets \u0026amp; Animals\t95 Science \u0026amp; Technology\t80 Sports\t253 Travel \u0026amp; Events\t113 ","date":"2020-10-17T20:00:00+08:00","permalink":"https://jinggqu.github.io/posts/mapreduce-distributed-programming/","title":"MapReduce 分布式编程"},{"content":"HDFS 文件管理 本文所有代码均可在 https://github.com/jinggqu/HDFSOperations 查看。\n通过命令行访问 HDFS 命令行是最简单、最直接操作文件的方式。这里介绍通过诸如读取文件、新建目录、移动文件、删除数据、列出目录等命令来进一步认识 HDFS。也可以输入 hadoop fs -help 命令获取每个命令的详细帮助。若熟悉 Linux 命令，Hadoop 命令看起来非常直观且易于使用。\n对文件和目录的操作 通过命令行对 HDFS 文件和目录的操作主要包括：创建、浏览、删除文件和目录，以及从本地文件系统与 HDFS 文件系统互相拷贝等。常用命令格式如下。\n1 2 3 4 5 6 7 8 9 10 11 12 hadoop fs -ls \u0026lt;path\u0026gt; # 列出 path 目录下的所有内容（文件和目录） hadoop fs -lsr \u0026lt;path\u0026gt; # 递归列出 path 下的所有内容（文件或目录） hadoop fs -df \u0026lt;path\u0026gt; # 查看目录的使用情况 hadoop fs -du \u0026lt;path\u0026gt; # 显示目录中所有文件及目录大小 hadoop fs -touchz \u0026lt;path\u0026gt; # 创建一个路径为为 path 的 0 字节的 HDFS 空文件 hadoop fs -mkdir \u0026lt;path\u0026gt; # 查看目录的使用情况 hadoop fs -rm [-skipTrash] \u0026lt;path\u0026gt; # 将 HDFS 上路径为 \u0026lt;path\u0026gt; 的文件移动到回收站，加上 -skipTrash，则直接删除 hadoop fs -rmr [-skipTrash] \u0026lt;path\u0026gt; # 将 HDFS 上路径为 \u0026lt;path\u0026gt; 的目录以及目录下的文件移动到回收站。如果加上 -skipTrash，则直接删除 hadoop fs -moveFromLocal \u0026lt;localsrc\u0026gt;...\u0026lt;dst\u0026gt; # 将 \u0026lt;localsrc\u0026gt; 本地文件移动到 HDFS 的 \u0026lt;dst\u0026gt; 目录下路径下 hadoop fs -moveToLocal [-crc] \u0026lt;src\u0026gt; \u0026lt;localdst\u0026gt; # 将 HDFS 上路径为 \u0026lt;src\u0026gt; 的文件移动到本地 \u0026lt;localdst\u0026gt; 路径下 hadoop fs -put \u0026lt;localsrc\u0026gt;...\u0026lt;dst\u0026gt; # 从本地文件系统中复制单个或者多个源路径到目标文件系统 hadoop fs -cat \u0026lt;src\u0026gt; # 浏览 HDFS 路径为 \u0026lt;src\u0026gt; 的文件的内容 修改权限或用户组 HDFS 提供了一些命令可以用来修改文件的权限、所属用户以及所属组别，具体格式如下:\nhadoop fs -chmod [-R] \u0026lt;MODE [,MODE]...|OCTALMODE\u0026gt; PATH...\n改变 HDFS 上路径为 PATH 的文件的权限，R 选项表示递归执行该操作。\n例如: hadoop fs -chmod -R +r /user/test，表示将 /user/test 目录下的所有文件赋予读的权限\nhadoop fs -chown [-R][OWNER][:[GROUP]]PATH...\n改变 HDFS 上路径为 PATH 的文件的所属用户，-R 选项表示递归执行该操作。\n例如: hadoop fs -chown -R hadoop:hadoop /user/test，表示将 /user/test 目录下所有文件的所属用户和所属组别改为 hadoop\nhadoop fs -chgrp ［-R] GROUP PATH...\n改变 HDFS 上路径为 PATH 的文件的所属组别，-R 选项表示递归执行该操作。\n例如: hadoop fs -chown -R hadoop /user/test 表示将 /user/test 目录下所有文件的所属组别改为 hadoop\n其他命令 HDFS 除了提供上述两类操作之外，还提供许多实用性较强的操作，如显示指定路径上的内容，上传本地文件到 HDFS 指定文件夹，以及从 HDFS 上下载文件到本地等命令。\nhadoop fs -tail [-f] \u0026lt;file\u0026gt;\n显示 HDFS 上路径为 \u0026lt;file\u0026gt; 的文件的最后 1KB 的字节，-f 选项会使显示的内容随着文件内容更新而更新。\n例如: hadoop fs -tail -f /user/test.txt\nhadoop fs -stat [format] \u0026lt;path\u0026gt;\n显示 HDFS 上路径为 \u0026lt;path\u0026gt; 的文件或目录的统计信息。格式为：%b 文件大小，%n 文件名，%r 复制因子，%y、%Y 修改日期。\n例如：hadoop fs -stat %b %n %o %r /user/test\nhadoop fs -put \u0026lt;localsrc\u0026gt;...\u0026lt;dt\u0026gt;\n将 \u0026lt;localsrc\u0026gt; 本地文件上传到 HDFS 的 \u0026lt;dst\u0026gt; 目录下。\n例如: hadoop fs -put /home/hadoop/test.txt /user/hadoop\nhadoop fs -count [-q] \u0026lt;path\u0026gt;\n显示 \u0026lt;path\u0026gt; 下的目录数及文件数，输出格式为“目录数 文件数 大小 文件名”，加上 -q 可以查看文件索引的情况。\n例如: hadoop fs -count /\nhadoop fs -get [-ignoreCrc] [-crc] \u0026lt;src\u0026gt; \u0026lt;localdst\u0026gt;\n将 HDFS 上 \u0026lt;src\u0026gt; 的文件下载到本地的 \u0026lt;localdst\u0026gt; 目录，可用 -ignorecrc 选项复制 CRC 校验失败的文件，使用 -crc 选项复制文件以及 CRC 信息。\n例如: hadoop fs -get /user/hadoop/a.txt /home/hadoop\nhadoop fs -getmerge \u0026lt;src\u0026gt; \u0026lt;localdst\u0026gt; [addnl]\n将 HDFS 上 \u0026lt;src\u0026gt; 目录下的所有文件按文件名排序并合并成一个文件输出到本地的 \u0026lt;localdst\u0026gt; 目录，addnl 是可选的，用于指定在每个文件结尾添加一个换行符。\n例如: hadoop fs -getmerge /user/test /home/hadoop/o\nhadoop fs -test -[ezd] \u0026lt;path\u0026gt;\n检查 HDFS 上路径为 \u0026lt;path\u0026gt; 的文件。-e 检查文件是否存在，如果存在则返回 0。-z 检查文件是否为 0 字节，如果是则返回 0。-d 检查路径是否是目录，如果是则返回 1，否则返回 0。\n例如：hadoop fs -test -e /user/test.txt\n通过 Java API 访问 HDFS 使用 Hadoop URL 读取数据 要从 Hadoop 文件系统读取数据，最简单的方法是使用 java.net.URL 对象打开数据流，从中读取数据。\n让 Java 程序能够识别 Hadoop 的 HDFS URL 方案还需要一些额外的工作，这里采用的方法是通过 org.apache.hdfs.FsUrlStreamHandlerFactor 实例调用 java.net.URL 对象的 setURLStreamHandlerFactory 实例方法。每个 Java 虚拟机只能调用一次这个方法，因此通常在静态方法中调用。下述范例展示的程序以标准输出方式显示 Hadoop 文件系统中的文件，类似于 UNIX 中的 cat 命令。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import org.apache.hadoop.fs.FsUrlStreamHandlerFactory; import org.apache.hadoop.io.IOUtils; import java.io.InputStream; import java.net.URL; public class URLCat { static { URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory()); } public static void main(String[] args) throws Exception{ InputStream inputStream = null; try { inputStream = new URL(args[0]).openStream(); IOUtils.copyBytes(inputStream, System.out, 4096, false); } finally { IOUtils.closeStream(inputStream); } } } 编译代码，导出为 URLCat.jar 文件，并在 /user/hadoop/ 中准备一个测试文件 test，然后执行命令：\n1 hadoop jar hdfsclient.jar URLCat hdfs://master:9000/user/hadoop/test 执行完成后可以在屏幕上看到 /user/hadoop/test 文件中的内容。该程序是从 HDFS 读取文件的最简单的方式，即用 java.net.URL 对象打开数据流。其中，第 8~10 行静态代码块的作用是设置 URL 类能够识别 Hadoop 的 HDFS URL。第 16 行 IOUtils 是 Hadoop 中定义的类，调用其静态方法 copyBytes 实现从 HDFS 文件系统拷贝文件到标准输出流。4096 表示用来拷贝的缓冲区大小，false 表示拷贝完成后不关闭拷贝源。\n通过 FileSystem API 读取数据 在实际开发中，访问 HDFS 最常用的类是 FileSystem 类。Hadoop 文件系统中通过 Hadoop Path 对象来定位文件。可以将路径视为一个 Hadoop 文件系统 URI，如 hdfs:localhost/user/hadoop/test。FileSystem 是一个通用的文件系统 API，获取 FileSystem 实例有下面几个静态方法:\n1 2 3 public static FileSystem get(Configuration conf) throws IOException public static FileSystem get(URI uri,Configuration conf) throws IOException public static FileSystem get(URI uri, Configuration conf, String user) throw IOException 下面分别给出几个常用操作的代码示例。\n读取文件 1 2 3 4 5 6 7 8 9 10 11 12 13 @Test public void readFile() throws Exception { String uri = \u0026#34;hdfs://master:9000/user/hadoop/test\u0026#34;; Configuration configuration = new Configuration(); FileSystem fileSystem = FileSystem.get(URI.create(uri), configuration); InputStream in = null; try { in = fileSystem.open(new Path(uri)); IOUtils.copyBytes(in, System.out, 4096, false); } finally { IOUtils.closeStream(in); } } 上述代码直接使用 FileSystem 以标准输出格式显示 Hadoop 文件系统中的文件。\n第 4 行产生一个 Confguation 类的实例，代表了 Hadoop 平台的配置信息，并在第 5 行作为引用传递到 FileSystem 的静态方法 get 中，产生 FileSystem 对象。\n第 9 行与上例类似，调用 Hadoop 中 IOUtils，并在 finally 字中关闭数据流，同时也可以在输入流和输出流之间复制数据。copyBytes 方的最后两个参数，第一个设置用于复制的缓冲区大小，第二个设置复制结束后是否关闭数据流。\n写入文件 1 2 3 4 5 6 7 8 9 10 @Test public void writeFile() throws Exception { String source = \u0026#34;C:\\\\Users\\\\Desktop\\\\test\u0026#34;; String destination = \u0026#34;hdfs://master:9000/user/hadoop/test2\u0026#34;; BufferedInputStream inputStream = new BufferedInputStream(new FileInputStream(source)); Configuration configuration = new Configuration(); FileSystem fileSystem = FileSystem.get(URI.create(destination), configuration); OutputStream outputStream = fileSystem.create(new Path(destination)); IOUtils.copyBytes(inputStream, outputStream, 4096, true); } 创建目录 1 2 3 4 5 6 7 8 9 @Test public void createFolder() throws Exception { String uri = \u0026#34;hdfs://master:9000/user/test\u0026#34;; Configuration configuration = new Configuration(); FileSystem fileSystem = FileSystem.get(URI.create(uri), configuration); Path path = new Path(uri); fileSystem.mkdirs(path); fileSystem.close(); } 删除文件或目录 1 2 3 4 5 6 7 8 9 10 @Test public void deleteFile() throws Exception { String uri = \u0026#34;hdfs://master:9000/user/hadoop/test2\u0026#34;; Configuration configuration = new Configuration(); FileSystem fileSystem = FileSystem.get(URI.create(uri), configuration); Path path = new Path(\u0026#34;hdfs://master:9000/user/hadoop\u0026#34;); boolean isDeleted = fileSystem.delete(path, true); System.out.println(isDeleted); fileSystem.close(); } 使用 FileSystem 的 delete() 方法可以永久性删除文件或目录。如果要递归删除文件夹，则需要将其第二个参数设为 true。\n列出文件或目录 1 2 3 4 5 6 7 8 9 10 11 12 @Test public void listFiles() throws Exception { String uri = \u0026#34;hdfs://master:9000/user\u0026#34;; Configuration configuration = new Configuration(); FileSystem fileSystem = FileSystem.get(URI.create(uri), configuration); Path path = new Path(uri); FileStatus[] status = fileSystem.listStatus(path); for (FileStatus fileStatus : status) { System.out.println(fileStatus.getPath().toString()); } fileSystem.close(); } 文件系统的重要特性是提供浏览和检索其目录结构下所存文件与目录相关信息的功能。 FileStatus 类封装了文件系统中文件和目录的元数据，例如文件长度、块大小、副本、修改时间、所有者以及权限信息等。编译运行上述代码后控制台将会打印出 /user 目录下的名称或者文件名。\n小结 HDFS 组成部分 HDFS 是一个分布式文件存储系统 Client 提交读写请求（拆分 blocksize） NameNode 全局把控（存储数据位置） DataNode 存储数据（将数据存储进去，且以 Pipeline 的方式把数据写完） HDFS 数据交互 写入数据 使用 HDFS 提供的客户端 Client，向远程的 NameNode 发起 RPC 请求 NameNode 会检查要创建的文件是否已经存在，创建者是否有权限进行操作，成功则会为文件创建一个记录，否则会让客户端抛出异常 当客户端开始写入文件的时候，客户端会将文件切分成多个 packets，并在内部以数据队列 data queue（数据队列） 的形式管理这些 packets，并向 NameNode 申请 blocks，获取用来存储 replicas 的合适的 DataNode 列表，列表的大小根据 NameNode 中 replication（副本份数）的设定而定 开始以 pipeline（管道）的形式将 packet 写入所有的 replicas 中。客户端把 packet 以流的方式写入第一个 DataNode，该 DataNode 把该 packet 存储之后，再将其传递给在此 pipeline 中的下一个 DataNode，直到最后一个 DataNode，这种写数据的方式呈流水线的形式 最后一个 DataNode 成功存储之后会返回一个 ack packet（确认队列），在 pipeline 里传递至客户端，在客户端的开发库内部维护着 \u0026ldquo;ack queue\u0026rdquo;，成功收到 DataNode 返回的 ack packet 后会从 \u0026ldquo;data queue\u0026rdquo; 移除相应的 packet 如果传输过程中，有某个 DataNode 出现了故障，那么当前的 pipeline 会被关闭，出现故障的 DataNode 会从当前的 pipeline 中移除，剩余的 block 会继续剩下的 DataNode 中继续以 pipeline 的形式传输，同时 NameNode 会分配一个新的 DataNode，保持 replicas 设定的数量。 客户端完成数据的写入后，会对数据流调用 close() 方法，关闭数据流 只要写入了 dfs.replication.min（最小写入成功的副本数）的复本数（默认为 1），写操作就会成功，并且这个块可以在集群中异步复制，直到达到其目标复本数（dfs.replication 的默认值为 3），因为 NameNode 已经知道文件由哪些块组成，所以它在返回成功前只需要等待数据块进行最小量的复制 读取数据 客户端调用 FileSystem 实例的 open 方法，获得这个文件对应的输入流 InputStream 通过 RPC 远程调用 NameNode，获得 NameNode 中此文件对应的数据块保存位置，包括这个文件的副本的保存位置（主要是各 DataNode 的地址） 获得输入流之后，客户端调用 read 方法读取数据。选择最近的 DataNode 建立连接并读取数据 如果客户端和其中一个 DataNode 位于同一机器（比如 MapReduce 过程中的 mapper 和 reducer)，那么就会直接从本地读取数据 到达数据块末端，关闭与这个 DataNode 的连接，然后重新查找下一个数据块 不断执行第 2~5 步直到数据全部读完 客户端调用 close，关闭输入流 DFS InputStream HDFS 漫画 以上漫画版权均归原图作者所有\n参考文章 https://www.cnblogs.com/qingyunzong/p/8548806.html\n","date":"2020-10-12T15:20:11+08:00","permalink":"https://jinggqu.github.io/posts/hdfs-file-system/","title":"HDFS 文件管理"},{"content":"初识 Hadoop 前言 本系列文章是基于《大数据技术基础》与 10 小时入门大数据 课程，如果有兴趣可以先阅读该书并观看视频教程。本系列文章中所用到的软件版本及其下载地址如下：\n名称 版本 下载地址 CentOS 8.2.2004 https://mirrors.tuna.tsinghua.edu.cn/centos/8.2.2004/isos/x86_64/CentOS-8.2.2004-x86_64-minimal.iso JDK 14.0.2 https://www.oracle.com/java/technologies/javase/jdk14-archive-downloads.html Hadoop 2.10.1 https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.10.1/hadoop-2.10.1-src.tar.gz 环境准备 配置网络 此篇文章所使用的 CentOS 环境均是使用 VMware 15 虚拟的，具体安装教程请查看 使用 VMware 15 安装虚拟机和使用 CentOS 8，此处不再赘述。安装好一个节点之后，我们可以采用“虚拟机克隆”的方式，直接完成另外两个节点系统的安装。\n虚拟机的网络配置采用 DHCP 自动分配模式，每台机器的 IP 地址可以通过命令 ip address 或 ifconfig 查看，其中 ifconfig 输出如下，第一组配置中 ens33 即为本机网络配置，inet 项对应的即为本机 ip（192.168.61.128）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ens33: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 192.168.61.128 netmask 255.255.255.0 broadcast 192.168.61.255 inet6 fe80::20c:29ff:fe65:9052 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 00:0c:29:65:90:52 txqueuelen 1000 (Ethernet) RX packets 38037 bytes 6542757 (6.2 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 30479 bytes 16809162 (16.0 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt; loop txqueuelen 1000 (Local Loopback) RX packets 23656 bytes 13542580 (12.9 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 23656 bytes 13542580 (12.9 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 virbr0: flags=4099\u0026lt;UP,BROADCAST,MULTICAST\u0026gt; mtu 1500 ether 52:54:00:d2:b3:31 txqueuelen 1000 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 本篇文章中三台集群的 IP 分别如下，下文中不再赘述。\n主机名 IP master 192.168.61.128 slave1 192.168.61.129 slave2 192.168.61.131 配置 host 以上三台机器要搭建成为集群，就需要让它们互相认识。这个认识的过程是通过 /etc/hosts 文件来实现的。这一步需要修改每一台机器的 hosts 文件，将以下内容分别粘贴到各个机器的 hosts 文件中。\n1 vim /etc/hosts 1 2 3 192.168.61.128 master 192.168.61.129 slave1 192.168.61.131 slave2 配置 JDK 因为 Hadoop 的环境依赖于 Java JDK，所以需要确保虚拟机中已经正确安装了 JDK，除此之外我们还需要将 JDK 地址配置到环境变量中。在本例中，我的 JDK 安装位置是 /usr/java/jdk-14.0.2。\n修改 bash_profile 1 vim ~/.bash_profile 添加以下内容到 .bash_profile 文件末尾：\n1 2 export JAVA_HOME=/usr/java/jdk-14.0.2 export PATH=$JAVA_HOME/bin:$PATH 修改完成并保存后，还需要执行 source 命令使环境变量立即生效。\n1 source ~/.bash_profile 然后即可使用 java -version 检查环境变量是否配置成功，执行结果如下所示。\n1 2 3 java version \u0026#34;14.0.2\u0026#34; 2020-07-14 Java(TM) SE Runtime Environment (build 14.0.2+12-46) Java HotSpot(TM) 64-Bit Server VM (build 14.0.2+12-46, mixed mode, sharing) 配置 SSH 免密钥登录 在 Linux 集群间配置免密钥登录，是 Hadoop 集群运维的基础。以下操作在 master 节点进行，实现从 master 免密钥登录 slave1、slave2 节点。生成 ssh 密钥的命令如下：\n1 ssh-keygen 生成过程中会有一些提示，一路回车即可。执行结果如下所示。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 root@master:/usr/local/software## ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): /root/.ssh/id_rsa already exists. Overwrite (y/n)? y Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub. The key fingerprint is: SHA256:DC7+sETaazn0f4OVgxozjdw2XM1Tb60cqoaQvDGXpg8 root@master The key\u0026#39;s randomart image is: +---[RSA 3072]----+ | | | . | | . o . ..| | . o . + . +| | oo.*S+ . + + | | =..% @ + . o | | ..=oE## = o | | .+==.o = | | .o..ooo . | +----[SHA256]-----+ 接下来需要将生成的公钥上传到 slave1 节点，命令如下：\n1 ssh-copy-id root@slave1 首次通过 master 终端将公钥传给 salve 终端，需要输入 slave 节点的登录密码。上述命令中我们是传输到 slave1 的 root 账户下，所以需要输入 root 用户的密码，传送完毕即可实现免密码登录。执行结果如下。\n1 2 3 4 5 6 7 8 9 10 root@master:/usr/local/software## ssh-copy-id root@slave1 /usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: \u0026#34;/root/.ssh/id_rsa.pub\u0026#34; /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed /usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys root@slave1\u0026#39;s password: Number of key(s) added: 1 Now try logging into the machine, with: \u0026#34;ssh \u0026#39;root@slave1\u0026#39;\u0026#34; and check to make sure that only the key(s) you wanted were added. slave2 节点命令同上，只需更改传送到的节点名称，执行结果如下。\n1 2 3 4 5 6 7 8 9 10 root@master:/usr/local/software## ssh-copy-id root@slave2 /usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: \u0026#34;/root/.ssh/id_rsa.pub\u0026#34; /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed /usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys root@slave2\u0026#39;s password: Number of key(s) added: 1 Now try logging into the machine, with: \u0026#34;ssh \u0026#39;root@slave2\u0026#39;\u0026#34; and check to make sure that only the key(s) you wanted were added. 现在可以尝试登录子节点 slave1 和 slave2。\n1 ssh root@slave1 成功登录 salve1 节点的提示如下。\n1 2 3 4 root@master:/usr/local/software## ssh root@slave1 Web console: https://slave1:9090/ or https://192.168.61.129:9090/ Last login: Fri Sep 24 14:56:46 2020 from 192.168.61.1 完善配置 以下配置均在 master 节点上完成，配置完成后可直接复制到 slave 节点，以免重复劳动。\n安装 Hadoop 1 2 3 4 5 cd /usr/local/software wget http://mirror.cogentco.com/pub/apache/hadoop/common/hadoop-2.10.1/hadoop-2.10.1-src.tar.gz tar -zxvf hadoop-2.10.1-src.tar.gz cd hadoop-2.10.1-src mv * ~/hadoop 在正式使用 Hadoop 集群之前，我们还需要对其配置文件进行修改。本节中的配置内容请以 官方文档 为准。\nHadoop 的配置文件均存放在 Hadoop 所在目录的 /etc/hadoop/ 文件夹下。\n修改配置文件 编辑 core-site.xml 文件 core-site.xml 用来配置 Hadoop 集群的通用属性，包括指定 NameNode 的地址、指定使用 Hadoop 时临时文件的存放路径、指定检查点备份日志的最长时间等。\n使用 vim 打开文件：\n1 vim ~/hadoop-2.10.1/etc/hadoop/core-site.xml 使用以下内容替换 core-site.xml 中的内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;?xml-stylesheet type=\u0026#34;text/xsl\u0026#34; href=\u0026#34;configuration.xsl\u0026#34;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;!-- 指定 namenode 的地址 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.defaultFS\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://master:9000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定使用 Hadoop 时临时文件的存放路径 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.tmp.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/home/hadoop/temp\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 第 69 行配置 fs.defaultFS 的属性为 hdfs://master:9000，master 是主机名；第 1215 行指定 Hadoop 的临时文件夹为 /home/hadoop/temp，此文件夹用户可以自己指定。\n编辑 hdfs-site.xml 文件 hdfs-site.xml 用来配置分布式文件系统 HDFS 的属性，包括指定 HDFS 保存数据的副本数量，指定 HDFS 中 NameNode、DataNode 的存储位置等。\n使用 vim 打开文件：\n1 vim ~/hadoop-2.10.1/etc/hadoop/hdfs-site.xml 使用以下内容替换 hdfs-site.xml 中的内容：\n1 2 3 4 5 6 7 8 9 10 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;?xml-stylesheet type=\u0026#34;text/xsl\u0026#34; href=\u0026#34;configuration.xsl\u0026#34;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;!-- 指定 HDFS 保存数据的副本数量 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.replication\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;1\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 其中，第 7~8 行，指定 HDFS 文件快的副本数为 1。数据块副本一般为 3 以上，本文章仅作示例，故指定为 1。\n编辑 yarn-site.xml YARN 是 MapReduce 的调度框架。文件 yarn-site.xml 用配置 YARN 的属性，包括指定 NameNodeManager 获取数据的方式，指定 ResourceManager 的地址，配置 YARN 打印工作日志等。\n使用 vim 打开文件：\n1 vim ~/hadoop-2.10.1/etc/hadoop/yarn-site.xml 使用以下内容替换 yarn-site.xml 中的内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;!-- 指定 NameNodeManager 获取数据的方式是 shuffle --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.aux-services\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;mapreduce_shuffle\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.env-whitelist\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定 YARN 中 ResourceManager 所在的主机名 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.hostname\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;master\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 其中，第 15~19 行配置了 ResourceManager 所在的主机名，如果不进行配置，将会导致 MapReduce 不能获得资源，任务不能执行。\n编辑 mapred-site.xml 文件 mapred-site.xml 主要是配置 MapReduce 的属性，主要是 Hadoop 系统提交的 Map/Reduce 程序运行在 YARN 上。\n首先复制一份 mapred-site.xml.template 文件为 mapred-site.xml，然后打开并进行修改。\n1 vim ~/hadoop-2.10.1/etc/hadoop/mapred-site.xml 使用以下内容替换 mapred-site.xml 中的内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;?xml-stylesheet type=\u0026#34;text/xsl\u0026#34; href=\u0026#34;configuration.xsl\u0026#34;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.framework.name\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;yarn\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.application.classpath\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 其中，第 5~8 行为 MapReduce 指定任务调度框架为 YARN。\n编辑 slaves slaves 文件为 Hadoop 提供了子节点的主机名。\n1 vim ~/hadoop-2.10.1/etc/hadoop/slaves 使用以下内容替换 slaves 中的内容：\n1 2 slave1 slave2 复制文件到子节点 使用下面的命令将 Hadoop 文件复制到其他节点，本文中为 slave1 和 slave2，命令如下：\n1 2 3 cd ~/hadoop scp -r hadoop-2.10.1 root@slave1:~/hadoop/ scp -r hadoop-2.10.1 root@slave2:~/hadoop/ 配置 Hadoop 环境变量 注意，此操作需要同时在所有节点（master，slave1，slave2）都执行一次，操作命令如下：\n1 vim ~/.bash_profile 将以下内容追加到 .bash_profile 文件末尾：\n1 2 3 #HADOOP export HADOOP_HOME=/root/hadoop/hadoop-2.10.1 export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH 然后执行下列命令使环境变量生效：\n1 source ~/.bash_profile 创建临时文件存放目录 我们在 core-site.xml 文件中指定了 Hadoop 临时文件存放路径，但是文件夹并没有创建，此操作需要同时在所有节点（master，slave1，slave2）都执行一次，操作命令如下：\n1 mkdir /home/hadoop/temp 启动集群 格式化文件系统 注意，格式化仅需要在第一次使用 Hadoop 集群时进行，后续使用时无需格式化，并且在使用过程中进行格式化，所有文件将会丢失。此操作需要在 master 节点上进行，执行如下命令：\n1 hdfs namenode -format 启动 Hadoop 集群 Hadoop 启动或停止服务的脚本均存放在 sbin 目录中，所以切换到 /home/hadoop/hadoop-2.10.1/sbin 目录下，执行以下命令：\n1 start-all.sh 需要注意的是，在启动过程中，Hadoop 会提示这样的启动方式已经过时，使用如下启动方式即可规避过时提示：\n1 2 start-dfs.sh start-yarn.sh 查看进程是否启动成功 在 master 节点终端执行 jps 命令，在打印结果中会看到四个进程，分别是 NodeManager、SecondaryNameNode、ResourceManager、Jps。如果出现了这四个进程表示启动成功。结果如下：\n1 2 3 4 5 root@master:~/hadoop/hadoop-2.10.1/sbin## jps 17874 NameNode 18070 SecondaryNameNode 18281 ResourceManager 18554 Jps 此时在 slave1 和 slave2 的节点的终端执行 jps 命令，在输出结果中会看到三个进程，分别是 Jps、NodeManager、DataNode，如果出现了这三个进程表示子节点进程启动成功。结果如下：\n1 2 3 4 root@slave1:~## jps 15776 NodeManager 15639 DataNode 16106 Jps 查看 WebUI Hadoop 页面 如果要在宿主机上访问虚拟机 master 节点的 WebUI，需要先将虚拟机的防火墙关闭（此处仅仅是做示例，生产环境不建议这么做），然后访问虚拟机 master 节点 IP:50070 即可。\n防火墙相关命令如下：\n1 2 3 4 5 6 7 8 ## 暂时关闭防火墙 systemctl stop firewalld ## 永久关闭防火墙 systemctl disable firewalld ## 启用防火墙 systemctl enable firewalld 例如本例中 master 节点地址为 192.168.61.128，则访问 192.168.61.128:50070，页面如下图：\nYARN 页面 如上例，与 Hadoop 管理页面不同的是，YARN Web 页面地址端口是 8088，页面如下图：\n运行实例 在 Hadoop 自带的 examples 中有一种利用分布式系统计算圆周率的方法，采用的是拟蒙特卡罗（Quasi-Monte Carlo）算法来对 $ \\pi $ 的值进行估算。下面通过运行该程序来检验 Hadoop 集群是否安装配置成功。\n在 master 节点终端中执行下面的命令：\n1 hadoop jar hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.10.1.jar pi 100 100000 Hadoop 的命令类似 Java 命令，通过 jar 指定要运行的程序所在的 jar 包 hadoop-mapreduce-examples-2.10.1.jar。参数 pi 表示需要计算的圆周率 $ \\pi $。后面两个参数中，100 是指要运行 100 次 map，100000 表示每个 map 的任务次数，即每个节点要模拟飞镖 100000 次。执行过程及结果如下图：\n1 2 Job Finished in 131.634 seconds Estimated value of Pi is 3.14158440000000000000 至此，Hadoop 环境配置完成。\n备注 如果在执行 mapreduce 任务中报错如 此问题 的描述，参考 此篇文章，需要在 mapred-site.xml 文件中添加下列配置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.map.memory.mb\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;4096\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.reduce.memory.mb\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;8192\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.map.java.opts\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;-Xmx3072m\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.reduce.java.opts\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;-Xmx6144m\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; ","date":"2020-09-24T09:20:11+08:00","permalink":"https://jinggqu.github.io/posts/getting-to-know-hadoop/","title":"初识 Hadoop"},{"content":"初识 Nginx（二） 应用示例 本篇文章中所使用的 Nginx 是通过下载软件包手动编译安装的，详见 上一篇文章 离线安装部分。\n在上一篇文章中，我们初步接触了 Nginx 的安装以及使用方法。在本篇文章中我们将以具体的静态网页作为例子，来详细介绍 Nginx 的部分细节。\n文件准备 我们以 C++ 文档 dlib 为例做介绍，官网 http://dlib.net，点击左下角的 Download 按钮并将下载好的文件解压。将文件夹中的 docs 目录内容复制到 Nginx 安装目录中的 dlib 目录中。相关的目录结构如下。\n1 2 3 4 5 6 7 8 9 10 11 drwxr-xr-x. 9 root root 258 9月 11 16:54 blog drwx------. 2 nobody root 6 9月 6 15:26 client_body_temp drwxr-xr-x. 2 root root 4096 9月 11 19:48 conf drwxrwxrwx. 10 root root 8192 8月 9 03:30 dlib drwx------. 2 nobody root 6 9月 6 15:26 fastcgi_temp drwxr-xr-x. 2 root root 40 9月 6 15:24 html drwxr-xr-x. 2 root root 58 9月 11 16:20 logs drwx------. 2 nobody root 6 9月 6 15:26 proxy_temp drwxr-xr-x. 2 root root 19 9月 6 15:24 sbin drwx------. 2 nobody root 6 9月 6 15:26 scgi_temp drwx------. 2 nobody root 6 9月 6 15:26 uwsgi_temp 修改配置 编辑 conf/nginx.conf，将 server 中的 location 部分修改为如下配置。\n1 2 3 4 location / { alias dlib/; #... } 其中，location 后的 / 代表根域名指向括号中的目录配置，alias 指定一个目录替代默认目录。\n更多信息可以查看官方文档\nhttp://nginx.org/en/docs/http/ngx_http_core_module.html#alias\n重新加载 执行以下命令，以新加载 Nginx 服务。\n1 nginx -s reload 访问 执行完以上步骤后，访问 Nginx 的地址，即可看到 dlib 下的静态文件已经被正常加载了。如下图。\n常用配置 以下内容均在 nginx.conf 文件中进行配置。\n数据压缩 根据以上的配置，我们已经可以正常访问部署好的静态网页，但是根据开发者工具我们可以看到，首页的大小是 26.4 kB。我们还可以进一步进行优化，将所需要加载的数据进行压缩，使其所需数据量大大减少。\n在 http 部分中添加以下配置。\n1 2 3 4 gzip on; gzip_min_length 1; gzip_comp_level 2; gzip_types text/plain application/x-javascript text/css application/xml text/javascript application/x- httpd- php image/jpeg image/gif image/png; 本例中所涉及到的配置释义如下。\n配置 释义 gzip on | off 是否启用数据压缩 gzip_min_length 会被压缩的响应的最小长度（单位 kB），即返回内容大于此配置时才会被压缩 gzip_comp_level 设置 gzip 压缩等级，等级越小压缩速度越快、文件压缩比越小。压缩等级范围是 1-9，压缩等级越高对性能要求越高。 gzip_types 设置需要压缩的 MIME 类型，非设置值不进行压缩，即匹配压缩类型 更多信息可以查看官方文档\nhttp://nginx.org/en/docs/http/ngx_http_gzip_module.html\n可以看到，开启 gzip 压缩后，加载的数据量大幅减少。\n加载速度 使用 limit_rate 可以对网页加载速度进行控制，详细如下。\n1 2 3 4 5 server { #... set $limit_rate 1k; #... } 其中，$limit_rate 是控制访问速度的变量。后面紧跟的 1k 是需要限制的速度，此例中的单位为 kB，也可以设置其他单位，例如 1M。\n更多信息可以查看官方文档\nhttp://nginx.org/en/docs/http/ngx_http_core_module.html#var_limit_rate\n记录日志 在 http 模块中，可以配置日志记录的格式，以及日志记录的位置和文件名等等，配置如下。\n1 2 3 4 5 6 7 8 9 http { #... log_format main \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39; \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39; \u0026#39;\u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34;\u0026#39;; access_log logs/sample.log main; #... } 更多信息可以查看官方文档\nhttp://nginx.org/en/docs/http/ngx_http_log_module.html#access_log\n反向代理 在此部分，我们使用两台 Nginx 服务器作为示例，分别是 192.168.61.128 和 192.168.61.129，简记为 CentOS_1 与 CentOS_2。\n修改 CentOS_1 的 Nginx 配置文件，修改部分如下，此时直接访问 192.168.61.128 已经不能正常进行加载。\n1 2 3 4 5 server { listen 127.0.0.1:80; server_name localhost; #... } 修改 CentOS_2 的 Nginx 配置文件，修改部分如下，我们将 192.168.61.129:80 指向了 192.168.61.128:80。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 upstream local { server 192.168.61.128:80; } server { listen 80; server_name 192.168.61.129; location / { proxy_set_header Host $host:$server_port; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://local/; } #... } 分别在两台机器上重新加载 Nginx 配置文件，并访问 192.168.61.129:80，此时可以正常打开 192.168.61.128:80 上所配置的静态文件。\n上述配置可以参考官方文档\nhttp://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_set_header\n日志可视化 在本例中，我们使用 GoAccess 首先日志试试可视化。\n安装 GoAccess 由于此例中使用的系统是 CentOS 8，安装 GoAccess 之前需要先配置 epel 源。\n配置 epel 源 安装 epel 配置包 1 yum install -y https://mirrors.aliyun.com/epel/epel-release-latest-8.noarch.rpm 将 repo 配置中的地址替换为阿里云镜像站地址 1 2 sed -i \u0026#39;s|^#baseurl=https://download.fedoraproject.org/pub|baseurl=https://mirrors.aliyun.com|\u0026#39; /etc/yum.repos.d/epel* sed -i \u0026#39;s|^metalink|#metalink|\u0026#39; /etc/yum.repos.d/epel* 安装 GeoIP 编译安装 GoAccess 需要用到 GeoIP。\n1 sudo yum -y --enablerepo=epel install geoip 安装 GoAccess 根据官网的文档，我们顺序执行以下命令即可。\n1 2 3 4 5 6 wget https://tar.goaccess.io/goaccess-1.4.tar.gz tar -xzvf goaccess-1.4.tar.gz cd goaccess-1.4/ ./configure --enable-utf8 --enable-geoip=legacy make make install 安装完成后，使用以下命令，若得到类似结果则表明安装成功。\n1 goaccess -v 1 2 3 4 5 6 7 GoAccess - 1.4. For more details visit: http://goaccess.io Copyright (C) 2009-2020 by Gerardo Orellana Build configure arguments: --enable-utf8 --enable-geoip=legacy 配置 GoAccess 在使用前，我们需要对 GoAccess 的配置文件进行一些修改，以方便后续使用。\n1 vim /usr/local/etc/goaccess/goaccess.conf 在此配置文件中，将以下内容取消注释，其他内容则保持不变。\n1 2 3 4 5 6 7 #... no-ip-validation true log-format COMBINED time-format %H:%M:%S date-format %d/%b/%Y real-time-html true #... 启动监听 在 Nginx 的 logs 目录中，我们以 access 为源文件，启动 GoAccess 进程后，会产生一个 websocket 长连接，持续监听客户端的请求数据，进而实时展现在 report.html 页面上。\n1 2 cd /usr/local/nginx/logs goaccess access.log -o ../html/report.html 此时我们还需要修改 nginx.conf 使报告页面可以直接访问，在 server 部分添加如下内容。\n1 2 3 location /report.html { alias html/report.html; } 然后重新加载配置即可。\n1 nginx -s reload 至此，日志可视化已经配置完成，我们可以直接访问 Nginx 服务地址/report.html 查看可视化页面，如下。\n附录 Linux 查看端口占用状态\n查看占用 1 netstat -anp 其中，参数 anp 分别表示：\na：显示所有活动的 TCP 连接，以及正在监听的 TCP 和 UDP 端口\nn：以数字形式表示地址和端口号，不试图去解析其名称（number），参数 -n 会将应用程序转为端口显示，即数字格式的地址，如：nfs-\u0026gt;2049，ftp-\u0026gt;21\np：列出与端口监听或连接相关的进程，即 pid\n关闭占用 在本例中，若要关闭 GoAccess 建立的连接，首先执行如下命令拿到其 pid。\n1 netstat -anp | grep goaccess 结果如下。\n1 2 tcp 0 0 0.0.0.0:7890 0.0.0.0:* LISTEN 2323/goaccess tcp 0 0 192.168.61.128:7890 192.168.61.1:56503 ESTABLISHED 2323/goaccess 然后手动 kill 其进程即可。\n1 kill 2323 ","date":"2020-09-11T19:51:07+08:00","permalink":"https://jinggqu.github.io/posts/getting-to-know-nginx-2/","title":"初识 Nginx（二）"},{"content":"初识 Nginx Nginx 简介 简介内容来自 Nginx 官网 http://nginx.org/en\nnginx [engine x] is an HTTP and reverse proxy server, a mail proxy server, and a generic TCP/UDP proxy server, originally written by Igor Sysoev. For a long time, it has been running on many heavily loaded Russian sites including Yandex, Mail.Ru, VK, and Rambler. According to Netcraft, nginx served or proxied 25.75% busiest sites in August 2020. Here are some of the success stories: Dropbox, Netflix, Wordpress.com, FastMail.FM.\nThe sources and documentation are distributed under the 2-clause BSD-like license.\nCommercial support is available from Nginx, Inc.\n简而言之，Nginx 是一个高性能的 HTTP 和反向代理服务器，特点是占有内存少，并发能力强。详细信息请查看 Nginx 官网介绍页面。\n以下两种安装方式，任意选择一种进行安装即可。\n在线安装 Nginx 注意，本篇文章基于 CentOS 8.2 版本，如使用其他系统，操作可能有一些变化，一切以官方网站安装教程为准。\n首先我们需要先安装 yum-utils 包，执行以下命令即可。\n1 sudo yum install yum-utils 然后配置 Nginx 仓库，我们需要在 /etc/yum.repo.d/ 中创建一个名为 nginx.repo 的文件，并填入以下内容。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [nginx-stable] name=nginx stable repo baseurl=http://nginx.org/packages/centos/$releasever/$basearch/ gpgcheck=1 enabled=1 gpgkey=https://nginx.org/keys/nginx_signing.key module_hotfixes=true [nginx-mainline] name=nginx mainline repo baseurl=http://nginx.org/packages/mainline/centos/$releasever/$basearch/ gpgcheck=1 enabled=0 gpgkey=https://nginx.org/keys/nginx_signing.key module_hotfixes=true 默认情况下使用的是 Nginx 稳定版仓库，即配置中的 nginx-stable。如果需要使用主线版仓库，可以执行下面的命令进行手动指定。\n1 sudo yum-config-manager --enable nginx-mainline 上述准备工作完成后，就可以开始安装 Nginx 了，执行下面这条命令即可。\n1 sudo yum install nginx 稍等片刻完成安装后，可使用下述命令来验证是否安装成功。\n1 2 nginx -v whereis nginx 得到类似于下面的输出，即代表安装成功。\n1 2 nginx version: nginx/1.18.0 nginx: /usr/sbin/nginx /usr/lib64/nginx /etc/nginx /usr/share/nginx /usr/share/man/man8/nginx.8.gz 离线安装 Nginx 下载并解压 1 2 3 4 5 6 7 cd /usr/local/software ## 下载 sudo wget http://nginx.org/download/nginx-1.18.0.tar.gz ## 解压 tar -zxvf nginx-1.18.0.tar.gz -C ./ 编译 1 2 cd nginx-1.18.0 sudo ./configure --prefix=/usr/local/nginx 其中 --prefix 的作用是指定编译后的文件存放位置，可以根据实际情况自由确定。\n编译过程中可能会遇到一些报错，详细信息和解决方案如下。\n此部分内容参照文章 Nginx 教程(一) Nginx 入门教程\n./configure: error: C compiler cc is not found\n错误原因：缺少编译环境，安装编译源码所需要的工具和库：\n执行命令：sudo yum install gcc gcc-c++ ncurses-devel perl\n./configure: error: the HTTP rewrite module requires the PCRE library\n错误原因：缺少 HTTP rewrite module 模块\n执行命令：sudo yum install pcre pcre-devel\n./configure: error: the HTTP gzip module requires the zlib library\n错误原因：缺少 HTTP zlib 类库，我们选择安装模块：\n执行命令：sudo yum install zlib gzip zlib-devel\n上述报错都解决了之后，再次执行编译命令，可以得到如下输出。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Configuration summary + using system PCRE library + OpenSSL library is not used + using system zlib library nginx path prefix: \u0026#34;/usr/local/nginx\u0026#34; nginx binary file: \u0026#34;/usr/local/nginx/sbin/nginx\u0026#34; nginx modules path: \u0026#34;/usr/local/nginx/modules\u0026#34; nginx configuration prefix: \u0026#34;/usr/local/nginx/conf\u0026#34; nginx configuration file: \u0026#34;/usr/local/nginx/conf/nginx.conf\u0026#34; nginx pid file: \u0026#34;/usr/local/nginx/logs/nginx.pid\u0026#34; nginx error log file: \u0026#34;/usr/local/nginx/logs/error.log\u0026#34; nginx http access log file: \u0026#34;/usr/local/nginx/logs/access.log\u0026#34; nginx http client request body temporary files: \u0026#34;client_body_temp\u0026#34; nginx http proxy temporary files: \u0026#34;proxy_temp\u0026#34; nginx http fastcgi temporary files: \u0026#34;fastcgi_temp\u0026#34; nginx http uwsgi temporary files: \u0026#34;uwsgi_temp\u0026#34; nginx http scgi temporary files: \u0026#34;scgi_temp\u0026#34; 安装 1 2 cd /usr/local/software/nginx-1.18.0 sudo make \u0026amp; make install 得到如下输出时，即表明 Nginx 已经安装成功。\n1 2 3 4 test -d \u0026#39;/usr/local/nginx/logs\u0026#39; \\ || mkdir -p \u0026#39;/usr/local/nginx/logs\u0026#39; make[1]: 离开目录\u0026#34;/usr/local/software/nginx-1.18.0\u0026#34; [1]+ 已完成 make 在线安装 Nginx 的启动方式 如果是采用手动编译安装 Nginx 的方式，请跳过本节查看下一节内容。\n配置 nginx.conf 首先编辑 /etc/nginx/nginx.conf 文件，配置 Nginx 端口与访问地址（即 server 部分）。在配置端口时，不建议设置为 80，以免与其他服务冲突。具体配置如下。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 user nginx; worker_processes 1; error_log /var/log/nginx/error.log warn; pid /var/run/nginx.pid; events { worker_connections 1024; } http { include /etc/nginx/mime.types; default_type application/octet-stream; log_format main \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39; \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39; \u0026#39;\u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34;\u0026#39;; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf; ## 在这里新增 server 配置 server { listen 8090; server_name localhost; location / { root html; index index.html index.htm; } } } 系统防火墙 由于我的 CentOS 是安装在虚拟机中，未安装图形界面，故需要在宿主机上进行测试并访问虚拟机地址，所以需要增加一步禁用 CentOS 防火墙的操作，具体命令如下。\n1 2 3 4 5 ## 关闭防火墙 systemctl stop firewalld.service ## 禁止防火墙开机自启 systemctl disable firewalld.service 现在可以正式启动 Nginx 服务了，执行下述命令即可。\n1 sudo nginx 执行之后，可以使用下面命令检查是否启动成功，以及访问地址和端口是否生效。\n查看包含 nginx 关键词的进程 1 ps -ef | grep nginx 结果如下，可以看到已经成功启动了。\n1 2 root 4781 1 0 14:31 ? 00:00:00 nginx: master process nginx nginx 5055 4781 0 14:36 ? 00:00:00 nginx: worker process 查看本机所有暴露的端口 1 netstat -ntlp 结果如下，可以看到此前配置的 8090 端口已经是 listen 状态，接下来就可以在浏览器中访问了。\n1 2 3 4 5 6 7 Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name ... tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN - tcp 0 0 0.0.0.0:8090 0.0.0.0:* LISTEN - tcp 0 0 127.0.0.1:6010 0.0.0.0:* LISTEN - ... 在浏览器中访问 虚拟机ip:8090，即可看到 Nginx 的欢迎页面。\n此时返回 404 是因为 Nginx 目录中并没有欢迎页面的 html 文件，但依然可以说明已经 Nginx 服务已经配置正确并启动成功。\n离线安装 Nginx 的启动方式 首先执行下述命令来启动 Nginx。\n1 2 cd /usr/local/nginx/sbin sudo ./nginx 同样的，我们可以按照上一节中介绍的方法，来验证 Nginx 是否启动成功，以及端口是否开放。\nNginx 服务默认的端口是 80，如果需要修改端口，也可以参照上一节中的内容进行手动修改，略有不同的是，通过手动编译安装的 Nginx，配置文件地址在 /usr/local/nginx/conf/nginx.conf，也就是编译时我们手动指定的路径下。 其余内容此处皆不再赘述。\n打开宿主机浏览器，访问 虚拟机ip:80，就可以看到 Nginx 的欢迎页面。\n修改 nginx.conf 如果后续需要修改 nginx.conf 中的内容，例如更改 Nginx 服务端口号，请务必在修改完成后重启服务。常用的 Nginx 命令如下。\n1 2 3 4 5 6 7 8 ## 检查 nginx.conf 是否配置正确 sudo nginx -t ## 重启 Nginx 服务 sudo nginx -s reload ## 停止 Nginx 服务 sudo nginx -s stop 附录 在 Linux 中查找某一个具体文件路径时，可以使用以下命令。\n1 sudo find / -name filename / 代表查找的目录，此例是根目录 -name 代表按照文件名进行查找 filename 代表具体的文件名，例如 nginx.conf 以本文章为例，在根目录中查找 nginx.conf 的结果如下。\n1 2 3 /etc/nginx/nginx.conf /usr/local/software/nginx-1.18.0/conf/nginx.conf /usr/local/nginx/conf/nginx.conf ","date":"2020-09-07T15:47:11+08:00","permalink":"https://jinggqu.github.io/posts/getting-to-know-nginx/","title":"初识 Nginx"},{"content":"初识 Docker Docker 简介 简介来自于 Docker 入门教程 - 阮一峰的网络日志\nDocker 属于 Linux 容器的一种封装，提供简单易用的容器使用接口。它是目前最流行的 Linux 容器解决方案。\nDocker 将应用程序与该程序的依赖，打包在一个文件里面。运行这个文件，就会生成一个虚拟容器。程序在这个虚拟容器里运行，就好像在真实的物理机上运行一样。有了 Docker，就不用担心环境问题。\n总体来说，Docker 的接口相当简单，用户可以方便地创建和使用容器，把自己的应用放入容器。容器还可以进行版本管理、复制、分享、修改，就像管理普通的代码一样。\n安装 Docker 在此部分，作者使用的是 Centos 8.2 进行的操作，下述的安装命令仅保证在该环境下运行。\n设置 Docker 仓库 根据官方教程，执行以下两条命令：\n1 2 3 4 5 sudo yum install -y yum-utils sudo yum-config-manager \\ --add-repo \\ http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 由于国内直接连接 Docker 官方镜像源十分缓慢，所以在第二个命令中将官方镜像源替换为阿里云镜像源。\n安装 Docker 引擎 1 sudo yum install docker-ce docker-ce-cli containerd.io 在执行这条命令时，极有可能会报错。比如作者遇到的报错如下：\n1 2 3 4 5 6 7 8 9 10 Error: Problem: package docker-ce-3:19.03.8-3.el7.x86_64 requires containerd.io \u0026gt;= 1.2.2-3, but none of the providers can be installed - cannot install the best candidate for the job - package containerd.io-1.2.10-3.2.el7.x86_64 is excluded - package containerd.io-1.2.13-3.1.el7.x86_64 is excluded - package containerd.io-1.2.2-3.3.el7.x86_64 is excluded - package containerd.io-1.2.2-3.el7.x86_64 is excluded - package containerd.io-1.2.4-3.1.el7.x86_64 is excluded - package containerd.io-1.2.5-3.1.el7.x86_64 is excluded - package containerd.io-1.2.6-3.3.el7.x86_64 is excluded 为了解决这个报错，需要先执行下述命令安装好 containerd.io 组件。\n1 2 3 4 sudo yum install -y \\ https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/edge/Packages/containerd.io-1.2.13-3.2.el7.x86_64.rpm sudo yum -y install ./containerd.io-1.2.13-3.1.el7.x86_64.rpm 至于为什么安装的 containerd.io 组件是 centos 7 目录下的，有两个原因：\n此版本在 centos 8 环境下也可以正常使用； 阿里云官方只提供了适配 centos 7 的 containerd.io 组件。 然后重新执行上述命令即可完成 Docker 的安装。\n1 sudo yum install docker-ce docker-ce-cli 我们可以通过两个命令来验证 Docker 是否安装成功。\n1 2 3 docker version ## 或 docker info 若输出类似于以下的内容，则配置正确。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 [root@localhost ~]## docker version Client: Docker Engine - Community Version: 19.03.12 API version: 1.40 Go version: go1.13.10 Git commit: 48a66213fe Built: Mon Jun 22 15:46:54 2020 OS/Arch: linux/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 19.03.12 API version: 1.40 (minimum version 1.12) Go version: go1.13.10 Git commit: 48a66213fe Built: Mon Jun 22 15:45:28 2020 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.2.6 GitCommit: 894b81a4b802e4eb2a91d1ce216b8817763c29fb runc: Version: 1.0.0-rc8 GitCommit: 425e105d5a03fabd737a126ad93d62a9eeede87f docker-init: Version: 0.18.0 GitCommit: fec3683 配置 Docker 仓库 由于在国内连接 Docker 官方仓库 https://hub.docker.com 十分缓慢，故我们可以将仓库地址更换为国内的各种源，详细步骤如下。\n在 /etc/docker 目录中新增一个名为 daemon.json 的配置文件，如果已经存在这个文件，则只需要进行修改。 将该文件中的 registry-mirrors 项修改为如下形式。 1 2 3 4 5 6 { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://kuamavit.mirror.aliyuncs.com\u0026#34;, \u0026#34;https://docker.mirrors.ustc.edu.cn\u0026#34; ] } 到此就配置完毕了。\n启动并运行 Docker 启动 1 2 3 $ sudo service docker start ## 或 $ sudo systemctl start docker 运行 hello-world 1 sudo docker run hello-world 此时，由于本地尚未安装 hello-world 实例，Docker 会自动从上文中配置的镜像中拉取 hello-world 实例，然后运行这个实例。具体输出如下。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 [root@localhost docker]## docker run hello-world ## 这里提示未在本地找到 hello-world 实例，将从镜像中拉取最新版。 Unable to find image \u0026#39;hello-world:latest\u0026#39; locally latest: Pulling from library/hello-world 0e03bdcc26d7: Pull complete Digest: sha256:7f0a9f93b4aa3022c3a4c147a449bf11e0941a1fd0bf4a8e6c9408b2600777c5 Status: Downloaded newer image for hello-world:latest ## 开始运行 Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \u0026#34;hello-world\u0026#34; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ 容器管理 当 image 文件开始运行之后，就会生成一个容器实例，容器实例实际上也是一个文件，故也称为实例文件。当容器实例停止运行时，容器文件并不会被删除。\n列出本机正在运行的容器 1 sudo docker container ls 列出本机所有容器（包括已停止运行的） 1 sudo docker container ls -all 输出结果如下。\n1 2 CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d3a382fde773 hello-world \u0026#34;/hello\u0026#34; 8 minutes ago Exited (0) 8 minutes ago romantic_proskuriakova 其中 CONTAINER ID 就是该容器的唯一标识符，在后续的终止容器运行时需要用到。\n终止容器运行 1 sudo docker container kill d3a382fde773 当然本例中 hello-world 容器已经停止运行了，所以不能再次停止，此处仅做示例。 即使一个容器文件已经停止运行，但是其依然会占据磁盘空间，可以使用下述命令进行删除。\n1 sudo docker container rm d3a382fde773 镜像管理 当我们拉取了多个镜像，其中某些又不需要使用了，则需要对镜像进行手动管理，详细操作如下。\n罗列所有本地已经安装的镜像 1 sudo docker image ls 输出如下\n1 2 REPOSITORY TAG IMAGE ID CREATED SIZE hello-world latest bf756fb1ae65 8 months ago 13.3kB 移除不需要的镜像 1 sudo docker image rm hello-world 注意，如果 image 在运行，或者已经生成了实例文件，是不能直接删除的，需要先将实例容器停止并删除实例文件，才可以正常删除。执行结果如下。\n1 2 3 4 Untagged: hello-world:latest Untagged: hello-world@sha256:7f0a9f93b4aa3022c3a4c147a449bf11e0941a1fd0bf4a8e6c9408b2600777c5 Deleted: sha256:bf756fb1ae65adf866bd8c456593cd24beb6a0a061dedf42b26a993176745f6b Deleted: sha256:9c27e219663c25e0f28493790cc0b88bc973ba3b1686355f221c38a36978ac63 ","date":"2020-09-02T18:37:11+08:00","permalink":"https://jinggqu.github.io/posts/getting-to-know-docker/","title":"初识 Docker"},{"content":" ……我细读来书，终觉得你不免作茧自缚。你自己去寻出一个本不成问题的问题，“人生有何意义？”其实这个问题是容易解答的。人生的意义全是各人自己寻出来、造出来的：高尚、卑劣、清贵、污浊、有用、无用，……全靠自己的作为。\n生命本身不过是一件生物学的事实，有什么意义可说？一个人与一只猎，一只狗，有什么分别？人生的意义不在于何以有生，而在自己怎样生活。你若情愿把这六尺之躯葬送在白昼作梦之上二那就是你这一生的意义。你若发愤振作起来，决心去寻求生命的意义，去创造自己的生命的意义，那么，你活一日便有一日的意义，作一事便添一事的意义，生命无穷，生命的意义也无穷了。\n总之，生命本没有意义，你要能给他什么意义，他就有什么意义。与其终日冥想人生有何意义，不如试用此生作点有意义的事……\n节选自《答某君书》—— 胡适\n","date":"2019-06-21T00:00:00+08:00","permalink":"https://jinggqu.github.io/posts/the-meaning-of-life/","title":"The meaning of life"}]